{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24919,"status":"ok","timestamp":1713148175525,"user":{"displayName":"M Faizan","userId":"01979528019072830709"},"user_tz":-300},"id":"7i0VhXZ0oyNb","outputId":"06126029-09af-4eec-a3c2-cab672185a95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzXrmfQIoz85","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713148457306,"user_tz":-300,"elapsed":281787,"user":{"displayName":"M Faizan","userId":"01979528019072830709"}},"outputId":"efca9824-6048-44c5-ef2b-0315c7197648"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1iS6gSWfUE3cZnmrNWbbV9_W_zQH3vaiu/enz-eff-project\n","Collecting pandas<1.6,>=1.4 (from -r requirements.txt (line 1))\n","  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.25.2)\n","Collecting rdkit (from -r requirements.txt (line 4))\n","  Downloading rdkit-2023.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fair-esm (from -r requirements.txt (line 5))\n","  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.0.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.7.1)\n","Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.2.7)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.2.2)\n","Collecting pickle5 (from -r requirements.txt (line 10))\n","  Downloading pickle5-0.0.11.tar.gz (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting biopython (from -r requirements.txt (line 11))\n","  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2023.12.25)\n","Collecting drfp (from -r requirements.txt (line 13))\n","  Downloading drfp-0.3.6-py2.py3-none-any.whl (9.4 kB)\n","Collecting zeep (from -r requirements.txt (line 14))\n","  Downloading zeep-4.2.1-py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.2/101.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cobra (from -r requirements.txt (line 15))\n","  Downloading cobra-0.29.0-py2.py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pubchempy (from -r requirements.txt (line 16))\n","  Downloading PubChemPy-1.0.4.tar.gz (29 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting suds (from -r requirements.txt (line 17))\n","  Downloading suds-1.1.2-py3-none-any.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bioservices (from -r requirements.txt (line 18))\n","  Downloading bioservices-1.11.2.tar.gz (191 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.9/191.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting fastapi (from -r requirements.txt (line 19))\n","  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting kaleido (from -r requirements.txt (line 20))\n","  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-multipart (from -r requirements.txt (line 21))\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Collecting uvicorn (from -r requirements.txt (line 22))\n","  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions<4.6.0 (from -r requirements.txt (line 23))\n","  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<1.6,>=1.4->-r requirements.txt (line 1)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<1.6,>=1.4->-r requirements.txt (line 1)) (2023.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (3.13.4)\n","INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n","Collecting torch (from -r requirements.txt (line 2))\n","  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m750.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 2))\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 2))\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 2))\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 2))\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 2))\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 2))\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 2))\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 2))\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 2))\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.18.1 (from torch->-r requirements.txt (line 2))\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 2))\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Collecting triton==2.1.0 (from torch->-r requirements.txt (line 2))\n","  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 2))\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->-r requirements.txt (line 4)) (9.4.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost->-r requirements.txt (line 6)) (1.11.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (3.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt->-r requirements.txt (line 8)) (1.16.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt->-r requirements.txt (line 8)) (0.18.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt->-r requirements.txt (line 8)) (4.66.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt->-r requirements.txt (line 8)) (2.2.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt->-r requirements.txt (line 8)) (0.10.9.7)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.4.0)\n","Collecting rdkit-pypi (from drfp->-r requirements.txt (line 13))\n","  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.10/dist-packages (from drfp->-r requirements.txt (line 13)) (8.1.7)\n","Requirement already satisfied: attrs>=17.2.0 in /usr/local/lib/python3.10/dist-packages (from zeep->-r requirements.txt (line 14)) (23.2.0)\n","Collecting isodate>=0.5.4 (from zeep->-r requirements.txt (line 14))\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from zeep->-r requirements.txt (line 14)) (4.9.4)\n","Requirement already satisfied: platformdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from zeep->-r requirements.txt (line 14)) (4.2.0)\n","Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from zeep->-r requirements.txt (line 14)) (2.31.0)\n","Collecting requests-toolbelt>=0.7.1 (from zeep->-r requirements.txt (line 14))\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests-file>=1.5.1 (from zeep->-r requirements.txt (line 14))\n","  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n","Requirement already satisfied: appdirs~=1.4 in /usr/local/lib/python3.10/dist-packages (from cobra->-r requirements.txt (line 15)) (1.4.4)\n","Collecting depinfo~=2.2 (from cobra->-r requirements.txt (line 15))\n","  Downloading depinfo-2.2.0-py3-none-any.whl (24 kB)\n","Collecting diskcache~=5.0 (from cobra->-r requirements.txt (line 15))\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx~=0.24 (from cobra->-r requirements.txt (line 15))\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from cobra->-r requirements.txt (line 15)) (6.4.0)\n","Collecting optlang~=1.8 (from cobra->-r requirements.txt (line 15))\n","  Downloading optlang-1.8.1-py2.py3-none-any.whl (142 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic>=1.6 in /usr/local/lib/python3.10/dist-packages (from cobra->-r requirements.txt (line 15)) (2.6.4)\n","Collecting python-libsbml~=5.19 (from cobra->-r requirements.txt (line 15))\n","  Downloading python_libsbml-5.20.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rich>=8.0 in /usr/local/lib/python3.10/dist-packages (from cobra->-r requirements.txt (line 15)) (13.7.1)\n","Collecting ruamel.yaml~=0.16 (from cobra->-r requirements.txt (line 15))\n","  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting swiglpk (from cobra->-r requirements.txt (line 15))\n","  Downloading swiglpk-5.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bioservices->-r requirements.txt (line 18)) (4.12.3)\n","Collecting colorlog (from bioservices->-r requirements.txt (line 18))\n","  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n","Collecting easydev>=0.12.0 (from bioservices->-r requirements.txt (line 18))\n","  Downloading easydev-0.13.1-py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting grequests (from bioservices->-r requirements.txt (line 18))\n","  Downloading grequests-0.7.0-py2.py3-none-any.whl (5.7 kB)\n","Collecting requests_cache (from bioservices->-r requirements.txt (line 18))\n","  Downloading requests_cache-1.2.0-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting suds-community>=0.7 (from bioservices->-r requirements.txt (line 18))\n","  Downloading suds_community-1.1.2-py3-none-any.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.9/144.9 kB\u001b[0m \u001b[31m878.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from bioservices->-r requirements.txt (line 18)) (1.14.1)\n","Collecting xmltodict (from bioservices->-r requirements.txt (line 18))\n","  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n","Collecting starlette<0.38.0,>=0.37.2 (from fastapi->-r requirements.txt (line 19))\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is looking at multiple versions of fastapi to determine which version is compatible with other requirements. This could take a while.\n","Collecting fastapi (from -r requirements.txt (line 19))\n","  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starlette<0.37.0,>=0.36.3 (from fastapi->-r requirements.txt (line 19))\n","  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi (from -r requirements.txt (line 19))\n","  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading fastapi-0.109.1-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starlette<0.36.0,>=0.35.0 (from fastapi->-r requirements.txt (line 19))\n","  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi (from -r requirements.txt (line 19))\n","  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading fastapi-0.108.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starlette<0.33.0,>=0.29.0 (from fastapi->-r requirements.txt (line 19))\n","  Downloading starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi (from -r requirements.txt (line 19))\n","  Downloading fastapi-0.107.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->-r requirements.txt (line 19)) (3.7.1)\n","Collecting starlette<0.29.0,>=0.28.0 (from fastapi->-r requirements.txt (line 19))\n","  Downloading starlette-0.28.0-py3-none-any.whl (68 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi (from -r requirements.txt (line 19))\n","  Downloading fastapi-0.106.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->-r requirements.txt (line 19))\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is looking at multiple versions of fastapi to determine which version is compatible with other requirements. This could take a while.\n","Collecting fastapi (from -r requirements.txt (line 19))\n","  Downloading fastapi-0.105.0-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading fastapi-0.104.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11>=0.8 (from uvicorn->-r requirements.txt (line 22))\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 19)) (3.6)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 19)) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 19)) (1.2.0)\n","Collecting colorama<0.5.0,>=0.4.6 (from easydev>=0.12.0->bioservices->-r requirements.txt (line 18))\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting line-profiler<5.0.0,>=4.1.2 (from easydev>=0.12.0->bioservices->-r requirements.txt (line 18))\n","  Downloading line_profiler-4.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (714 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m714.8/714.8 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pexpect<5.0.0,>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from easydev>=0.12.0->bioservices->-r requirements.txt (line 18)) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx~=0.24->cobra->-r requirements.txt (line 15)) (2024.2.2)\n","Collecting httpcore==1.* (from httpx~=0.24->cobra->-r requirements.txt (line 15))\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.6->cobra->-r requirements.txt (line 15)) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.6->cobra->-r requirements.txt (line 15)) (2.16.3)\n","INFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n","Collecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.7.0-py3-none-any.whl (407 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.9/407.9 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.18.1 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading pydantic-2.6.2-py3-none-any.whl (394 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.8/394.8 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.16.2 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.16.1 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.14.6 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.14.5 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n","Collecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.5.1-py3-none-any.whl (381 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.6/381.6 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.14.3 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.14.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.5.0-py3-none-any.whl (407 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.5/407.5 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.14.1 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.10.1 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.4.1-py3-none-any.whl (395 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.3/395.3 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading pydantic-2.4.0-py3-none-any.whl (395 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.4/395.4 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.10.0 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","Collecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.3.0-py3-none-any.whl (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.6.3 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.2.1-py3-none-any.whl (373 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.4/373.4 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.6.1 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.2.0-py3-none-any.whl (373 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.2/373.2 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.6.0 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.4.0 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.0.3-py3-none-any.whl (364 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.0/364.0 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.3.0 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-2.0.2-py3-none-any.whl (359 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.1/359.1 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic-core==2.1.2 (from pydantic>=1.6->cobra->-r requirements.txt (line 15))\n","  Downloading pydantic_core-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic>=1.6 (from cobra->-r requirements.txt (line 15))\n","  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->zeep->-r requirements.txt (line 14)) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->zeep->-r requirements.txt (line 14)) (2.0.7)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=8.0->cobra->-r requirements.txt (line 15)) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=8.0->cobra->-r requirements.txt (line 15)) (2.16.1)\n","Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml~=0.16->cobra->-r requirements.txt (line 15))\n","  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bioservices->-r requirements.txt (line 18)) (2.5)\n","Collecting gevent (from grequests->bioservices->-r requirements.txt (line 18))\n","  Downloading gevent-24.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (6.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (2.1.5)\n","Collecting cattrs>=22.2 (from requests_cache->bioservices->-r requirements.txt (line 18))\n","  Downloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting url-normalize>=1.4 (from requests_cache->bioservices->-r requirements.txt (line 18))\n","  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=8.0->cobra->-r requirements.txt (line 15)) (0.1.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect<5.0.0,>=4.9.0->easydev>=0.12.0->bioservices->-r requirements.txt (line 18)) (0.7.0)\n","Collecting zope.event (from gevent->grequests->bioservices->-r requirements.txt (line 18))\n","  Downloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n","Collecting zope.interface (from gevent->grequests->bioservices->-r requirements.txt (line 18))\n","  Downloading zope.interface-6.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent->grequests->bioservices->-r requirements.txt (line 18)) (3.0.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.event->gevent->grequests->bioservices->-r requirements.txt (line 18)) (67.7.2)\n","Building wheels for collected packages: pickle5, pubchempy, bioservices\n","  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=255319 sha256=3969200e6df26847392ea3aab0211f98d53e1870fc302f45bab74ffdf572cfc5\n","  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\n","  Building wheel for pubchempy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pubchempy: filename=PubChemPy-1.0.4-py3-none-any.whl size=13820 sha256=816eaa93b1c06d3771feebda4d478b9cfaf4fd43841691b482e359c29e200767\n","  Stored in directory: /root/.cache/pip/wheels/90/7c/45/18a0671e3c3316966ef7ed9ad2b3f3300a7e41d3421a44e799\n","  Building wheel for bioservices (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bioservices: filename=bioservices-1.11.2-py3-none-any.whl size=223232 sha256=24bc97661770c3e0c8f57febdb80245fb4d20d72ca3e430af8c24976275e8492\n","  Stored in directory: /root/.cache/pip/wheels/bf/ac/b3/dc05e53581bbb58641e9ac52428b89d7fc0d9ddc44f9b16456\n","Successfully built pickle5 pubchempy bioservices\n","Installing collected packages: swiglpk, python-libsbml, pubchempy, pickle5, kaleido, fair-esm, zope.interface, zope.event, xmltodict, url-normalize, typing-extensions, triton, suds-community, suds, ruamel.yaml.clib, rdkit-pypi, rdkit, python-multipart, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, line-profiler, isodate, h11, diskcache, depinfo, colorlog, colorama, biopython, uvicorn, starlette, ruamel.yaml, requests-toolbelt, requests-file, pydantic, pandas, optlang, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, gevent, easydev, drfp, cattrs, zeep, requests_cache, nvidia-cusolver-cu12, httpx, grequests, fastapi, torch, cobra, bioservices\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.11.0\n","    Uninstalling typing_extensions-4.11.0:\n","      Successfully uninstalled typing_extensions-4.11.0\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.2.0\n","    Uninstalling triton-2.2.0:\n","      Successfully uninstalled triton-2.2.0\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.6.4\n","    Uninstalling pydantic-2.6.4:\n","      Successfully uninstalled pydantic-2.6.4\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.0.3\n","    Uninstalling pandas-2.0.3:\n","      Successfully uninstalled pandas-2.0.3\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.2.1+cu121\n","    Uninstalling torch-2.2.1+cu121:\n","      Successfully uninstalled torch-2.2.1+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sqlalchemy 2.0.29 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\n","pydantic-core 2.16.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n","torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n","torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n","torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed biopython-1.83 bioservices-1.11.2 cattrs-23.2.3 cobra-0.29.0 colorama-0.4.6 colorlog-6.8.2 depinfo-2.2.0 diskcache-5.6.3 drfp-0.3.6 easydev-0.13.1 fair-esm-2.0.0 fastapi-0.103.2 gevent-24.2.1 grequests-0.7.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 isodate-0.6.1 kaleido-0.2.1 line-profiler-4.1.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 optlang-1.8.1 pandas-1.5.3 pickle5-0.0.11 pubchempy-1.0.4 pydantic-1.10.15 python-libsbml-5.20.2 python-multipart-0.0.9 rdkit-2023.9.5 rdkit-pypi-2022.9.5 requests-file-2.0.0 requests-toolbelt-1.0.0 requests_cache-1.2.0 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 starlette-0.27.0 suds-1.1.2 suds-community-1.1.2 swiglpk-5.0.10 torch-2.1.2 triton-2.1.0 typing-extensions-4.5.0 url-normalize-1.4.3 uvicorn-0.29.0 xmltodict-0.13.0 zeep-4.2.1 zope.event-5.0 zope.interface-6.3\n"]}],"source":["%cd /content/drive/MyDrive/enz-eff-project\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1713148457306,"user":{"displayName":"M Faizan","userId":"01979528019072830709"},"user_tz":-300},"id":"8kW5ulYTp3jI","outputId":"69422d0d-1a3c-409c-d72a-9f34e2d4d591"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1iS6gSWfUE3cZnmrNWbbV9_W_zQH3vaiu/enz-eff-project/improved_code/model_training\n"]}],"source":["%cd improved_code/model_training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Syu1B0UTXhfz"},"outputs":[],"source":["import numpy as np\n","import pickle\n","import pandas as pd\n","from os.path import join\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from sklearn.metrics import r2_score, mean_squared_error\n","from scipy import stats\n","import xgboost as xgb"]},{"cell_type":"markdown","metadata":{"id":"1lcatv5AXhf1"},"source":["## Loading training and test data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezh_o6RtXhf2"},"outputs":[],"source":["train_indices = list(np.load(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"CV_train_indices.npy\"), allow_pickle = True))\n","test_indices = list(np.load(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"CV_test_indices.npy\"), allow_pickle = True))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhmBXogHXhf2","executionInfo":{"status":"ok","timestamp":1713148470582,"user_tz":-300,"elapsed":8888,"user":{"displayName":"M Faizan","userId":"01979528019072830709"}},"outputId":"ef26f59f-cfc2-4833-8c54-79792c265f8a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3391, 874)"]},"metadata":{},"execution_count":6}],"source":["data_train = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"train_df_kcat_new_feats.pkl\"))\n","data_test = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"test_df_kcat_new_feats.pkl\"))\n","\n","# print((train_indices))\n","# print((test_indices))\n","\n","data_train.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n","data_test.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n","len(data_train), len(data_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzozZCL6Xhf5","executionInfo":{"status":"ok","timestamp":1713148470582,"user_tz":-300,"elapsed":25,"user":{"displayName":"M Faizan","userId":"01979528019072830709"}},"outputId":"abe98577-183a-4916-9543-4ca31835ac3e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Reaction ID', 'Sequence ID', 'kcat_values', 'Uniprot IDs',\n","       'from_BRENDA', 'from_Sabio', 'from_Uniprot', 'checked', 'Sequence',\n","       'substrates',\n","       ...\n","       'socn_normalized', 'qso_combined', 'qso_normalized', 'traid_combined',\n","       'traid_normalized', 'paac_combined', 'paac_normalized', 'ESM1b_norm',\n","       'ESM1b_ts_norm', 'log10_kcat_norm'],\n","      dtype='object', length=2302)"]},"metadata":{},"execution_count":7}],"source":["data_train.columns"]},{"cell_type":"markdown","metadata":{"id":"kwpfiD6oXhf6"},"source":["**Saving Xgboost Models**"]},{"cell_type":"code","source":["%ls ../../models/train_models"],"metadata":{"id":"RDdtERk8YSB9","executionInfo":{"status":"ok","timestamp":1713148471317,"user_tz":-300,"elapsed":753,"user":{"displayName":"M Faizan","userId":"01979528019072830709"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"70644127-e44e-4cf1-e5f5-8fe4ae2d037b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" difference_fp.h5  'esm1bts_drfp (1).h5'   xgboost_diff.h5            xgboost_esm1bts_drfp.h5\n"," drfp.h5            esm1bts_drfp.h5        xgboost_drfp.h5            xgboost_esm1b_ts.h5\n"," esm1b.h5           esm1bts.h5             xgboost_esm1b.h5           xgboost_structFp.h5\n"," esm1bts_diff.h5    structural_fp.h5       xgboost_esm1b_ts_diff.h5\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAg3OKsoXhf6"},"outputs":[],"source":["def save_pickel_model(model, model_name):\n","    xgboost_trained_models = \"../../models/train_models\"\n","    model_path = join(xgboost_trained_models, model_name)\n","    with open(model_path, 'wb') as model_file:\n","        pickle.dump(model, model_file)"]},{"cell_type":"markdown","metadata":{"id":"UfKM8sY_Xhf7"},"source":["## 1. Training a model with only sequence information (ESM-1b):"]},{"cell_type":"markdown","metadata":{"id":"tPzC-PUAXhf7"},"source":["#### (a) Creating input matrices:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFQtuNINXhf8"},"outputs":[],"source":["train_ESM1b = np.array(list(data_train[\"ESM1b\"]))\n","train_X = train_ESM1b\n","train_Y = np.array(list(data_train[\"log10_kcat\"]))\n","\n","test_ESM1b = np.array(list(data_test[\"ESM1b\"]))\n","test_X = test_ESM1b\n","test_Y = np.array(list(data_test[\"log10_kcat\"]))"]},{"cell_type":"code","source":["for i in range(5):\n","    train_index, test_index  = train_indices[i], test_indices[i]\n","    print(i)\n","    print(train_X[test_index])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRk2iXqUwDZ8","executionInfo":{"status":"ok","timestamp":1713153760075,"user_tz":-300,"elapsed":455,"user":{"displayName":"M Faizan","userId":"01979528019072830709"}},"outputId":"f701596d-16c7-4eb8-d15b-4b2e86546a08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","[[ 0.07964781  0.16702731  0.22767723 ... -0.10863274 -0.14281555\n","   0.14292285]\n"," [ 0.07430488  0.17219836 -0.01209073 ... -0.13870294 -0.11353551\n","   0.09485441]\n"," [-0.01462269  0.18103217 -0.00500855 ... -0.10428829 -0.05840648\n","   0.01361028]\n"," ...\n"," [-0.02755747  0.18008837 -0.03818262 ... -0.03201058 -0.03821661\n","   0.08363783]\n"," [ 0.16225535  0.27029133 -0.03522961 ... -0.07823634 -0.19223613\n","   0.00614813]\n"," [ 0.05414189  0.19421057 -0.05987677 ... -0.06891491 -0.16208781\n","   0.05734136]]\n","1\n","[[ 0.04166316  0.181561    0.07668334 ...  0.19656172 -0.13912867\n","   0.03144771]\n"," [ 0.1302101   0.24495757 -0.02362511 ... -0.00437272 -0.07560533\n","   0.03059028]\n"," [-0.06354112 -0.01563576 -0.06732959 ... -0.20669037 -0.00257164\n","  -0.03227103]\n"," ...\n"," [-0.00988316  0.13642907  0.08164488 ... -0.08296695  0.02208484\n","   0.00863433]\n"," [ 0.03321628  0.13176644 -0.01363077 ... -0.12953989 -0.02000922\n","   0.03607321]\n"," [ 0.04347761  0.12669434  0.01724108 ... -0.04599944 -0.04682548\n","   0.17133021]]\n","2\n","[[ 0.09489869  0.03850541  0.06174522 ... -0.10172571  0.04139237\n","   0.14651003]\n"," [ 0.01087973  0.21497154 -0.01182905 ... -0.00380631 -0.10795988\n","   0.03740551]\n"," [ 0.07669757  0.30792984  0.02893904 ... -0.12819555 -0.09891498\n","   0.15475316]\n"," ...\n"," [ 0.12094393  0.20587547 -0.21497132 ... -0.1054042   0.01046101\n","   0.07623293]\n"," [-0.02555367  0.17914419  0.15986805 ... -0.00546745  0.02212811\n","   0.04040845]\n"," [ 0.06679008  0.06933105  0.06523842 ... -0.02516524 -0.02844398\n","   0.06462974]]\n","3\n","[[ 0.04646186  0.14093406  0.02405289 ... -0.04924423 -0.08713297\n","   0.06608839]\n"," [-0.04858899  0.21096128 -0.1015213  ... -0.05698982 -0.11231375\n","   0.1398341 ]\n"," [-0.00155085  0.22776552  0.17977771 ... -0.07464918  0.02437625\n","   0.09475537]\n"," ...\n"," [-0.01515534  0.25871485  0.11286816 ... -0.0107566  -0.14827766\n","  -0.00687186]\n"," [ 0.03752532  0.36062133 -0.10130832 ... -0.02339658  0.03045405\n","   0.1309979 ]\n"," [ 0.04387357  0.17853488  0.04556957 ... -0.04064966 -0.04915055\n","   0.18645096]]\n","4\n","[[ 0.0657595   0.3598805  -0.15595236 ...  0.0011054  -0.03071124\n","   0.11408781]\n"," [ 0.04752294  0.21256794  0.08701975 ... -0.02547193  0.01711247\n","   0.02951541]\n"," [-0.03838312  0.24037075 -0.02724026 ... -0.05700446  0.00666462\n","   0.01708197]\n"," ...\n"," [ 0.14140767  0.13103117  0.00128061 ... -0.1002517  -0.08140564\n","  -0.1562538 ]\n"," [ 0.08167847  0.16345303 -0.03819959 ... -0.04545181 -0.05787089\n","  -0.00693049]\n"," [-0.04333702  0.09611035 -0.04730722 ... -0.18820457 -0.13896686\n","   0.08363543]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"RNZGFFDiXhf8"},"source":["#### (b) Hyperparameter optimization:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Kgk5v78EXhf8","executionInfo":{"status":"error","timestamp":1713148636487,"user_tz":-300,"elapsed":707,"user":{"displayName":"M Faizan","userId":"01979528019072830709"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e0e6049d-f0a5-47c6-b74b-efcc01d42d31"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 6, 2576, 11, 2581, 2584, 16, 21, 2590, 26, 2596, 31, 2602, 2603, 36, 41, 46, 2614, 51, 2621, 2625, 57, 2626, 2628, 62, 2632, 2633, 2631, 67, 2636, 2638, 72, 2641, 77, 2647, 82, 87, 2658, 2661, 92, 2663, 97, 102, 2676, 2674, 107, 2679, 112, 2684, 117, 2689, 2693, 122, 2698, 2697, 127, 2703, 2702, 132, 2706, 2709, 137, 142, 2715, 2719, 147, 152, 2726, 2731, 157, 161, 162, 2736, 167, 2742, 2747, 172, 2748, 177, 2758, 187, 2764, 192, 2768, 197, 2773, 2778, 202, 207, 2782, 211, 216, 2792, 2794, 221, 2797, 224, 227, 2803, 229, 232, 2808, 2813, 2816, 242, 2819, 2823, 247, 2824, 249, 252, 2833, 257, 2834, 261, 2837, 263, 266, 268, 2844, 273, 274, 2848, 277, 2852, 282, 2859, 2857, 2863, 2864, 292, 2868, 297, 2873, 303, 2878, 308, 2884, 313, 2889, 2892, 2896, 2897, 323, 2901, 2909, 332, 2912, 337, 2924, 346, 2931, 2932, 356, 2939, 361, 362, 2944, 367, 2951, 2952, 373, 2953, 378, 379, 2963, 2968, 389, 2973, 2974, 399, 2978, 406, 411, 2992, 418, 2994, 2998, 422, 2999, 428, 3003, 433, 3011, 439, 444, 449, 3027, 3026, 3029, 454, 3034, 459, 464, 3039, 3044, 477, 3052, 482, 3057, 487, 3062, 490, 3065, 3066, 3068, 496, 3071, 501, 3077, 3081, 506, 3086, 510, 3088, 3091, 511, 3089, 517, 3100, 522, 3105, 3109, 532, 537, 538, 3114, 542, 547, 3123, 552, 3129, 557, 562, 3139, 566, 567, 572, 3148, 576, 3152, 581, 584, 586, 3166, 594, 596, 601, 3180, 606, 3182, 3181, 611, 3185, 3184, 616, 3188, 618, 622, 3193, 627, 3198, 633, 3203, 636, 638, 3208, 3214, 643, 648, 3222, 3223, 653, 658, 3229, 3228, 662, 3233, 668, 3236, 672, 677, 3246, 679, 3248, 681, 3251, 3256, 685, 3257, 689, 3262, 692, 694, 3266, 699, 704, 3276, 709, 711, 716, 3287, 3289, 3295, 3296, 728, 3299, 732, 3301, 3306, 3309, 742, 3312, 3318, 746, 3322, 756, 3327, 761, 3333, 764, 3338, 771, 3343, 774, 3351, 3353, 781, 3358, 783, 3360, 786, 3366, 3368, 794, 797, 799, 3379, 3383, 804, 3387, 3384, 808, 819, 824, 829, 834, 838, 844, 849, 851, 852, 857, 863, 866, 867, 877, 883, 886, 888, 891, 893, 896, 902, 907, 910, 911, 916, 921, 932, 938, 942, 948, 959, 963, 964, 969, 974, 978, 983, 989, 996, 1000, 1004, 1011, 1014, 1017, 1018, 1019, 1022, 1023, 1033, 1038, 1042, 1048, 1053, 1058, 1063, 1068, 1073, 1077, 1080, 1081, 1086, 1088, 1089, 1092, 1097, 1101, 1106, 1111, 1119, 1124, 1129, 1134, 1137, 1139, 1143, 1144, 1145, 1148, 1153, 1158, 1163, 1173, 1178, 1183, 1187, 1188, 1189, 1193, 1199, 1203, 1208, 1211, 1212, 1214, 1217, 1218, 1222, 1223, 1227, 1232, 1236, 1244, 1248, 1252, 1258, 1263, 1265, 1267, 1272, 1277, 1287, 1293, 1294, 1298, 1299, 1305, 1307, 1312, 1317, 1322, 1328, 1332, 1333, 1344, 1347, 1351, 1357, 1359, 1361, 1364, 1366, 1373, 1376, 1380, 1389, 1396, 1399, 1402, 1407, 1411, 1416, 1421, 1426, 1431, 1436, 1438, 1441, 1446, 1451, 1459, 1463, 1471, 1473, 1474, 1477, 1482, 1484, 1488, 1493, 1502, 1507, 1512, 1515, 1516, 1519, 1521, 1526, 1532, 1533, 1537, 1542, 1547, 1552, 1553, 1556, 1561, 1562, 1569, 1571, 1576, 1580, 1584, 1586, 1587, 1591, 1599, 1604, 1608, 1609, 1610, 1613, 1618, 1622, 1627, 1629, 1633, 1639, 1642, 1649, 1654, 1659, 1664, 1669, 1672, 1674, 1676, 1679, 1682, 1683, 1684, 1687, 1692, 1716, 1721, 1734, 1739, 1744, 1745, 1749, 1751, 1759, 1764, 1768, 1773, 1775, 1777, 1782, 1783, 1792, 1801, 1805, 1811, 1821, 1822, 1827, 1829, 1836, 1844, 1846, 1847, 1851, 1860, 1866, 1868, 1874, 1876, 1884, 1886, 1891, 1894, 1896, 1900, 1904, 1908, 1910, 1918, 1919, 1924, 1932, 1942, 1948, 1953, 1958, 1959, 1963, 1966, 1969, 1970, 1974, 1978, 1983, 1987, 1988, 1989, 1990, 1992, 1997, 1998, 2003, 2009, 2011, 2014, 2016, 2027, 2029, 2049, 2056, 2066, 2071, 2075, 2076, 2077, 2079, 2083, 2089, 2090, 2098, 2103, 2104, 2105, 2106, 2107, 2112, 2117, 2118, 2122, 2123, 2128, 2133, 2137, 2138, 2143, 2148, 2152, 2156, 2161, 2166, 2172, 2176, 2178, 2182, 2185, 2187, 2192, 2198, 2201, 2204, 2208, 2209, 2213, 2214, 2219, 2222, 2226, 2227, 2234, 2246, 2254, 2258, 2263, 2265, 2267, 2273, 2282, 2283, 2284, 2287, 2291, 2299, 2302, 2309, 2316, 2319, 2321, 2326, 2329, 2331, 2335, 2344, 2351, 2379, 2381, 2383, 2385, 2387, 2393, 2399, 2408, 2413, 2414, 2417, 2419, 2421, 2426, 2427, 2430, 2433, 2452, 2456, 2461, 2463, 2471, 2476, 2479, 2485, 2486, 2487, 2491, 2500, 2504, 2507, 2509, 2511, 2513, 2528, 2536, 2538, 2546, 2552, 2558, 2567, 2, 7, 12, 17, 22, 27, 32, 37, 42, 47, 52, 58, 63, 68, 73, 78, 83, 88, 93, 98, 103, 108, 113, 118, 123, 128, 133, 138, 143, 148, 153, 158, 164, 169, 171, 173, 174, 179, 182, 183, 188, 193, 198, 203, 208, 212, 217, 223, 231, 236, 237, 239, 244, 251, 256, 264, 271, 281, 286, 289, 294, 299, 306, 311, 316, 319, 324, 331, 336, 341, 344, 348, 352, 357, 363, 364, 371, 376, 382, 386, 393, 396, 401, 407, 414, 419, 423, 429, 434, 441, 451, 456, 457, 461, 466, 476, 478, 481, 486, 491, 492, 494, 499, 504, 512, 518, 523, 531, 536, 543, 548, 551, 553, 558, 563, 569, 574, 578, 589, 593, 599, 604, 609, 614, 623, 634, 641, 646, 651, 656, 666, 674, 684, 688, 696, 701, 706, 713, 721, 724, 729, 730, 733, 737, 741, 744, 745, 749, 753, 758, 763, 768, 772, 773, 776, 778, 784, 789, 792, 796, 803, 805, 807, 812, 817, 822, 827, 832, 836, 842, 847, 858, 864, 872, 873, 876, 878, 882, 887, 894, 901, 906, 912, 917, 923, 927, 931, 941, 946, 952, 957, 962, 968, 973, 977, 982, 986, 988, 994, 999, 1003, 1009, 1021, 1032, 1037, 1040, 1041, 1047, 1052, 1061, 1062, 1067, 1072, 1075, 1076, 1082, 1091, 1095, 1104, 1108, 1109, 1117, 1123, 1128, 1133, 1141, 1147, 1152, 1157, 1162, 1167, 1171, 1181, 1186, 1194, 1204, 1209, 1216, 1224, 1229, 1234, 1238, 1240, 1241, 1242, 1245, 1246, 1249, 1250, 1254, 1264, 1274, 1279, 1283, 1289, 1296, 1301, 1302, 1311, 1316, 1321, 1327, 1334, 1342, 1346, 1356, 1367, 1368, 1372, 1377, 1378, 1382, 1383, 1386, 1393, 1397, 1404, 1406, 1413, 1418, 1422, 1428, 1444, 1450, 1454, 1457, 1462, 1467, 1469, 1486, 1491, 1498, 1503, 1508, 1513, 1518, 1523, 1524, 1531, 1538, 1543, 1548, 1554, 1555, 1558, 1564, 1574, 1579, 1588, 1589, 1593, 1595, 1596, 1598, 1600, 1606, 1612, 1617, 1621, 1634, 1645, 1646, 1651, 1653, 1656, 1661, 1665, 1671, 1678, 1680, 1694, 1697, 1698, 1702, 1705, 1708, 1712, 1717, 1719, 1722, 1729, 1737, 1754, 1758, 1767, 1772, 1778, 1786, 1787, 1789, 1790, 1793, 1804, 1809, 1812, 1814, 1819, 1834, 1841, 1852, 1858, 1861, 1864, 1878, 1882, 1885, 1887, 1898, 1906, 1909, 1913, 1917, 1927, 1928, 1931, 1937, 1943, 1949, 1964, 1965, 1968, 1979, 1982, 1984, 1993, 1999, 2006, 2008, 2013, 2031, 2034, 2039, 2041, 2043, 2046, 2047, 2054, 2057, 2058, 2061, 2064, 2069, 2074, 2080, 2087, 2091, 2092, 2096, 2100, 2101, 2109, 2114, 2124, 2129, 2134, 2135, 2141, 2146, 2154, 2163, 2169, 2174, 2179, 2189, 2193, 2194, 2202, 2211, 2216, 2221, 2229, 2231, 2233, 2241, 2242, 2247, 2253, 2255, 2257, 2262, 2268, 2270, 2275, 2279, 2289, 2293, 2297, 2298, 2307, 2313, 2318, 2323, 2332, 2336, 2339, 2343, 2349, 2356, 2357, 2361, 2364, 2366, 2371, 2374, 2377, 2382, 2388, 2401, 2407, 2429, 2436, 2443, 2462, 2464, 2465, 2466, 2472, 2473, 2475, 2496, 2502, 2516, 2517, 2532, 2533, 2544, 2556, 2563, 2570, 2572, 2577, 2587, 2591, 2597, 2604, 2605, 2606, 2610, 2619, 2623, 2627, 2637, 2643, 2651, 2659, 2666, 2669, 2673, 2677, 2682, 2687, 2696, 2704, 2708, 2713, 2717, 2718, 2720, 2722, 2728, 2733, 2746, 2749, 2752, 2753, 2756, 2766, 2771, 2780, 2781, 2784, 2788, 2789, 2793, 2806, 2810, 2818, 2825, 2827, 2831, 2836, 2838, 2841, 2849, 2850, 2861, 2867, 2869, 2870, 2871, 2877, 2883, 2898, 2908, 2916, 2919, 2926, 2928, 2936, 2938, 2941, 2945, 2947, 2956, 2957, 2959, 2981, 2982, 2985, 2991, 2996, 3001, 3007, 3010, 3017, 3018, 3024, 3032, 3036, 3038, 3041, 3053, 3058, 3059, 3063, 3069, 3076, 3087, 3093, 3098, 3103, 3107, 3112, 3115, 3116, 3124, 3130, 3134, 3137, 3141, 3143, 3146, 3150, 3156, 3159, 3161, 3163, 3164, 3165, 3177, 3189, 3194, 3204, 3209, 3216, 3219, 3224, 3234, 3242, 3247, 3252, 3254, 3259, 3270, 3273, 3279, 3292, 3293, 3314, 3316, 3319, 3323, 3326, 3328, 3334, 3339, 3341, 3346, 3356, 3357, 3364, 3372, 3374, 3377, 3378, 3382, 3, 8, 13, 18, 23, 28, 33, 38, 43, 48, 54, 59, 64, 69, 74, 79, 84, 89, 94, 99, 104, 109, 114, 119, 124, 129, 134, 139, 144, 149, 154, 159, 166, 176, 181, 186, 191, 196, 201, 206, 214, 219, 228, 234, 241, 246, 253, 258, 267, 272, 275, 276, 279, 284, 288, 293, 298, 304, 309, 314, 318, 322, 326, 327, 328, 333, 338, 345, 349, 359, 368, 374, 381, 383, 384, 391, 397, 398, 404, 409, 417, 421, 432, 437, 443, 447, 452, 458, 463, 468, 471, 479, 484, 489, 497, 502, 507, 514, 519, 524, 529, 534, 541, 546, 554, 559, 564, 571, 579, 583, 588, 598, 603, 608, 613, 626, 630, 637, 642, 647, 657, 660, 663, 669, 671, 676, 680, 683, 687, 693, 698, 707, 708, 717, 723, 727, 734, 738, 743, 748, 751, 752, 757, 762, 766, 767, 769, 777, 782, 788, 790, 791, 802, 809, 818, 823, 828, 833, 835, 837, 843, 848, 859, 869, 871, 874, 881, 892, 899, 904, 909, 919, 926, 929, 936, 937, 939, 940, 943, 949, 958, 966, 971, 975, 979, 987, 993, 998, 1002, 1008, 1012, 1013, 1026, 1034, 1039, 1051, 1056, 1057, 1059, 1064, 1066, 1071, 1078, 1079, 1083, 1093, 1098, 1100, 1103, 1107, 1118, 1126, 1127, 1131, 1136, 1142, 1149, 1154, 1159, 1164, 1166, 1168, 1172, 1179, 1184, 1192, 1201, 1206, 1213, 1219, 1221, 1226, 1231, 1235, 1239, 1243, 1247, 1256, 1262, 1268, 1271, 1276, 1281, 1282, 1284, 1291, 1297, 1304, 1306, 1308, 1313, 1318, 1324, 1329, 1336, 1338, 1341, 1345, 1349, 1353, 1358, 1362, 1369, 1371, 1374, 1381, 1387, 1394, 1398, 1408, 1409, 1414, 1420, 1424, 1429, 1433, 1434, 1437, 1442, 1447, 1452, 1455, 1456, 1458, 1472, 1479, 1487, 1492, 1501, 1506, 1511, 1517, 1527, 1528, 1534, 1539, 1544, 1557, 1563, 1567, 1573, 1575, 1577, 1581, 1592, 1597, 1603, 1611, 1616, 1620, 1624, 1628, 1636, 1641, 1647, 1663, 1677, 1685, 1688, 1689, 1691, 1693, 1696, 1701, 1704, 1706, 1707, 1711, 1714, 1723, 1726, 1730, 1731, 1738, 1748, 1753, 1756, 1761, 1763, 1766, 1771, 1776, 1784, 1791, 1797, 1799, 1803, 1823, 1830, 1831, 1832, 1837, 1854, 1855, 1856, 1863, 1869, 1872, 1873, 1879, 1883, 1888, 1892, 1899, 1902, 1921, 1923, 1930, 1936, 1938, 1947, 1952, 1954, 1956, 1967, 1972, 1973, 1976, 1980, 1991, 1996, 2017, 2024, 2026, 2032, 2033, 2042, 2044, 2048, 2052, 2062, 2072, 2073, 2078, 2081, 2082, 2088, 2094, 2102, 2111, 2119, 2131, 2139, 2142, 2144, 2149, 2150, 2151, 2153, 2155, 2157, 2158, 2159, 2162, 2167, 2173, 2177, 2197, 2203, 2206, 2207, 2217, 2223, 2232, 2236, 2237, 2238, 2244, 2250, 2251, 2256, 2261, 2266, 2271, 2274, 2276, 2277, 2288, 2290, 2308, 2314, 2322, 2327, 2333, 2337, 2341, 2342, 2347, 2353, 2359, 2367, 2372, 2378, 2380, 2384, 2391, 2397, 2409, 2415, 2416, 2428, 2434, 2439, 2440, 2441, 2444, 2446, 2447, 2449, 2451, 2457, 2459, 2469, 2477, 2481, 2483, 2492, 2495, 2497, 2501, 2506, 2512, 2519, 2524, 2527, 2541, 2543, 2549, 2551, 2559, 2562, 2565, 2568, 2574, 2580, 2582, 2583, 2585, 2588, 2589, 2592, 2598, 2600, 2608, 2609, 2612, 2616, 2622, 2642, 2648, 2654, 2655, 2662, 2667, 2668, 2672, 2686, 2691, 2705, 2707, 2712, 2721, 2724, 2732, 2739, 2741, 2751, 2769, 2774, 2777, 2783, 2796, 2804, 2817, 2821, 2822, 2828, 2842, 2843, 2846, 2851, 2853, 2855, 2862, 2874, 2879, 2886, 2888, 2902, 2904, 2906, 2911, 2913, 2914, 2921, 2922, 2933, 2937, 2948, 2954, 2958, 2961, 2964, 2967, 2971, 2979, 2983, 2984, 2987, 2997, 3002, 3008, 3012, 3014, 3016, 3019, 3022, 3028, 3031, 3035, 3037, 3043, 3046, 3048, 3054, 3056, 3061, 3067, 3073, 3079, 3083, 3092, 3097, 3102, 3106, 3108, 3111, 3117, 3118, 3122, 3127, 3133, 3136, 3147, 3151, 3157, 3160, 3167, 3170, 3172, 3174, 3178, 3187, 3192, 3197, 3199, 3206, 3217, 3220, 3225, 3230, 3232, 3235, 3237, 3238, 3239, 3243, 3249, 3265, 3267, 3271, 3272, 3274, 3282, 3283, 3288, 3291, 3297, 3302, 3304, 3311, 3324, 3331, 3337, 3347, 3349, 3359, 3369, 3376, 3388, 3389, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 56, 61, 66, 71, 76, 81, 86, 91, 96, 101, 106, 111, 116, 121, 126, 131, 136, 141, 146, 151, 156, 163, 168, 178, 184, 189, 194, 199, 204, 209, 210, 213, 218, 226, 233, 238, 243, 248, 254, 262, 269, 278, 283, 287, 291, 296, 302, 307, 312, 317, 321, 325, 329, 334, 339, 342, 343, 347, 351, 353, 354, 366, 372, 377, 387, 394, 395, 403, 408, 416, 420, 426, 427, 431, 436, 442, 446, 448, 453, 462, 467, 469, 472, 483, 488, 493, 498, 503, 508, 516, 521, 526, 527, 528, 533, 539, 544, 549, 556, 561, 568, 573, 575, 577, 582, 587, 591, 592, 597, 602, 607, 612, 617, 621, 624, 628, 632, 639, 644, 649, 652, 654, 659, 661, 667, 670, 673, 678, 682, 686, 691, 697, 702, 703, 714, 722, 726, 731, 736, 739, 747, 754, 759, 779, 787, 793, 795, 801, 806, 811, 814, 816, 821, 826, 831, 841, 846, 854, 861, 865, 868, 879, 884, 885, 889, 898, 903, 908, 913, 914, 918, 924, 928, 933, 944, 951, 953, 954, 961, 967, 972, 976, 981, 991, 997, 1001, 1007, 1024, 1027, 1028, 1031, 1036, 1043, 1045, 1049, 1054, 1070, 1074, 1084, 1094, 1099, 1102, 1114, 1116, 1121, 1132, 1138, 1146, 1151, 1156, 1161, 1169, 1174, 1176, 1177, 1182, 1191, 1196, 1197, 1198, 1202, 1207, 1210, 1228, 1233, 1237, 1251, 1257, 1259, 1261, 1266, 1269, 1273, 1278, 1286, 1292, 1303, 1309, 1314, 1319, 1326, 1331, 1337, 1339, 1343, 1348, 1352, 1354, 1360, 1363, 1379, 1384, 1388, 1395, 1403, 1410, 1412, 1417, 1423, 1427, 1432, 1439, 1443, 1448, 1453, 1461, 1464, 1466, 1478, 1481, 1489, 1494, 1495, 1496, 1497, 1499, 1504, 1509, 1514, 1522, 1536, 1541, 1546, 1549, 1551, 1559, 1566, 1568, 1578, 1583, 1594, 1601, 1607, 1614, 1619, 1623, 1626, 1631, 1637, 1643, 1648, 1652, 1657, 1658, 1662, 1666, 1668, 1673, 1681, 1686, 1699, 1703, 1709, 1713, 1718, 1724, 1727, 1728, 1732, 1733, 1736, 1741, 1746, 1747, 1752, 1755, 1757, 1762, 1765, 1769, 1774, 1781, 1788, 1794, 1795, 1796, 1798, 1802, 1806, 1808, 1813, 1816, 1818, 1824, 1826, 1828, 1833, 1839, 1842, 1843, 1848, 1849, 1850, 1853, 1857, 1867, 1871, 1881, 1889, 1895, 1897, 1901, 1903, 1905, 1907, 1914, 1916, 1922, 1925, 1926, 1929, 1933, 1934, 1939, 1941, 1944, 1951, 1957, 1971, 1977, 1986, 1994, 2001, 2004, 2012, 2021, 2022, 2036, 2037, 2045, 2051, 2059, 2063, 2067, 2068, 2086, 2093, 2099, 2108, 2113, 2121, 2126, 2127, 2132, 2147, 2164, 2171, 2175, 2183, 2184, 2191, 2199, 2212, 2218, 2228, 2239, 2243, 2248, 2249, 2252, 2259, 2264, 2269, 2272, 2286, 2292, 2294, 2296, 2304, 2311, 2317, 2324, 2328, 2334, 2338, 2348, 2354, 2362, 2369, 2373, 2375, 2376, 2386, 2392, 2394, 2396, 2402, 2403, 2404, 2405, 2406, 2411, 2412, 2418, 2420, 2422, 2423, 2424, 2431, 2438, 2442, 2448, 2450, 2454, 2458, 2467, 2474, 2478, 2482, 2489, 2493, 2494, 2498, 2503, 2510, 2518, 2521, 2526, 2529, 2537, 2539, 2542, 2548, 2553, 2554, 2561, 2564, 2566, 2569, 2571, 2573, 2579, 2593, 2599, 2611, 2613, 2617, 2629, 2634, 2639, 2646, 2652, 2656, 2657, 2664, 2678, 2681, 2683, 2688, 2699, 2701, 2710, 2714, 2723, 2729, 2734, 2740, 2743, 2754, 2757, 2759, 2762, 2763, 2765, 2767, 2772, 2776, 2779, 2786, 2787, 2791, 2799, 2801, 2802, 2807, 2811, 2812, 2826, 2832, 2839, 2845, 2847, 2856, 2866, 2876, 2882, 2887, 2891, 2894, 2899, 2907, 2923, 2927, 2929, 2934, 2942, 2943, 2946, 2949, 2962, 2966, 2969, 2972, 2976, 2977, 2986, 2988, 3000, 3004, 3021, 3023, 3033, 3042, 3049, 3064, 3072, 3078, 3094, 3099, 3104, 3113, 3119, 3121, 3126, 3131, 3135, 3142, 3144, 3145, 3149, 3153, 3158, 3162, 3168, 3169, 3171, 3176, 3179, 3183, 3186, 3191, 3196, 3201, 3202, 3207, 3211, 3212, 3218, 3221, 3227, 3231, 3241, 3244, 3258, 3261, 3263, 3264, 3268, 3277, 3281, 3284, 3286, 3294, 3298, 3303, 3307, 3317, 3320, 3321, 3329, 3336, 3342, 3361, 3362, 3363, 3371, 3381]\n","  0%|          | 0/200 [00:00<?, ?trial/s, best loss=?]"]},{"output_type":"stream","name":"stderr","text":["ERROR:hyperopt.fmin:job exception: [02:37:15] /workspace/src/tree/updater_gpu_hist.cu:781: Exception in gpu_hist: [02:37:15] /workspace/src/tree/updater_gpu_hist.cu:787: Check failed: ctx_->gpu_id >= 0 (-1 vs. 0) : Must have at least one device\n","Stack trace:\n","  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7ea6fdc26f2a]\n","  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb3e95a) [0x7ea6fdc3d95a]\n","  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb483cd) [0x7ea6fdc473cd]\n","  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7ea6fd55fc79]\n","  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7ea6fd56076c]\n","  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7ea6fd5c44f7]\n","  [bt] (6) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7ea6fd260ef0]\n","  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7ea765f49e2e]\n","  [bt] (8) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7ea765f46493]\n","\n","\n","\n","Stack trace:\n","  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7ea6fdc26f2a]\n","  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb485c9) [0x7ea6fdc475c9]\n","  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7ea6fd55fc79]\n","  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7ea6fd56076c]\n","  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7ea6fd5c44f7]\n","  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7ea6fd260ef0]\n","  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7ea765f49e2e]\n","  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7ea765f46493]\n","  [bt] (8) /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0xa3e9) [0x7ea765f6f3e9]\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\r  0%|          | 0/200 [00:00<?, ?trial/s, best loss=?]\n"]},{"output_type":"error","ename":"XGBoostError","evalue":"[02:37:15] /workspace/src/tree/updater_gpu_hist.cu:781: Exception in gpu_hist: [02:37:15] /workspace/src/tree/updater_gpu_hist.cu:787: Check failed: ctx_->gpu_id >= 0 (-1 vs. 0) : Must have at least one device\nStack trace:\n  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7ea6fdc26f2a]\n  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb3e95a) [0x7ea6fdc3d95a]\n  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb483cd) [0x7ea6fdc473cd]\n  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7ea6fd55fc79]\n  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7ea6fd56076c]\n  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7ea6fd5c44f7]\n  [bt] (6) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7ea6fd260ef0]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7ea765f49e2e]\n  [bt] (8) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7ea765f46493]\n\n\n\nStack trace:\n  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7ea6fdc26f2a]\n  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb485c9) [0x7ea6fdc475c9]\n  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7ea6fd55fc79]\n  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7ea6fd56076c]\n  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7ea6fd5c44f7]\n  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7ea6fd260ef0]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7ea765f49e2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7ea765f46493]\n  [bt] (8) /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0xa3e9) [0x7ea765f6f3e9]\n\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-e23c0ec8c08c>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n\u001b[0m\u001b[1;32m     37\u001b[0m             algo=rand.suggest, max_evals = 200, trials=trials)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mallow_trials_fmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fmin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         return trials.fmin(\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         return fmin(\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             )\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-e23c0ec8c08c>\u001b[0m in \u001b[0;36mcross_validation_mse_gradient_boosting\u001b[0;34m(param)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0my_valid_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mMSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_valid_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2050\u001b[0;31m             _check_call(\n\u001b[0m\u001b[1;32m   2051\u001b[0m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \"\"\"\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mXGBoostError\u001b[0m: [02:37:15] /workspace/src/tree/updater_gpu_hist.cu:781: Exception in gpu_hist: [02:37:15] /workspace/src/tree/updater_gpu_hist.cu:787: Check failed: ctx_->gpu_id >= 0 (-1 vs. 0) : Must have at least one device\nStack trace:\n  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7ea6fdc26f2a]\n  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb3e95a) [0x7ea6fdc3d95a]\n  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb483cd) [0x7ea6fdc473cd]\n  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7ea6fd55fc79]\n  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7ea6fd56076c]\n  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7ea6fd5c44f7]\n  [bt] (6) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7ea6fd260ef0]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7ea765f49e2e]\n  [bt] (8) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7ea765f46493]\n\n\n\nStack trace:\n  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7ea6fdc26f2a]\n  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0xb485c9) [0x7ea6fdc475c9]\n  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7ea6fd55fc79]\n  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7ea6fd56076c]\n  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7ea6fd5c44f7]\n  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7ea6fd260ef0]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7ea765f49e2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7ea765f46493]\n  [bt] (8) /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0xa3e9) [0x7ea765f6f3e9]\n\n"]}],"source":["def cross_validation_mse_gradient_boosting(param):\n","    num_round = param[\"num_rounds\"]\n","    del param[\"num_rounds\"]\n","    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","    param[\"tree_method\"] = \"gpu_hist\"\n","    param[\"sampling_method\"] = \"gradient_based\"\n","\n","    MSE = []\n","    R2 = []\n","    for i in range(5):\n","        train_index, test_index  = train_indices[i], test_indices[i]\n","        print(train_index)\n","        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","        dvalid = xgb.DMatrix(train_X[test_index])\n","        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","        y_valid_pred = bst.predict(dvalid)\n","        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n","    return(-np.mean(R2))\n","\n","\n","from hyperopt import fmin, tpe, rand, hp, Trials\n","\n","space_gradient_boosting = {\n","    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n","    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n","    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n","    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n","    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n","    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n","    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n","    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n","\n","\n","trials = Trials()\n","best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n","            algo=rand.suggest, max_evals = 200, trials=trials)"]},{"cell_type":"code","source":["\n","for train_indice, test_indice in zip(train_indices, test_indices):\n","    print(len(train_indice), len(test_indice))\n","    print(type(train_indice), type(test_indice))\n","    print(train_indice)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HixzQLk-Vy1g","executionInfo":{"status":"ok","timestamp":1713148471318,"user_tz":-300,"elapsed":15,"user":{"displayName":"M Faizan","userId":"01979528019072830709"}},"outputId":"6784e35a-490c-4cc4-f1ba-c0d2b429ad77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2688 703\n","<class 'list'> <class 'list'>\n","[1, 6, 2576, 11, 2581, 2584, 16, 21, 2590, 26, 2596, 31, 2602, 2603, 36, 41, 46, 2614, 51, 2621, 2625, 57, 2626, 2628, 62, 2632, 2633, 2631, 67, 2636, 2638, 72, 2641, 77, 2647, 82, 87, 2658, 2661, 92, 2663, 97, 102, 2676, 2674, 107, 2679, 112, 2684, 117, 2689, 2693, 122, 2698, 2697, 127, 2703, 2702, 132, 2706, 2709, 137, 142, 2715, 2719, 147, 152, 2726, 2731, 157, 161, 162, 2736, 167, 2742, 2747, 172, 2748, 177, 2758, 187, 2764, 192, 2768, 197, 2773, 2778, 202, 207, 2782, 211, 216, 2792, 2794, 221, 2797, 224, 227, 2803, 229, 232, 2808, 2813, 2816, 242, 2819, 2823, 247, 2824, 249, 252, 2833, 257, 2834, 261, 2837, 263, 266, 268, 2844, 273, 274, 2848, 277, 2852, 282, 2859, 2857, 2863, 2864, 292, 2868, 297, 2873, 303, 2878, 308, 2884, 313, 2889, 2892, 2896, 2897, 323, 2901, 2909, 332, 2912, 337, 2924, 346, 2931, 2932, 356, 2939, 361, 362, 2944, 367, 2951, 2952, 373, 2953, 378, 379, 2963, 2968, 389, 2973, 2974, 399, 2978, 406, 411, 2992, 418, 2994, 2998, 422, 2999, 428, 3003, 433, 3011, 439, 444, 449, 3027, 3026, 3029, 454, 3034, 459, 464, 3039, 3044, 477, 3052, 482, 3057, 487, 3062, 490, 3065, 3066, 3068, 496, 3071, 501, 3077, 3081, 506, 3086, 510, 3088, 3091, 511, 3089, 517, 3100, 522, 3105, 3109, 532, 537, 538, 3114, 542, 547, 3123, 552, 3129, 557, 562, 3139, 566, 567, 572, 3148, 576, 3152, 581, 584, 586, 3166, 594, 596, 601, 3180, 606, 3182, 3181, 611, 3185, 3184, 616, 3188, 618, 622, 3193, 627, 3198, 633, 3203, 636, 638, 3208, 3214, 643, 648, 3222, 3223, 653, 658, 3229, 3228, 662, 3233, 668, 3236, 672, 677, 3246, 679, 3248, 681, 3251, 3256, 685, 3257, 689, 3262, 692, 694, 3266, 699, 704, 3276, 709, 711, 716, 3287, 3289, 3295, 3296, 728, 3299, 732, 3301, 3306, 3309, 742, 3312, 3318, 746, 3322, 756, 3327, 761, 3333, 764, 3338, 771, 3343, 774, 3351, 3353, 781, 3358, 783, 3360, 786, 3366, 3368, 794, 797, 799, 3379, 3383, 804, 3387, 3384, 808, 819, 824, 829, 834, 838, 844, 849, 851, 852, 857, 863, 866, 867, 877, 883, 886, 888, 891, 893, 896, 902, 907, 910, 911, 916, 921, 932, 938, 942, 948, 959, 963, 964, 969, 974, 978, 983, 989, 996, 1000, 1004, 1011, 1014, 1017, 1018, 1019, 1022, 1023, 1033, 1038, 1042, 1048, 1053, 1058, 1063, 1068, 1073, 1077, 1080, 1081, 1086, 1088, 1089, 1092, 1097, 1101, 1106, 1111, 1119, 1124, 1129, 1134, 1137, 1139, 1143, 1144, 1145, 1148, 1153, 1158, 1163, 1173, 1178, 1183, 1187, 1188, 1189, 1193, 1199, 1203, 1208, 1211, 1212, 1214, 1217, 1218, 1222, 1223, 1227, 1232, 1236, 1244, 1248, 1252, 1258, 1263, 1265, 1267, 1272, 1277, 1287, 1293, 1294, 1298, 1299, 1305, 1307, 1312, 1317, 1322, 1328, 1332, 1333, 1344, 1347, 1351, 1357, 1359, 1361, 1364, 1366, 1373, 1376, 1380, 1389, 1396, 1399, 1402, 1407, 1411, 1416, 1421, 1426, 1431, 1436, 1438, 1441, 1446, 1451, 1459, 1463, 1471, 1473, 1474, 1477, 1482, 1484, 1488, 1493, 1502, 1507, 1512, 1515, 1516, 1519, 1521, 1526, 1532, 1533, 1537, 1542, 1547, 1552, 1553, 1556, 1561, 1562, 1569, 1571, 1576, 1580, 1584, 1586, 1587, 1591, 1599, 1604, 1608, 1609, 1610, 1613, 1618, 1622, 1627, 1629, 1633, 1639, 1642, 1649, 1654, 1659, 1664, 1669, 1672, 1674, 1676, 1679, 1682, 1683, 1684, 1687, 1692, 1716, 1721, 1734, 1739, 1744, 1745, 1749, 1751, 1759, 1764, 1768, 1773, 1775, 1777, 1782, 1783, 1792, 1801, 1805, 1811, 1821, 1822, 1827, 1829, 1836, 1844, 1846, 1847, 1851, 1860, 1866, 1868, 1874, 1876, 1884, 1886, 1891, 1894, 1896, 1900, 1904, 1908, 1910, 1918, 1919, 1924, 1932, 1942, 1948, 1953, 1958, 1959, 1963, 1966, 1969, 1970, 1974, 1978, 1983, 1987, 1988, 1989, 1990, 1992, 1997, 1998, 2003, 2009, 2011, 2014, 2016, 2027, 2029, 2049, 2056, 2066, 2071, 2075, 2076, 2077, 2079, 2083, 2089, 2090, 2098, 2103, 2104, 2105, 2106, 2107, 2112, 2117, 2118, 2122, 2123, 2128, 2133, 2137, 2138, 2143, 2148, 2152, 2156, 2161, 2166, 2172, 2176, 2178, 2182, 2185, 2187, 2192, 2198, 2201, 2204, 2208, 2209, 2213, 2214, 2219, 2222, 2226, 2227, 2234, 2246, 2254, 2258, 2263, 2265, 2267, 2273, 2282, 2283, 2284, 2287, 2291, 2299, 2302, 2309, 2316, 2319, 2321, 2326, 2329, 2331, 2335, 2344, 2351, 2379, 2381, 2383, 2385, 2387, 2393, 2399, 2408, 2413, 2414, 2417, 2419, 2421, 2426, 2427, 2430, 2433, 2452, 2456, 2461, 2463, 2471, 2476, 2479, 2485, 2486, 2487, 2491, 2500, 2504, 2507, 2509, 2511, 2513, 2528, 2536, 2538, 2546, 2552, 2558, 2567, 2, 7, 12, 17, 22, 27, 32, 37, 42, 47, 52, 58, 63, 68, 73, 78, 83, 88, 93, 98, 103, 108, 113, 118, 123, 128, 133, 138, 143, 148, 153, 158, 164, 169, 171, 173, 174, 179, 182, 183, 188, 193, 198, 203, 208, 212, 217, 223, 231, 236, 237, 239, 244, 251, 256, 264, 271, 281, 286, 289, 294, 299, 306, 311, 316, 319, 324, 331, 336, 341, 344, 348, 352, 357, 363, 364, 371, 376, 382, 386, 393, 396, 401, 407, 414, 419, 423, 429, 434, 441, 451, 456, 457, 461, 466, 476, 478, 481, 486, 491, 492, 494, 499, 504, 512, 518, 523, 531, 536, 543, 548, 551, 553, 558, 563, 569, 574, 578, 589, 593, 599, 604, 609, 614, 623, 634, 641, 646, 651, 656, 666, 674, 684, 688, 696, 701, 706, 713, 721, 724, 729, 730, 733, 737, 741, 744, 745, 749, 753, 758, 763, 768, 772, 773, 776, 778, 784, 789, 792, 796, 803, 805, 807, 812, 817, 822, 827, 832, 836, 842, 847, 858, 864, 872, 873, 876, 878, 882, 887, 894, 901, 906, 912, 917, 923, 927, 931, 941, 946, 952, 957, 962, 968, 973, 977, 982, 986, 988, 994, 999, 1003, 1009, 1021, 1032, 1037, 1040, 1041, 1047, 1052, 1061, 1062, 1067, 1072, 1075, 1076, 1082, 1091, 1095, 1104, 1108, 1109, 1117, 1123, 1128, 1133, 1141, 1147, 1152, 1157, 1162, 1167, 1171, 1181, 1186, 1194, 1204, 1209, 1216, 1224, 1229, 1234, 1238, 1240, 1241, 1242, 1245, 1246, 1249, 1250, 1254, 1264, 1274, 1279, 1283, 1289, 1296, 1301, 1302, 1311, 1316, 1321, 1327, 1334, 1342, 1346, 1356, 1367, 1368, 1372, 1377, 1378, 1382, 1383, 1386, 1393, 1397, 1404, 1406, 1413, 1418, 1422, 1428, 1444, 1450, 1454, 1457, 1462, 1467, 1469, 1486, 1491, 1498, 1503, 1508, 1513, 1518, 1523, 1524, 1531, 1538, 1543, 1548, 1554, 1555, 1558, 1564, 1574, 1579, 1588, 1589, 1593, 1595, 1596, 1598, 1600, 1606, 1612, 1617, 1621, 1634, 1645, 1646, 1651, 1653, 1656, 1661, 1665, 1671, 1678, 1680, 1694, 1697, 1698, 1702, 1705, 1708, 1712, 1717, 1719, 1722, 1729, 1737, 1754, 1758, 1767, 1772, 1778, 1786, 1787, 1789, 1790, 1793, 1804, 1809, 1812, 1814, 1819, 1834, 1841, 1852, 1858, 1861, 1864, 1878, 1882, 1885, 1887, 1898, 1906, 1909, 1913, 1917, 1927, 1928, 1931, 1937, 1943, 1949, 1964, 1965, 1968, 1979, 1982, 1984, 1993, 1999, 2006, 2008, 2013, 2031, 2034, 2039, 2041, 2043, 2046, 2047, 2054, 2057, 2058, 2061, 2064, 2069, 2074, 2080, 2087, 2091, 2092, 2096, 2100, 2101, 2109, 2114, 2124, 2129, 2134, 2135, 2141, 2146, 2154, 2163, 2169, 2174, 2179, 2189, 2193, 2194, 2202, 2211, 2216, 2221, 2229, 2231, 2233, 2241, 2242, 2247, 2253, 2255, 2257, 2262, 2268, 2270, 2275, 2279, 2289, 2293, 2297, 2298, 2307, 2313, 2318, 2323, 2332, 2336, 2339, 2343, 2349, 2356, 2357, 2361, 2364, 2366, 2371, 2374, 2377, 2382, 2388, 2401, 2407, 2429, 2436, 2443, 2462, 2464, 2465, 2466, 2472, 2473, 2475, 2496, 2502, 2516, 2517, 2532, 2533, 2544, 2556, 2563, 2570, 2572, 2577, 2587, 2591, 2597, 2604, 2605, 2606, 2610, 2619, 2623, 2627, 2637, 2643, 2651, 2659, 2666, 2669, 2673, 2677, 2682, 2687, 2696, 2704, 2708, 2713, 2717, 2718, 2720, 2722, 2728, 2733, 2746, 2749, 2752, 2753, 2756, 2766, 2771, 2780, 2781, 2784, 2788, 2789, 2793, 2806, 2810, 2818, 2825, 2827, 2831, 2836, 2838, 2841, 2849, 2850, 2861, 2867, 2869, 2870, 2871, 2877, 2883, 2898, 2908, 2916, 2919, 2926, 2928, 2936, 2938, 2941, 2945, 2947, 2956, 2957, 2959, 2981, 2982, 2985, 2991, 2996, 3001, 3007, 3010, 3017, 3018, 3024, 3032, 3036, 3038, 3041, 3053, 3058, 3059, 3063, 3069, 3076, 3087, 3093, 3098, 3103, 3107, 3112, 3115, 3116, 3124, 3130, 3134, 3137, 3141, 3143, 3146, 3150, 3156, 3159, 3161, 3163, 3164, 3165, 3177, 3189, 3194, 3204, 3209, 3216, 3219, 3224, 3234, 3242, 3247, 3252, 3254, 3259, 3270, 3273, 3279, 3292, 3293, 3314, 3316, 3319, 3323, 3326, 3328, 3334, 3339, 3341, 3346, 3356, 3357, 3364, 3372, 3374, 3377, 3378, 3382, 3, 8, 13, 18, 23, 28, 33, 38, 43, 48, 54, 59, 64, 69, 74, 79, 84, 89, 94, 99, 104, 109, 114, 119, 124, 129, 134, 139, 144, 149, 154, 159, 166, 176, 181, 186, 191, 196, 201, 206, 214, 219, 228, 234, 241, 246, 253, 258, 267, 272, 275, 276, 279, 284, 288, 293, 298, 304, 309, 314, 318, 322, 326, 327, 328, 333, 338, 345, 349, 359, 368, 374, 381, 383, 384, 391, 397, 398, 404, 409, 417, 421, 432, 437, 443, 447, 452, 458, 463, 468, 471, 479, 484, 489, 497, 502, 507, 514, 519, 524, 529, 534, 541, 546, 554, 559, 564, 571, 579, 583, 588, 598, 603, 608, 613, 626, 630, 637, 642, 647, 657, 660, 663, 669, 671, 676, 680, 683, 687, 693, 698, 707, 708, 717, 723, 727, 734, 738, 743, 748, 751, 752, 757, 762, 766, 767, 769, 777, 782, 788, 790, 791, 802, 809, 818, 823, 828, 833, 835, 837, 843, 848, 859, 869, 871, 874, 881, 892, 899, 904, 909, 919, 926, 929, 936, 937, 939, 940, 943, 949, 958, 966, 971, 975, 979, 987, 993, 998, 1002, 1008, 1012, 1013, 1026, 1034, 1039, 1051, 1056, 1057, 1059, 1064, 1066, 1071, 1078, 1079, 1083, 1093, 1098, 1100, 1103, 1107, 1118, 1126, 1127, 1131, 1136, 1142, 1149, 1154, 1159, 1164, 1166, 1168, 1172, 1179, 1184, 1192, 1201, 1206, 1213, 1219, 1221, 1226, 1231, 1235, 1239, 1243, 1247, 1256, 1262, 1268, 1271, 1276, 1281, 1282, 1284, 1291, 1297, 1304, 1306, 1308, 1313, 1318, 1324, 1329, 1336, 1338, 1341, 1345, 1349, 1353, 1358, 1362, 1369, 1371, 1374, 1381, 1387, 1394, 1398, 1408, 1409, 1414, 1420, 1424, 1429, 1433, 1434, 1437, 1442, 1447, 1452, 1455, 1456, 1458, 1472, 1479, 1487, 1492, 1501, 1506, 1511, 1517, 1527, 1528, 1534, 1539, 1544, 1557, 1563, 1567, 1573, 1575, 1577, 1581, 1592, 1597, 1603, 1611, 1616, 1620, 1624, 1628, 1636, 1641, 1647, 1663, 1677, 1685, 1688, 1689, 1691, 1693, 1696, 1701, 1704, 1706, 1707, 1711, 1714, 1723, 1726, 1730, 1731, 1738, 1748, 1753, 1756, 1761, 1763, 1766, 1771, 1776, 1784, 1791, 1797, 1799, 1803, 1823, 1830, 1831, 1832, 1837, 1854, 1855, 1856, 1863, 1869, 1872, 1873, 1879, 1883, 1888, 1892, 1899, 1902, 1921, 1923, 1930, 1936, 1938, 1947, 1952, 1954, 1956, 1967, 1972, 1973, 1976, 1980, 1991, 1996, 2017, 2024, 2026, 2032, 2033, 2042, 2044, 2048, 2052, 2062, 2072, 2073, 2078, 2081, 2082, 2088, 2094, 2102, 2111, 2119, 2131, 2139, 2142, 2144, 2149, 2150, 2151, 2153, 2155, 2157, 2158, 2159, 2162, 2167, 2173, 2177, 2197, 2203, 2206, 2207, 2217, 2223, 2232, 2236, 2237, 2238, 2244, 2250, 2251, 2256, 2261, 2266, 2271, 2274, 2276, 2277, 2288, 2290, 2308, 2314, 2322, 2327, 2333, 2337, 2341, 2342, 2347, 2353, 2359, 2367, 2372, 2378, 2380, 2384, 2391, 2397, 2409, 2415, 2416, 2428, 2434, 2439, 2440, 2441, 2444, 2446, 2447, 2449, 2451, 2457, 2459, 2469, 2477, 2481, 2483, 2492, 2495, 2497, 2501, 2506, 2512, 2519, 2524, 2527, 2541, 2543, 2549, 2551, 2559, 2562, 2565, 2568, 2574, 2580, 2582, 2583, 2585, 2588, 2589, 2592, 2598, 2600, 2608, 2609, 2612, 2616, 2622, 2642, 2648, 2654, 2655, 2662, 2667, 2668, 2672, 2686, 2691, 2705, 2707, 2712, 2721, 2724, 2732, 2739, 2741, 2751, 2769, 2774, 2777, 2783, 2796, 2804, 2817, 2821, 2822, 2828, 2842, 2843, 2846, 2851, 2853, 2855, 2862, 2874, 2879, 2886, 2888, 2902, 2904, 2906, 2911, 2913, 2914, 2921, 2922, 2933, 2937, 2948, 2954, 2958, 2961, 2964, 2967, 2971, 2979, 2983, 2984, 2987, 2997, 3002, 3008, 3012, 3014, 3016, 3019, 3022, 3028, 3031, 3035, 3037, 3043, 3046, 3048, 3054, 3056, 3061, 3067, 3073, 3079, 3083, 3092, 3097, 3102, 3106, 3108, 3111, 3117, 3118, 3122, 3127, 3133, 3136, 3147, 3151, 3157, 3160, 3167, 3170, 3172, 3174, 3178, 3187, 3192, 3197, 3199, 3206, 3217, 3220, 3225, 3230, 3232, 3235, 3237, 3238, 3239, 3243, 3249, 3265, 3267, 3271, 3272, 3274, 3282, 3283, 3288, 3291, 3297, 3302, 3304, 3311, 3324, 3331, 3337, 3347, 3349, 3359, 3369, 3376, 3388, 3389, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 56, 61, 66, 71, 76, 81, 86, 91, 96, 101, 106, 111, 116, 121, 126, 131, 136, 141, 146, 151, 156, 163, 168, 178, 184, 189, 194, 199, 204, 209, 210, 213, 218, 226, 233, 238, 243, 248, 254, 262, 269, 278, 283, 287, 291, 296, 302, 307, 312, 317, 321, 325, 329, 334, 339, 342, 343, 347, 351, 353, 354, 366, 372, 377, 387, 394, 395, 403, 408, 416, 420, 426, 427, 431, 436, 442, 446, 448, 453, 462, 467, 469, 472, 483, 488, 493, 498, 503, 508, 516, 521, 526, 527, 528, 533, 539, 544, 549, 556, 561, 568, 573, 575, 577, 582, 587, 591, 592, 597, 602, 607, 612, 617, 621, 624, 628, 632, 639, 644, 649, 652, 654, 659, 661, 667, 670, 673, 678, 682, 686, 691, 697, 702, 703, 714, 722, 726, 731, 736, 739, 747, 754, 759, 779, 787, 793, 795, 801, 806, 811, 814, 816, 821, 826, 831, 841, 846, 854, 861, 865, 868, 879, 884, 885, 889, 898, 903, 908, 913, 914, 918, 924, 928, 933, 944, 951, 953, 954, 961, 967, 972, 976, 981, 991, 997, 1001, 1007, 1024, 1027, 1028, 1031, 1036, 1043, 1045, 1049, 1054, 1070, 1074, 1084, 1094, 1099, 1102, 1114, 1116, 1121, 1132, 1138, 1146, 1151, 1156, 1161, 1169, 1174, 1176, 1177, 1182, 1191, 1196, 1197, 1198, 1202, 1207, 1210, 1228, 1233, 1237, 1251, 1257, 1259, 1261, 1266, 1269, 1273, 1278, 1286, 1292, 1303, 1309, 1314, 1319, 1326, 1331, 1337, 1339, 1343, 1348, 1352, 1354, 1360, 1363, 1379, 1384, 1388, 1395, 1403, 1410, 1412, 1417, 1423, 1427, 1432, 1439, 1443, 1448, 1453, 1461, 1464, 1466, 1478, 1481, 1489, 1494, 1495, 1496, 1497, 1499, 1504, 1509, 1514, 1522, 1536, 1541, 1546, 1549, 1551, 1559, 1566, 1568, 1578, 1583, 1594, 1601, 1607, 1614, 1619, 1623, 1626, 1631, 1637, 1643, 1648, 1652, 1657, 1658, 1662, 1666, 1668, 1673, 1681, 1686, 1699, 1703, 1709, 1713, 1718, 1724, 1727, 1728, 1732, 1733, 1736, 1741, 1746, 1747, 1752, 1755, 1757, 1762, 1765, 1769, 1774, 1781, 1788, 1794, 1795, 1796, 1798, 1802, 1806, 1808, 1813, 1816, 1818, 1824, 1826, 1828, 1833, 1839, 1842, 1843, 1848, 1849, 1850, 1853, 1857, 1867, 1871, 1881, 1889, 1895, 1897, 1901, 1903, 1905, 1907, 1914, 1916, 1922, 1925, 1926, 1929, 1933, 1934, 1939, 1941, 1944, 1951, 1957, 1971, 1977, 1986, 1994, 2001, 2004, 2012, 2021, 2022, 2036, 2037, 2045, 2051, 2059, 2063, 2067, 2068, 2086, 2093, 2099, 2108, 2113, 2121, 2126, 2127, 2132, 2147, 2164, 2171, 2175, 2183, 2184, 2191, 2199, 2212, 2218, 2228, 2239, 2243, 2248, 2249, 2252, 2259, 2264, 2269, 2272, 2286, 2292, 2294, 2296, 2304, 2311, 2317, 2324, 2328, 2334, 2338, 2348, 2354, 2362, 2369, 2373, 2375, 2376, 2386, 2392, 2394, 2396, 2402, 2403, 2404, 2405, 2406, 2411, 2412, 2418, 2420, 2422, 2423, 2424, 2431, 2438, 2442, 2448, 2450, 2454, 2458, 2467, 2474, 2478, 2482, 2489, 2493, 2494, 2498, 2503, 2510, 2518, 2521, 2526, 2529, 2537, 2539, 2542, 2548, 2553, 2554, 2561, 2564, 2566, 2569, 2571, 2573, 2579, 2593, 2599, 2611, 2613, 2617, 2629, 2634, 2639, 2646, 2652, 2656, 2657, 2664, 2678, 2681, 2683, 2688, 2699, 2701, 2710, 2714, 2723, 2729, 2734, 2740, 2743, 2754, 2757, 2759, 2762, 2763, 2765, 2767, 2772, 2776, 2779, 2786, 2787, 2791, 2799, 2801, 2802, 2807, 2811, 2812, 2826, 2832, 2839, 2845, 2847, 2856, 2866, 2876, 2882, 2887, 2891, 2894, 2899, 2907, 2923, 2927, 2929, 2934, 2942, 2943, 2946, 2949, 2962, 2966, 2969, 2972, 2976, 2977, 2986, 2988, 3000, 3004, 3021, 3023, 3033, 3042, 3049, 3064, 3072, 3078, 3094, 3099, 3104, 3113, 3119, 3121, 3126, 3131, 3135, 3142, 3144, 3145, 3149, 3153, 3158, 3162, 3168, 3169, 3171, 3176, 3179, 3183, 3186, 3191, 3196, 3201, 3202, 3207, 3211, 3212, 3218, 3221, 3227, 3231, 3241, 3244, 3258, 3261, 3263, 3264, 3268, 3277, 3281, 3284, 3286, 3294, 3298, 3303, 3307, 3317, 3320, 3321, 3329, 3336, 3342, 3361, 3362, 3363, 3371, 3381]\n","2647 744\n","<class 'list'> <class 'list'>\n","[0, 2050, 2053, 5, 2055, 10, 2060, 15, 2065, 20, 2070, 25, 30, 35, 2084, 2085, 40, 45, 2095, 2097, 50, 53, 55, 60, 2110, 65, 2115, 2116, 70, 2120, 75, 2125, 80, 2130, 85, 2136, 90, 2140, 95, 2145, 100, 105, 110, 2160, 115, 2165, 2168, 120, 2170, 125, 130, 2180, 2181, 135, 2186, 140, 2188, 2190, 145, 2195, 2196, 150, 2200, 155, 2205, 160, 2210, 165, 2215, 170, 2220, 175, 2224, 2225, 180, 2230, 185, 2235, 190, 2240, 195, 2245, 200, 205, 2260, 215, 220, 222, 225, 230, 2278, 2280, 2281, 235, 2285, 240, 245, 2295, 250, 2300, 2301, 2303, 255, 2305, 2306, 259, 260, 2310, 2312, 265, 2315, 270, 2320, 2325, 280, 2330, 285, 290, 2340, 295, 2345, 2346, 300, 301, 2350, 2352, 305, 2355, 310, 2358, 2360, 315, 2363, 2365, 320, 2368, 2370, 330, 335, 340, 2389, 2390, 2395, 350, 2398, 2400, 355, 358, 360, 2410, 365, 369, 370, 375, 2425, 380, 2432, 385, 2435, 388, 2437, 390, 392, 2445, 400, 402, 2453, 405, 2455, 410, 412, 413, 2460, 415, 2468, 2470, 424, 425, 430, 2480, 435, 2484, 438, 440, 2488, 2490, 445, 450, 2499, 455, 2505, 460, 2508, 465, 2514, 2515, 470, 2520, 473, 474, 2523, 475, 2522, 2525, 480, 2530, 2531, 485, 2534, 2535, 2540, 495, 2545, 2547, 500, 2550, 505, 2555, 2557, 509, 2560, 513, 515, 520, 525, 2575, 2578, 530, 535, 2586, 540, 545, 2594, 2595, 550, 2601, 555, 2607, 560, 565, 2615, 570, 2618, 2620, 2624, 580, 2630, 585, 2635, 590, 2640, 595, 2644, 2645, 600, 2649, 2650, 2653, 605, 610, 2660, 615, 2665, 619, 620, 2670, 2671, 625, 2675, 629, 631, 2680, 635, 2685, 640, 2690, 2692, 645, 2694, 2695, 650, 2700, 655, 2711, 664, 665, 2716, 675, 2725, 2727, 2730, 2735, 2737, 690, 2738, 695, 2744, 2745, 700, 2750, 705, 2755, 710, 712, 2761, 2760, 715, 718, 719, 720, 2770, 725, 2775, 735, 2785, 740, 2790, 2795, 750, 2798, 2800, 755, 2805, 760, 2809, 765, 2814, 2815, 770, 2820, 775, 780, 2829, 2830, 785, 2835, 2840, 798, 800, 2854, 2858, 810, 2860, 813, 815, 2865, 820, 2872, 825, 2875, 830, 2880, 2881, 2885, 839, 840, 2890, 845, 2893, 2895, 850, 2900, 853, 855, 856, 2903, 2905, 860, 862, 2910, 2915, 2917, 870, 2918, 2920, 875, 2925, 880, 2930, 2935, 890, 2940, 895, 897, 900, 2950, 905, 2955, 2960, 915, 2965, 920, 2970, 922, 925, 2975, 930, 2980, 934, 935, 2989, 2990, 2993, 945, 947, 2995, 950, 955, 956, 3005, 3006, 960, 3009, 965, 3013, 3015, 970, 3020, 3025, 980, 3030, 984, 985, 990, 992, 3040, 995, 3045, 3047, 3050, 3051, 1005, 1006, 3055, 1010, 3060, 1015, 1016, 1020, 3070, 1025, 3074, 3075, 1029, 1030, 3080, 3082, 1035, 3084, 3085, 3090, 1044, 1046, 3095, 3096, 1050, 3101, 1055, 1060, 3110, 1065, 1069, 3120, 3125, 3128, 3132, 1085, 1087, 3138, 1090, 3140, 1096, 1105, 3154, 3155, 1110, 1112, 1113, 1115, 1120, 1122, 1125, 3173, 3175, 1130, 1135, 1140, 3190, 3195, 1150, 3200, 1155, 3205, 1160, 3210, 3213, 1165, 3215, 1170, 1175, 3226, 1180, 1185, 1190, 3240, 1195, 3245, 1200, 3250, 3253, 1205, 3255, 3260, 1215, 1220, 3269, 1225, 3275, 3278, 1230, 3280, 3285, 3290, 3300, 1253, 1255, 3305, 3308, 1260, 3310, 3313, 3315, 1270, 1275, 3325, 1280, 3330, 3332, 1285, 3335, 1288, 1290, 3340, 1295, 3344, 3345, 1300, 3348, 3350, 3352, 3354, 3355, 1310, 1315, 3365, 3367, 1320, 3370, 1323, 1325, 3373, 3375, 1330, 3380, 1335, 3385, 3386, 1340, 3390, 1350, 1355, 1365, 1370, 1375, 1385, 1390, 1391, 1392, 1400, 1401, 1405, 1415, 1419, 1425, 1430, 1435, 1440, 1445, 1449, 1460, 1465, 1468, 1470, 1475, 1476, 1480, 1483, 1485, 1490, 1500, 1505, 1510, 1520, 1525, 1529, 1530, 1535, 1540, 1545, 1550, 1560, 1565, 1570, 1572, 1582, 1585, 1590, 1602, 1605, 1615, 1625, 1630, 1632, 1635, 1638, 1640, 1644, 1650, 1655, 1660, 1667, 1670, 1675, 1690, 1695, 1700, 1710, 1715, 1720, 1725, 1735, 1740, 1742, 1743, 1750, 1760, 1770, 1779, 1780, 1785, 1800, 1807, 1810, 1815, 1817, 1820, 1825, 1835, 1838, 1840, 1845, 1859, 1862, 1865, 1870, 1875, 1877, 1880, 1890, 1893, 1911, 1912, 1915, 1920, 1935, 1940, 1945, 1946, 1950, 1955, 1960, 1961, 1962, 1975, 1981, 1985, 1995, 2000, 2002, 2005, 2007, 2010, 2015, 2018, 2019, 2020, 2023, 2025, 2028, 2030, 2035, 2038, 2040, 2, 7, 12, 17, 22, 27, 32, 37, 42, 47, 52, 58, 63, 68, 73, 78, 83, 88, 93, 98, 103, 108, 113, 118, 123, 128, 133, 138, 143, 148, 153, 158, 164, 169, 171, 173, 174, 179, 182, 183, 188, 193, 198, 203, 208, 212, 217, 223, 231, 236, 237, 239, 244, 251, 256, 264, 271, 281, 286, 289, 294, 299, 306, 311, 316, 319, 324, 331, 336, 341, 344, 348, 352, 357, 363, 364, 371, 376, 382, 386, 393, 396, 401, 407, 414, 419, 423, 429, 434, 441, 451, 456, 457, 461, 466, 476, 478, 481, 486, 491, 492, 494, 499, 504, 512, 518, 523, 531, 536, 543, 548, 551, 553, 558, 563, 569, 574, 578, 589, 593, 599, 604, 609, 614, 623, 634, 641, 646, 651, 656, 666, 674, 684, 688, 696, 701, 706, 713, 721, 724, 729, 730, 733, 737, 741, 744, 745, 749, 753, 758, 763, 768, 772, 773, 776, 778, 784, 789, 792, 796, 803, 805, 807, 812, 817, 822, 827, 832, 836, 842, 847, 858, 864, 872, 873, 876, 878, 882, 887, 894, 901, 906, 912, 917, 923, 927, 931, 941, 946, 952, 957, 962, 968, 973, 977, 982, 986, 988, 994, 999, 1003, 1009, 1021, 1032, 1037, 1040, 1041, 1047, 1052, 1061, 1062, 1067, 1072, 1075, 1076, 1082, 1091, 1095, 1104, 1108, 1109, 1117, 1123, 1128, 1133, 1141, 1147, 1152, 1157, 1162, 1167, 1171, 1181, 1186, 1194, 1204, 1209, 1216, 1224, 1229, 1234, 1238, 1240, 1241, 1242, 1245, 1246, 1249, 1250, 1254, 1264, 1274, 1279, 1283, 1289, 1296, 1301, 1302, 1311, 1316, 1321, 1327, 1334, 1342, 1346, 1356, 1367, 1368, 1372, 1377, 1378, 1382, 1383, 1386, 1393, 1397, 1404, 1406, 1413, 1418, 1422, 1428, 1444, 1450, 1454, 1457, 1462, 1467, 1469, 1486, 1491, 1498, 1503, 1508, 1513, 1518, 1523, 1524, 1531, 1538, 1543, 1548, 1554, 1555, 1558, 1564, 1574, 1579, 1588, 1589, 1593, 1595, 1596, 1598, 1600, 1606, 1612, 1617, 1621, 1634, 1645, 1646, 1651, 1653, 1656, 1661, 1665, 1671, 1678, 1680, 1694, 1697, 1698, 1702, 1705, 1708, 1712, 1717, 1719, 1722, 1729, 1737, 1754, 1758, 1767, 1772, 1778, 1786, 1787, 1789, 1790, 1793, 1804, 1809, 1812, 1814, 1819, 1834, 1841, 1852, 1858, 1861, 1864, 1878, 1882, 1885, 1887, 1898, 1906, 1909, 1913, 1917, 1927, 1928, 1931, 1937, 1943, 1949, 1964, 1965, 1968, 1979, 1982, 1984, 1993, 1999, 2006, 2008, 2013, 2031, 2034, 2039, 2041, 2043, 2046, 2047, 2054, 2057, 2058, 2061, 2064, 2069, 2074, 2080, 2087, 2091, 2092, 2096, 2100, 2101, 2109, 2114, 2124, 2129, 2134, 2135, 2141, 2146, 2154, 2163, 2169, 2174, 2179, 2189, 2193, 2194, 2202, 2211, 2216, 2221, 2229, 2231, 2233, 2241, 2242, 2247, 2253, 2255, 2257, 2262, 2268, 2270, 2275, 2279, 2289, 2293, 2297, 2298, 2307, 2313, 2318, 2323, 2332, 2336, 2339, 2343, 2349, 2356, 2357, 2361, 2364, 2366, 2371, 2374, 2377, 2382, 2388, 2401, 2407, 2429, 2436, 2443, 2462, 2464, 2465, 2466, 2472, 2473, 2475, 2496, 2502, 2516, 2517, 2532, 2533, 2544, 2556, 2563, 2570, 2572, 2577, 2587, 2591, 2597, 2604, 2605, 2606, 2610, 2619, 2623, 2627, 2637, 2643, 2651, 2659, 2666, 2669, 2673, 2677, 2682, 2687, 2696, 2704, 2708, 2713, 2717, 2718, 2720, 2722, 2728, 2733, 2746, 2749, 2752, 2753, 2756, 2766, 2771, 2780, 2781, 2784, 2788, 2789, 2793, 2806, 2810, 2818, 2825, 2827, 2831, 2836, 2838, 2841, 2849, 2850, 2861, 2867, 2869, 2870, 2871, 2877, 2883, 2898, 2908, 2916, 2919, 2926, 2928, 2936, 2938, 2941, 2945, 2947, 2956, 2957, 2959, 2981, 2982, 2985, 2991, 2996, 3001, 3007, 3010, 3017, 3018, 3024, 3032, 3036, 3038, 3041, 3053, 3058, 3059, 3063, 3069, 3076, 3087, 3093, 3098, 3103, 3107, 3112, 3115, 3116, 3124, 3130, 3134, 3137, 3141, 3143, 3146, 3150, 3156, 3159, 3161, 3163, 3164, 3165, 3177, 3189, 3194, 3204, 3209, 3216, 3219, 3224, 3234, 3242, 3247, 3252, 3254, 3259, 3270, 3273, 3279, 3292, 3293, 3314, 3316, 3319, 3323, 3326, 3328, 3334, 3339, 3341, 3346, 3356, 3357, 3364, 3372, 3374, 3377, 3378, 3382, 3, 8, 13, 18, 23, 28, 33, 38, 43, 48, 54, 59, 64, 69, 74, 79, 84, 89, 94, 99, 104, 109, 114, 119, 124, 129, 134, 139, 144, 149, 154, 159, 166, 176, 181, 186, 191, 196, 201, 206, 214, 219, 228, 234, 241, 246, 253, 258, 267, 272, 275, 276, 279, 284, 288, 293, 298, 304, 309, 314, 318, 322, 326, 327, 328, 333, 338, 345, 349, 359, 368, 374, 381, 383, 384, 391, 397, 398, 404, 409, 417, 421, 432, 437, 443, 447, 452, 458, 463, 468, 471, 479, 484, 489, 497, 502, 507, 514, 519, 524, 529, 534, 541, 546, 554, 559, 564, 571, 579, 583, 588, 598, 603, 608, 613, 626, 630, 637, 642, 647, 657, 660, 663, 669, 671, 676, 680, 683, 687, 693, 698, 707, 708, 717, 723, 727, 734, 738, 743, 748, 751, 752, 757, 762, 766, 767, 769, 777, 782, 788, 790, 791, 802, 809, 818, 823, 828, 833, 835, 837, 843, 848, 859, 869, 871, 874, 881, 892, 899, 904, 909, 919, 926, 929, 936, 937, 939, 940, 943, 949, 958, 966, 971, 975, 979, 987, 993, 998, 1002, 1008, 1012, 1013, 1026, 1034, 1039, 1051, 1056, 1057, 1059, 1064, 1066, 1071, 1078, 1079, 1083, 1093, 1098, 1100, 1103, 1107, 1118, 1126, 1127, 1131, 1136, 1142, 1149, 1154, 1159, 1164, 1166, 1168, 1172, 1179, 1184, 1192, 1201, 1206, 1213, 1219, 1221, 1226, 1231, 1235, 1239, 1243, 1247, 1256, 1262, 1268, 1271, 1276, 1281, 1282, 1284, 1291, 1297, 1304, 1306, 1308, 1313, 1318, 1324, 1329, 1336, 1338, 1341, 1345, 1349, 1353, 1358, 1362, 1369, 1371, 1374, 1381, 1387, 1394, 1398, 1408, 1409, 1414, 1420, 1424, 1429, 1433, 1434, 1437, 1442, 1447, 1452, 1455, 1456, 1458, 1472, 1479, 1487, 1492, 1501, 1506, 1511, 1517, 1527, 1528, 1534, 1539, 1544, 1557, 1563, 1567, 1573, 1575, 1577, 1581, 1592, 1597, 1603, 1611, 1616, 1620, 1624, 1628, 1636, 1641, 1647, 1663, 1677, 1685, 1688, 1689, 1691, 1693, 1696, 1701, 1704, 1706, 1707, 1711, 1714, 1723, 1726, 1730, 1731, 1738, 1748, 1753, 1756, 1761, 1763, 1766, 1771, 1776, 1784, 1791, 1797, 1799, 1803, 1823, 1830, 1831, 1832, 1837, 1854, 1855, 1856, 1863, 1869, 1872, 1873, 1879, 1883, 1888, 1892, 1899, 1902, 1921, 1923, 1930, 1936, 1938, 1947, 1952, 1954, 1956, 1967, 1972, 1973, 1976, 1980, 1991, 1996, 2017, 2024, 2026, 2032, 2033, 2042, 2044, 2048, 2052, 2062, 2072, 2073, 2078, 2081, 2082, 2088, 2094, 2102, 2111, 2119, 2131, 2139, 2142, 2144, 2149, 2150, 2151, 2153, 2155, 2157, 2158, 2159, 2162, 2167, 2173, 2177, 2197, 2203, 2206, 2207, 2217, 2223, 2232, 2236, 2237, 2238, 2244, 2250, 2251, 2256, 2261, 2266, 2271, 2274, 2276, 2277, 2288, 2290, 2308, 2314, 2322, 2327, 2333, 2337, 2341, 2342, 2347, 2353, 2359, 2367, 2372, 2378, 2380, 2384, 2391, 2397, 2409, 2415, 2416, 2428, 2434, 2439, 2440, 2441, 2444, 2446, 2447, 2449, 2451, 2457, 2459, 2469, 2477, 2481, 2483, 2492, 2495, 2497, 2501, 2506, 2512, 2519, 2524, 2527, 2541, 2543, 2549, 2551, 2559, 2562, 2565, 2568, 2574, 2580, 2582, 2583, 2585, 2588, 2589, 2592, 2598, 2600, 2608, 2609, 2612, 2616, 2622, 2642, 2648, 2654, 2655, 2662, 2667, 2668, 2672, 2686, 2691, 2705, 2707, 2712, 2721, 2724, 2732, 2739, 2741, 2751, 2769, 2774, 2777, 2783, 2796, 2804, 2817, 2821, 2822, 2828, 2842, 2843, 2846, 2851, 2853, 2855, 2862, 2874, 2879, 2886, 2888, 2902, 2904, 2906, 2911, 2913, 2914, 2921, 2922, 2933, 2937, 2948, 2954, 2958, 2961, 2964, 2967, 2971, 2979, 2983, 2984, 2987, 2997, 3002, 3008, 3012, 3014, 3016, 3019, 3022, 3028, 3031, 3035, 3037, 3043, 3046, 3048, 3054, 3056, 3061, 3067, 3073, 3079, 3083, 3092, 3097, 3102, 3106, 3108, 3111, 3117, 3118, 3122, 3127, 3133, 3136, 3147, 3151, 3157, 3160, 3167, 3170, 3172, 3174, 3178, 3187, 3192, 3197, 3199, 3206, 3217, 3220, 3225, 3230, 3232, 3235, 3237, 3238, 3239, 3243, 3249, 3265, 3267, 3271, 3272, 3274, 3282, 3283, 3288, 3291, 3297, 3302, 3304, 3311, 3324, 3331, 3337, 3347, 3349, 3359, 3369, 3376, 3388, 3389, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 56, 61, 66, 71, 76, 81, 86, 91, 96, 101, 106, 111, 116, 121, 126, 131, 136, 141, 146, 151, 156, 163, 168, 178, 184, 189, 194, 199, 204, 209, 210, 213, 218, 226, 233, 238, 243, 248, 254, 262, 269, 278, 283, 287, 291, 296, 302, 307, 312, 317, 321, 325, 329, 334, 339, 342, 343, 347, 351, 353, 354, 366, 372, 377, 387, 394, 395, 403, 408, 416, 420, 426, 427, 431, 436, 442, 446, 448, 453, 462, 467, 469, 472, 483, 488, 493, 498, 503, 508, 516, 521, 526, 527, 528, 533, 539, 544, 549, 556, 561, 568, 573, 575, 577, 582, 587, 591, 592, 597, 602, 607, 612, 617, 621, 624, 628, 632, 639, 644, 649, 652, 654, 659, 661, 667, 670, 673, 678, 682, 686, 691, 697, 702, 703, 714, 722, 726, 731, 736, 739, 747, 754, 759, 779, 787, 793, 795, 801, 806, 811, 814, 816, 821, 826, 831, 841, 846, 854, 861, 865, 868, 879, 884, 885, 889, 898, 903, 908, 913, 914, 918, 924, 928, 933, 944, 951, 953, 954, 961, 967, 972, 976, 981, 991, 997, 1001, 1007, 1024, 1027, 1028, 1031, 1036, 1043, 1045, 1049, 1054, 1070, 1074, 1084, 1094, 1099, 1102, 1114, 1116, 1121, 1132, 1138, 1146, 1151, 1156, 1161, 1169, 1174, 1176, 1177, 1182, 1191, 1196, 1197, 1198, 1202, 1207, 1210, 1228, 1233, 1237, 1251, 1257, 1259, 1261, 1266, 1269, 1273, 1278, 1286, 1292, 1303, 1309, 1314, 1319, 1326, 1331, 1337, 1339, 1343, 1348, 1352, 1354, 1360, 1363, 1379, 1384, 1388, 1395, 1403, 1410, 1412, 1417, 1423, 1427, 1432, 1439, 1443, 1448, 1453, 1461, 1464, 1466, 1478, 1481, 1489, 1494, 1495, 1496, 1497, 1499, 1504, 1509, 1514, 1522, 1536, 1541, 1546, 1549, 1551, 1559, 1566, 1568, 1578, 1583, 1594, 1601, 1607, 1614, 1619, 1623, 1626, 1631, 1637, 1643, 1648, 1652, 1657, 1658, 1662, 1666, 1668, 1673, 1681, 1686, 1699, 1703, 1709, 1713, 1718, 1724, 1727, 1728, 1732, 1733, 1736, 1741, 1746, 1747, 1752, 1755, 1757, 1762, 1765, 1769, 1774, 1781, 1788, 1794, 1795, 1796, 1798, 1802, 1806, 1808, 1813, 1816, 1818, 1824, 1826, 1828, 1833, 1839, 1842, 1843, 1848, 1849, 1850, 1853, 1857, 1867, 1871, 1881, 1889, 1895, 1897, 1901, 1903, 1905, 1907, 1914, 1916, 1922, 1925, 1926, 1929, 1933, 1934, 1939, 1941, 1944, 1951, 1957, 1971, 1977, 1986, 1994, 2001, 2004, 2012, 2021, 2022, 2036, 2037, 2045, 2051, 2059, 2063, 2067, 2068, 2086, 2093, 2099, 2108, 2113, 2121, 2126, 2127, 2132, 2147, 2164, 2171, 2175, 2183, 2184, 2191, 2199, 2212, 2218, 2228, 2239, 2243, 2248, 2249, 2252, 2259, 2264, 2269, 2272, 2286, 2292, 2294, 2296, 2304, 2311, 2317, 2324, 2328, 2334, 2338, 2348, 2354, 2362, 2369, 2373, 2375, 2376, 2386, 2392, 2394, 2396, 2402, 2403, 2404, 2405, 2406, 2411, 2412, 2418, 2420, 2422, 2423, 2424, 2431, 2438, 2442, 2448, 2450, 2454, 2458, 2467, 2474, 2478, 2482, 2489, 2493, 2494, 2498, 2503, 2510, 2518, 2521, 2526, 2529, 2537, 2539, 2542, 2548, 2553, 2554, 2561, 2564, 2566, 2569, 2571, 2573, 2579, 2593, 2599, 2611, 2613, 2617, 2629, 2634, 2639, 2646, 2652, 2656, 2657, 2664, 2678, 2681, 2683, 2688, 2699, 2701, 2710, 2714, 2723, 2729, 2734, 2740, 2743, 2754, 2757, 2759, 2762, 2763, 2765, 2767, 2772, 2776, 2779, 2786, 2787, 2791, 2799, 2801, 2802, 2807, 2811, 2812, 2826, 2832, 2839, 2845, 2847, 2856, 2866, 2876, 2882, 2887, 2891, 2894, 2899, 2907, 2923, 2927, 2929, 2934, 2942, 2943, 2946, 2949, 2962, 2966, 2969, 2972, 2976, 2977, 2986, 2988, 3000, 3004, 3021, 3023, 3033, 3042, 3049, 3064, 3072, 3078, 3094, 3099, 3104, 3113, 3119, 3121, 3126, 3131, 3135, 3142, 3144, 3145, 3149, 3153, 3158, 3162, 3168, 3169, 3171, 3176, 3179, 3183, 3186, 3191, 3196, 3201, 3202, 3207, 3211, 3212, 3218, 3221, 3227, 3231, 3241, 3244, 3258, 3261, 3263, 3264, 3268, 3277, 3281, 3284, 3286, 3294, 3298, 3303, 3307, 3317, 3320, 3321, 3329, 3336, 3342, 3361, 3362, 3363, 3371, 3381]\n","2745 646\n","<class 'list'> <class 'list'>\n","[0, 2050, 2053, 5, 2055, 10, 2060, 15, 2065, 20, 2070, 25, 30, 35, 2084, 2085, 40, 45, 2095, 2097, 50, 53, 55, 60, 2110, 65, 2115, 2116, 70, 2120, 75, 2125, 80, 2130, 85, 2136, 90, 2140, 95, 2145, 100, 105, 110, 2160, 115, 2165, 2168, 120, 2170, 125, 130, 2180, 2181, 135, 2186, 140, 2188, 2190, 145, 2195, 2196, 150, 2200, 155, 2205, 160, 2210, 165, 2215, 170, 2220, 175, 2224, 2225, 180, 2230, 185, 2235, 190, 2240, 195, 2245, 200, 205, 2260, 215, 220, 222, 225, 230, 2278, 2280, 2281, 235, 2285, 240, 245, 2295, 250, 2300, 2301, 2303, 255, 2305, 2306, 259, 260, 2310, 2312, 265, 2315, 270, 2320, 2325, 280, 2330, 285, 290, 2340, 295, 2345, 2346, 300, 301, 2350, 2352, 305, 2355, 310, 2358, 2360, 315, 2363, 2365, 320, 2368, 2370, 330, 335, 340, 2389, 2390, 2395, 350, 2398, 2400, 355, 358, 360, 2410, 365, 369, 370, 375, 2425, 380, 2432, 385, 2435, 388, 2437, 390, 392, 2445, 400, 402, 2453, 405, 2455, 410, 412, 413, 2460, 415, 2468, 2470, 424, 425, 430, 2480, 435, 2484, 438, 440, 2488, 2490, 445, 450, 2499, 455, 2505, 460, 2508, 465, 2514, 2515, 470, 2520, 473, 474, 2523, 475, 2522, 2525, 480, 2530, 2531, 485, 2534, 2535, 2540, 495, 2545, 2547, 500, 2550, 505, 2555, 2557, 509, 2560, 513, 515, 520, 525, 2575, 2578, 530, 535, 2586, 540, 545, 2594, 2595, 550, 2601, 555, 2607, 560, 565, 2615, 570, 2618, 2620, 2624, 580, 2630, 585, 2635, 590, 2640, 595, 2644, 2645, 600, 2649, 2650, 2653, 605, 610, 2660, 615, 2665, 619, 620, 2670, 2671, 625, 2675, 629, 631, 2680, 635, 2685, 640, 2690, 2692, 645, 2694, 2695, 650, 2700, 655, 2711, 664, 665, 2716, 675, 2725, 2727, 2730, 2735, 2737, 690, 2738, 695, 2744, 2745, 700, 2750, 705, 2755, 710, 712, 2761, 2760, 715, 718, 719, 720, 2770, 725, 2775, 735, 2785, 740, 2790, 2795, 750, 2798, 2800, 755, 2805, 760, 2809, 765, 2814, 2815, 770, 2820, 775, 780, 2829, 2830, 785, 2835, 2840, 798, 800, 2854, 2858, 810, 2860, 813, 815, 2865, 820, 2872, 825, 2875, 830, 2880, 2881, 2885, 839, 840, 2890, 845, 2893, 2895, 850, 2900, 853, 855, 856, 2903, 2905, 860, 862, 2910, 2915, 2917, 870, 2918, 2920, 875, 2925, 880, 2930, 2935, 890, 2940, 895, 897, 900, 2950, 905, 2955, 2960, 915, 2965, 920, 2970, 922, 925, 2975, 930, 2980, 934, 935, 2989, 2990, 2993, 945, 947, 2995, 950, 955, 956, 3005, 3006, 960, 3009, 965, 3013, 3015, 970, 3020, 3025, 980, 3030, 984, 985, 990, 992, 3040, 995, 3045, 3047, 3050, 3051, 1005, 1006, 3055, 1010, 3060, 1015, 1016, 1020, 3070, 1025, 3074, 3075, 1029, 1030, 3080, 3082, 1035, 3084, 3085, 3090, 1044, 1046, 3095, 3096, 1050, 3101, 1055, 1060, 3110, 1065, 1069, 3120, 3125, 3128, 3132, 1085, 1087, 3138, 1090, 3140, 1096, 1105, 3154, 3155, 1110, 1112, 1113, 1115, 1120, 1122, 1125, 3173, 3175, 1130, 1135, 1140, 3190, 3195, 1150, 3200, 1155, 3205, 1160, 3210, 3213, 1165, 3215, 1170, 1175, 3226, 1180, 1185, 1190, 3240, 1195, 3245, 1200, 3250, 3253, 1205, 3255, 3260, 1215, 1220, 3269, 1225, 3275, 3278, 1230, 3280, 3285, 3290, 3300, 1253, 1255, 3305, 3308, 1260, 3310, 3313, 3315, 1270, 1275, 3325, 1280, 3330, 3332, 1285, 3335, 1288, 1290, 3340, 1295, 3344, 3345, 1300, 3348, 3350, 3352, 3354, 3355, 1310, 1315, 3365, 3367, 1320, 3370, 1323, 1325, 3373, 3375, 1330, 3380, 1335, 3385, 3386, 1340, 3390, 1350, 1355, 1365, 1370, 1375, 1385, 1390, 1391, 1392, 1400, 1401, 1405, 1415, 1419, 1425, 1430, 1435, 1440, 1445, 1449, 1460, 1465, 1468, 1470, 1475, 1476, 1480, 1483, 1485, 1490, 1500, 1505, 1510, 1520, 1525, 1529, 1530, 1535, 1540, 1545, 1550, 1560, 1565, 1570, 1572, 1582, 1585, 1590, 1602, 1605, 1615, 1625, 1630, 1632, 1635, 1638, 1640, 1644, 1650, 1655, 1660, 1667, 1670, 1675, 1690, 1695, 1700, 1710, 1715, 1720, 1725, 1735, 1740, 1742, 1743, 1750, 1760, 1770, 1779, 1780, 1785, 1800, 1807, 1810, 1815, 1817, 1820, 1825, 1835, 1838, 1840, 1845, 1859, 1862, 1865, 1870, 1875, 1877, 1880, 1890, 1893, 1911, 1912, 1915, 1920, 1935, 1940, 1945, 1946, 1950, 1955, 1960, 1961, 1962, 1975, 1981, 1985, 1995, 2000, 2002, 2005, 2007, 2010, 2015, 2018, 2019, 2020, 2023, 2025, 2028, 2030, 2035, 2038, 2040, 1, 6, 2576, 11, 2581, 2584, 16, 21, 2590, 26, 2596, 31, 2602, 2603, 36, 41, 46, 2614, 51, 2621, 2625, 57, 2626, 2628, 62, 2632, 2633, 2631, 67, 2636, 2638, 72, 2641, 77, 2647, 82, 87, 2658, 2661, 92, 2663, 97, 102, 2676, 2674, 107, 2679, 112, 2684, 117, 2689, 2693, 122, 2698, 2697, 127, 2703, 2702, 132, 2706, 2709, 137, 142, 2715, 2719, 147, 152, 2726, 2731, 157, 161, 162, 2736, 167, 2742, 2747, 172, 2748, 177, 2758, 187, 2764, 192, 2768, 197, 2773, 2778, 202, 207, 2782, 211, 216, 2792, 2794, 221, 2797, 224, 227, 2803, 229, 232, 2808, 2813, 2816, 242, 2819, 2823, 247, 2824, 249, 252, 2833, 257, 2834, 261, 2837, 263, 266, 268, 2844, 273, 274, 2848, 277, 2852, 282, 2859, 2857, 2863, 2864, 292, 2868, 297, 2873, 303, 2878, 308, 2884, 313, 2889, 2892, 2896, 2897, 323, 2901, 2909, 332, 2912, 337, 2924, 346, 2931, 2932, 356, 2939, 361, 362, 2944, 367, 2951, 2952, 373, 2953, 378, 379, 2963, 2968, 389, 2973, 2974, 399, 2978, 406, 411, 2992, 418, 2994, 2998, 422, 2999, 428, 3003, 433, 3011, 439, 444, 449, 3027, 3026, 3029, 454, 3034, 459, 464, 3039, 3044, 477, 3052, 482, 3057, 487, 3062, 490, 3065, 3066, 3068, 496, 3071, 501, 3077, 3081, 506, 3086, 510, 3088, 3091, 511, 3089, 517, 3100, 522, 3105, 3109, 532, 537, 538, 3114, 542, 547, 3123, 552, 3129, 557, 562, 3139, 566, 567, 572, 3148, 576, 3152, 581, 584, 586, 3166, 594, 596, 601, 3180, 606, 3182, 3181, 611, 3185, 3184, 616, 3188, 618, 622, 3193, 627, 3198, 633, 3203, 636, 638, 3208, 3214, 643, 648, 3222, 3223, 653, 658, 3229, 3228, 662, 3233, 668, 3236, 672, 677, 3246, 679, 3248, 681, 3251, 3256, 685, 3257, 689, 3262, 692, 694, 3266, 699, 704, 3276, 709, 711, 716, 3287, 3289, 3295, 3296, 728, 3299, 732, 3301, 3306, 3309, 742, 3312, 3318, 746, 3322, 756, 3327, 761, 3333, 764, 3338, 771, 3343, 774, 3351, 3353, 781, 3358, 783, 3360, 786, 3366, 3368, 794, 797, 799, 3379, 3383, 804, 3387, 3384, 808, 819, 824, 829, 834, 838, 844, 849, 851, 852, 857, 863, 866, 867, 877, 883, 886, 888, 891, 893, 896, 902, 907, 910, 911, 916, 921, 932, 938, 942, 948, 959, 963, 964, 969, 974, 978, 983, 989, 996, 1000, 1004, 1011, 1014, 1017, 1018, 1019, 1022, 1023, 1033, 1038, 1042, 1048, 1053, 1058, 1063, 1068, 1073, 1077, 1080, 1081, 1086, 1088, 1089, 1092, 1097, 1101, 1106, 1111, 1119, 1124, 1129, 1134, 1137, 1139, 1143, 1144, 1145, 1148, 1153, 1158, 1163, 1173, 1178, 1183, 1187, 1188, 1189, 1193, 1199, 1203, 1208, 1211, 1212, 1214, 1217, 1218, 1222, 1223, 1227, 1232, 1236, 1244, 1248, 1252, 1258, 1263, 1265, 1267, 1272, 1277, 1287, 1293, 1294, 1298, 1299, 1305, 1307, 1312, 1317, 1322, 1328, 1332, 1333, 1344, 1347, 1351, 1357, 1359, 1361, 1364, 1366, 1373, 1376, 1380, 1389, 1396, 1399, 1402, 1407, 1411, 1416, 1421, 1426, 1431, 1436, 1438, 1441, 1446, 1451, 1459, 1463, 1471, 1473, 1474, 1477, 1482, 1484, 1488, 1493, 1502, 1507, 1512, 1515, 1516, 1519, 1521, 1526, 1532, 1533, 1537, 1542, 1547, 1552, 1553, 1556, 1561, 1562, 1569, 1571, 1576, 1580, 1584, 1586, 1587, 1591, 1599, 1604, 1608, 1609, 1610, 1613, 1618, 1622, 1627, 1629, 1633, 1639, 1642, 1649, 1654, 1659, 1664, 1669, 1672, 1674, 1676, 1679, 1682, 1683, 1684, 1687, 1692, 1716, 1721, 1734, 1739, 1744, 1745, 1749, 1751, 1759, 1764, 1768, 1773, 1775, 1777, 1782, 1783, 1792, 1801, 1805, 1811, 1821, 1822, 1827, 1829, 1836, 1844, 1846, 1847, 1851, 1860, 1866, 1868, 1874, 1876, 1884, 1886, 1891, 1894, 1896, 1900, 1904, 1908, 1910, 1918, 1919, 1924, 1932, 1942, 1948, 1953, 1958, 1959, 1963, 1966, 1969, 1970, 1974, 1978, 1983, 1987, 1988, 1989, 1990, 1992, 1997, 1998, 2003, 2009, 2011, 2014, 2016, 2027, 2029, 2049, 2056, 2066, 2071, 2075, 2076, 2077, 2079, 2083, 2089, 2090, 2098, 2103, 2104, 2105, 2106, 2107, 2112, 2117, 2118, 2122, 2123, 2128, 2133, 2137, 2138, 2143, 2148, 2152, 2156, 2161, 2166, 2172, 2176, 2178, 2182, 2185, 2187, 2192, 2198, 2201, 2204, 2208, 2209, 2213, 2214, 2219, 2222, 2226, 2227, 2234, 2246, 2254, 2258, 2263, 2265, 2267, 2273, 2282, 2283, 2284, 2287, 2291, 2299, 2302, 2309, 2316, 2319, 2321, 2326, 2329, 2331, 2335, 2344, 2351, 2379, 2381, 2383, 2385, 2387, 2393, 2399, 2408, 2413, 2414, 2417, 2419, 2421, 2426, 2427, 2430, 2433, 2452, 2456, 2461, 2463, 2471, 2476, 2479, 2485, 2486, 2487, 2491, 2500, 2504, 2507, 2509, 2511, 2513, 2528, 2536, 2538, 2546, 2552, 2558, 2567, 3, 8, 13, 18, 23, 28, 33, 38, 43, 48, 54, 59, 64, 69, 74, 79, 84, 89, 94, 99, 104, 109, 114, 119, 124, 129, 134, 139, 144, 149, 154, 159, 166, 176, 181, 186, 191, 196, 201, 206, 214, 219, 228, 234, 241, 246, 253, 258, 267, 272, 275, 276, 279, 284, 288, 293, 298, 304, 309, 314, 318, 322, 326, 327, 328, 333, 338, 345, 349, 359, 368, 374, 381, 383, 384, 391, 397, 398, 404, 409, 417, 421, 432, 437, 443, 447, 452, 458, 463, 468, 471, 479, 484, 489, 497, 502, 507, 514, 519, 524, 529, 534, 541, 546, 554, 559, 564, 571, 579, 583, 588, 598, 603, 608, 613, 626, 630, 637, 642, 647, 657, 660, 663, 669, 671, 676, 680, 683, 687, 693, 698, 707, 708, 717, 723, 727, 734, 738, 743, 748, 751, 752, 757, 762, 766, 767, 769, 777, 782, 788, 790, 791, 802, 809, 818, 823, 828, 833, 835, 837, 843, 848, 859, 869, 871, 874, 881, 892, 899, 904, 909, 919, 926, 929, 936, 937, 939, 940, 943, 949, 958, 966, 971, 975, 979, 987, 993, 998, 1002, 1008, 1012, 1013, 1026, 1034, 1039, 1051, 1056, 1057, 1059, 1064, 1066, 1071, 1078, 1079, 1083, 1093, 1098, 1100, 1103, 1107, 1118, 1126, 1127, 1131, 1136, 1142, 1149, 1154, 1159, 1164, 1166, 1168, 1172, 1179, 1184, 1192, 1201, 1206, 1213, 1219, 1221, 1226, 1231, 1235, 1239, 1243, 1247, 1256, 1262, 1268, 1271, 1276, 1281, 1282, 1284, 1291, 1297, 1304, 1306, 1308, 1313, 1318, 1324, 1329, 1336, 1338, 1341, 1345, 1349, 1353, 1358, 1362, 1369, 1371, 1374, 1381, 1387, 1394, 1398, 1408, 1409, 1414, 1420, 1424, 1429, 1433, 1434, 1437, 1442, 1447, 1452, 1455, 1456, 1458, 1472, 1479, 1487, 1492, 1501, 1506, 1511, 1517, 1527, 1528, 1534, 1539, 1544, 1557, 1563, 1567, 1573, 1575, 1577, 1581, 1592, 1597, 1603, 1611, 1616, 1620, 1624, 1628, 1636, 1641, 1647, 1663, 1677, 1685, 1688, 1689, 1691, 1693, 1696, 1701, 1704, 1706, 1707, 1711, 1714, 1723, 1726, 1730, 1731, 1738, 1748, 1753, 1756, 1761, 1763, 1766, 1771, 1776, 1784, 1791, 1797, 1799, 1803, 1823, 1830, 1831, 1832, 1837, 1854, 1855, 1856, 1863, 1869, 1872, 1873, 1879, 1883, 1888, 1892, 1899, 1902, 1921, 1923, 1930, 1936, 1938, 1947, 1952, 1954, 1956, 1967, 1972, 1973, 1976, 1980, 1991, 1996, 2017, 2024, 2026, 2032, 2033, 2042, 2044, 2048, 2052, 2062, 2072, 2073, 2078, 2081, 2082, 2088, 2094, 2102, 2111, 2119, 2131, 2139, 2142, 2144, 2149, 2150, 2151, 2153, 2155, 2157, 2158, 2159, 2162, 2167, 2173, 2177, 2197, 2203, 2206, 2207, 2217, 2223, 2232, 2236, 2237, 2238, 2244, 2250, 2251, 2256, 2261, 2266, 2271, 2274, 2276, 2277, 2288, 2290, 2308, 2314, 2322, 2327, 2333, 2337, 2341, 2342, 2347, 2353, 2359, 2367, 2372, 2378, 2380, 2384, 2391, 2397, 2409, 2415, 2416, 2428, 2434, 2439, 2440, 2441, 2444, 2446, 2447, 2449, 2451, 2457, 2459, 2469, 2477, 2481, 2483, 2492, 2495, 2497, 2501, 2506, 2512, 2519, 2524, 2527, 2541, 2543, 2549, 2551, 2559, 2562, 2565, 2568, 2574, 2580, 2582, 2583, 2585, 2588, 2589, 2592, 2598, 2600, 2608, 2609, 2612, 2616, 2622, 2642, 2648, 2654, 2655, 2662, 2667, 2668, 2672, 2686, 2691, 2705, 2707, 2712, 2721, 2724, 2732, 2739, 2741, 2751, 2769, 2774, 2777, 2783, 2796, 2804, 2817, 2821, 2822, 2828, 2842, 2843, 2846, 2851, 2853, 2855, 2862, 2874, 2879, 2886, 2888, 2902, 2904, 2906, 2911, 2913, 2914, 2921, 2922, 2933, 2937, 2948, 2954, 2958, 2961, 2964, 2967, 2971, 2979, 2983, 2984, 2987, 2997, 3002, 3008, 3012, 3014, 3016, 3019, 3022, 3028, 3031, 3035, 3037, 3043, 3046, 3048, 3054, 3056, 3061, 3067, 3073, 3079, 3083, 3092, 3097, 3102, 3106, 3108, 3111, 3117, 3118, 3122, 3127, 3133, 3136, 3147, 3151, 3157, 3160, 3167, 3170, 3172, 3174, 3178, 3187, 3192, 3197, 3199, 3206, 3217, 3220, 3225, 3230, 3232, 3235, 3237, 3238, 3239, 3243, 3249, 3265, 3267, 3271, 3272, 3274, 3282, 3283, 3288, 3291, 3297, 3302, 3304, 3311, 3324, 3331, 3337, 3347, 3349, 3359, 3369, 3376, 3388, 3389, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 56, 61, 66, 71, 76, 81, 86, 91, 96, 101, 106, 111, 116, 121, 126, 131, 136, 141, 146, 151, 156, 163, 168, 178, 184, 189, 194, 199, 204, 209, 210, 213, 218, 226, 233, 238, 243, 248, 254, 262, 269, 278, 283, 287, 291, 296, 302, 307, 312, 317, 321, 325, 329, 334, 339, 342, 343, 347, 351, 353, 354, 366, 372, 377, 387, 394, 395, 403, 408, 416, 420, 426, 427, 431, 436, 442, 446, 448, 453, 462, 467, 469, 472, 483, 488, 493, 498, 503, 508, 516, 521, 526, 527, 528, 533, 539, 544, 549, 556, 561, 568, 573, 575, 577, 582, 587, 591, 592, 597, 602, 607, 612, 617, 621, 624, 628, 632, 639, 644, 649, 652, 654, 659, 661, 667, 670, 673, 678, 682, 686, 691, 697, 702, 703, 714, 722, 726, 731, 736, 739, 747, 754, 759, 779, 787, 793, 795, 801, 806, 811, 814, 816, 821, 826, 831, 841, 846, 854, 861, 865, 868, 879, 884, 885, 889, 898, 903, 908, 913, 914, 918, 924, 928, 933, 944, 951, 953, 954, 961, 967, 972, 976, 981, 991, 997, 1001, 1007, 1024, 1027, 1028, 1031, 1036, 1043, 1045, 1049, 1054, 1070, 1074, 1084, 1094, 1099, 1102, 1114, 1116, 1121, 1132, 1138, 1146, 1151, 1156, 1161, 1169, 1174, 1176, 1177, 1182, 1191, 1196, 1197, 1198, 1202, 1207, 1210, 1228, 1233, 1237, 1251, 1257, 1259, 1261, 1266, 1269, 1273, 1278, 1286, 1292, 1303, 1309, 1314, 1319, 1326, 1331, 1337, 1339, 1343, 1348, 1352, 1354, 1360, 1363, 1379, 1384, 1388, 1395, 1403, 1410, 1412, 1417, 1423, 1427, 1432, 1439, 1443, 1448, 1453, 1461, 1464, 1466, 1478, 1481, 1489, 1494, 1495, 1496, 1497, 1499, 1504, 1509, 1514, 1522, 1536, 1541, 1546, 1549, 1551, 1559, 1566, 1568, 1578, 1583, 1594, 1601, 1607, 1614, 1619, 1623, 1626, 1631, 1637, 1643, 1648, 1652, 1657, 1658, 1662, 1666, 1668, 1673, 1681, 1686, 1699, 1703, 1709, 1713, 1718, 1724, 1727, 1728, 1732, 1733, 1736, 1741, 1746, 1747, 1752, 1755, 1757, 1762, 1765, 1769, 1774, 1781, 1788, 1794, 1795, 1796, 1798, 1802, 1806, 1808, 1813, 1816, 1818, 1824, 1826, 1828, 1833, 1839, 1842, 1843, 1848, 1849, 1850, 1853, 1857, 1867, 1871, 1881, 1889, 1895, 1897, 1901, 1903, 1905, 1907, 1914, 1916, 1922, 1925, 1926, 1929, 1933, 1934, 1939, 1941, 1944, 1951, 1957, 1971, 1977, 1986, 1994, 2001, 2004, 2012, 2021, 2022, 2036, 2037, 2045, 2051, 2059, 2063, 2067, 2068, 2086, 2093, 2099, 2108, 2113, 2121, 2126, 2127, 2132, 2147, 2164, 2171, 2175, 2183, 2184, 2191, 2199, 2212, 2218, 2228, 2239, 2243, 2248, 2249, 2252, 2259, 2264, 2269, 2272, 2286, 2292, 2294, 2296, 2304, 2311, 2317, 2324, 2328, 2334, 2338, 2348, 2354, 2362, 2369, 2373, 2375, 2376, 2386, 2392, 2394, 2396, 2402, 2403, 2404, 2405, 2406, 2411, 2412, 2418, 2420, 2422, 2423, 2424, 2431, 2438, 2442, 2448, 2450, 2454, 2458, 2467, 2474, 2478, 2482, 2489, 2493, 2494, 2498, 2503, 2510, 2518, 2521, 2526, 2529, 2537, 2539, 2542, 2548, 2553, 2554, 2561, 2564, 2566, 2569, 2571, 2573, 2579, 2593, 2599, 2611, 2613, 2617, 2629, 2634, 2639, 2646, 2652, 2656, 2657, 2664, 2678, 2681, 2683, 2688, 2699, 2701, 2710, 2714, 2723, 2729, 2734, 2740, 2743, 2754, 2757, 2759, 2762, 2763, 2765, 2767, 2772, 2776, 2779, 2786, 2787, 2791, 2799, 2801, 2802, 2807, 2811, 2812, 2826, 2832, 2839, 2845, 2847, 2856, 2866, 2876, 2882, 2887, 2891, 2894, 2899, 2907, 2923, 2927, 2929, 2934, 2942, 2943, 2946, 2949, 2962, 2966, 2969, 2972, 2976, 2977, 2986, 2988, 3000, 3004, 3021, 3023, 3033, 3042, 3049, 3064, 3072, 3078, 3094, 3099, 3104, 3113, 3119, 3121, 3126, 3131, 3135, 3142, 3144, 3145, 3149, 3153, 3158, 3162, 3168, 3169, 3171, 3176, 3179, 3183, 3186, 3191, 3196, 3201, 3202, 3207, 3211, 3212, 3218, 3221, 3227, 3231, 3241, 3244, 3258, 3261, 3263, 3264, 3268, 3277, 3281, 3284, 3286, 3294, 3298, 3303, 3307, 3317, 3320, 3321, 3329, 3336, 3342, 3361, 3362, 3363, 3371, 3381]\n","2740 651\n","<class 'list'> <class 'list'>\n","[0, 2050, 2053, 5, 2055, 10, 2060, 15, 2065, 20, 2070, 25, 30, 35, 2084, 2085, 40, 45, 2095, 2097, 50, 53, 55, 60, 2110, 65, 2115, 2116, 70, 2120, 75, 2125, 80, 2130, 85, 2136, 90, 2140, 95, 2145, 100, 105, 110, 2160, 115, 2165, 2168, 120, 2170, 125, 130, 2180, 2181, 135, 2186, 140, 2188, 2190, 145, 2195, 2196, 150, 2200, 155, 2205, 160, 2210, 165, 2215, 170, 2220, 175, 2224, 2225, 180, 2230, 185, 2235, 190, 2240, 195, 2245, 200, 205, 2260, 215, 220, 222, 225, 230, 2278, 2280, 2281, 235, 2285, 240, 245, 2295, 250, 2300, 2301, 2303, 255, 2305, 2306, 259, 260, 2310, 2312, 265, 2315, 270, 2320, 2325, 280, 2330, 285, 290, 2340, 295, 2345, 2346, 300, 301, 2350, 2352, 305, 2355, 310, 2358, 2360, 315, 2363, 2365, 320, 2368, 2370, 330, 335, 340, 2389, 2390, 2395, 350, 2398, 2400, 355, 358, 360, 2410, 365, 369, 370, 375, 2425, 380, 2432, 385, 2435, 388, 2437, 390, 392, 2445, 400, 402, 2453, 405, 2455, 410, 412, 413, 2460, 415, 2468, 2470, 424, 425, 430, 2480, 435, 2484, 438, 440, 2488, 2490, 445, 450, 2499, 455, 2505, 460, 2508, 465, 2514, 2515, 470, 2520, 473, 474, 2523, 475, 2522, 2525, 480, 2530, 2531, 485, 2534, 2535, 2540, 495, 2545, 2547, 500, 2550, 505, 2555, 2557, 509, 2560, 513, 515, 520, 525, 2575, 2578, 530, 535, 2586, 540, 545, 2594, 2595, 550, 2601, 555, 2607, 560, 565, 2615, 570, 2618, 2620, 2624, 580, 2630, 585, 2635, 590, 2640, 595, 2644, 2645, 600, 2649, 2650, 2653, 605, 610, 2660, 615, 2665, 619, 620, 2670, 2671, 625, 2675, 629, 631, 2680, 635, 2685, 640, 2690, 2692, 645, 2694, 2695, 650, 2700, 655, 2711, 664, 665, 2716, 675, 2725, 2727, 2730, 2735, 2737, 690, 2738, 695, 2744, 2745, 700, 2750, 705, 2755, 710, 712, 2761, 2760, 715, 718, 719, 720, 2770, 725, 2775, 735, 2785, 740, 2790, 2795, 750, 2798, 2800, 755, 2805, 760, 2809, 765, 2814, 2815, 770, 2820, 775, 780, 2829, 2830, 785, 2835, 2840, 798, 800, 2854, 2858, 810, 2860, 813, 815, 2865, 820, 2872, 825, 2875, 830, 2880, 2881, 2885, 839, 840, 2890, 845, 2893, 2895, 850, 2900, 853, 855, 856, 2903, 2905, 860, 862, 2910, 2915, 2917, 870, 2918, 2920, 875, 2925, 880, 2930, 2935, 890, 2940, 895, 897, 900, 2950, 905, 2955, 2960, 915, 2965, 920, 2970, 922, 925, 2975, 930, 2980, 934, 935, 2989, 2990, 2993, 945, 947, 2995, 950, 955, 956, 3005, 3006, 960, 3009, 965, 3013, 3015, 970, 3020, 3025, 980, 3030, 984, 985, 990, 992, 3040, 995, 3045, 3047, 3050, 3051, 1005, 1006, 3055, 1010, 3060, 1015, 1016, 1020, 3070, 1025, 3074, 3075, 1029, 1030, 3080, 3082, 1035, 3084, 3085, 3090, 1044, 1046, 3095, 3096, 1050, 3101, 1055, 1060, 3110, 1065, 1069, 3120, 3125, 3128, 3132, 1085, 1087, 3138, 1090, 3140, 1096, 1105, 3154, 3155, 1110, 1112, 1113, 1115, 1120, 1122, 1125, 3173, 3175, 1130, 1135, 1140, 3190, 3195, 1150, 3200, 1155, 3205, 1160, 3210, 3213, 1165, 3215, 1170, 1175, 3226, 1180, 1185, 1190, 3240, 1195, 3245, 1200, 3250, 3253, 1205, 3255, 3260, 1215, 1220, 3269, 1225, 3275, 3278, 1230, 3280, 3285, 3290, 3300, 1253, 1255, 3305, 3308, 1260, 3310, 3313, 3315, 1270, 1275, 3325, 1280, 3330, 3332, 1285, 3335, 1288, 1290, 3340, 1295, 3344, 3345, 1300, 3348, 3350, 3352, 3354, 3355, 1310, 1315, 3365, 3367, 1320, 3370, 1323, 1325, 3373, 3375, 1330, 3380, 1335, 3385, 3386, 1340, 3390, 1350, 1355, 1365, 1370, 1375, 1385, 1390, 1391, 1392, 1400, 1401, 1405, 1415, 1419, 1425, 1430, 1435, 1440, 1445, 1449, 1460, 1465, 1468, 1470, 1475, 1476, 1480, 1483, 1485, 1490, 1500, 1505, 1510, 1520, 1525, 1529, 1530, 1535, 1540, 1545, 1550, 1560, 1565, 1570, 1572, 1582, 1585, 1590, 1602, 1605, 1615, 1625, 1630, 1632, 1635, 1638, 1640, 1644, 1650, 1655, 1660, 1667, 1670, 1675, 1690, 1695, 1700, 1710, 1715, 1720, 1725, 1735, 1740, 1742, 1743, 1750, 1760, 1770, 1779, 1780, 1785, 1800, 1807, 1810, 1815, 1817, 1820, 1825, 1835, 1838, 1840, 1845, 1859, 1862, 1865, 1870, 1875, 1877, 1880, 1890, 1893, 1911, 1912, 1915, 1920, 1935, 1940, 1945, 1946, 1950, 1955, 1960, 1961, 1962, 1975, 1981, 1985, 1995, 2000, 2002, 2005, 2007, 2010, 2015, 2018, 2019, 2020, 2023, 2025, 2028, 2030, 2035, 2038, 2040, 1, 6, 2576, 11, 2581, 2584, 16, 21, 2590, 26, 2596, 31, 2602, 2603, 36, 41, 46, 2614, 51, 2621, 2625, 57, 2626, 2628, 62, 2632, 2633, 2631, 67, 2636, 2638, 72, 2641, 77, 2647, 82, 87, 2658, 2661, 92, 2663, 97, 102, 2676, 2674, 107, 2679, 112, 2684, 117, 2689, 2693, 122, 2698, 2697, 127, 2703, 2702, 132, 2706, 2709, 137, 142, 2715, 2719, 147, 152, 2726, 2731, 157, 161, 162, 2736, 167, 2742, 2747, 172, 2748, 177, 2758, 187, 2764, 192, 2768, 197, 2773, 2778, 202, 207, 2782, 211, 216, 2792, 2794, 221, 2797, 224, 227, 2803, 229, 232, 2808, 2813, 2816, 242, 2819, 2823, 247, 2824, 249, 252, 2833, 257, 2834, 261, 2837, 263, 266, 268, 2844, 273, 274, 2848, 277, 2852, 282, 2859, 2857, 2863, 2864, 292, 2868, 297, 2873, 303, 2878, 308, 2884, 313, 2889, 2892, 2896, 2897, 323, 2901, 2909, 332, 2912, 337, 2924, 346, 2931, 2932, 356, 2939, 361, 362, 2944, 367, 2951, 2952, 373, 2953, 378, 379, 2963, 2968, 389, 2973, 2974, 399, 2978, 406, 411, 2992, 418, 2994, 2998, 422, 2999, 428, 3003, 433, 3011, 439, 444, 449, 3027, 3026, 3029, 454, 3034, 459, 464, 3039, 3044, 477, 3052, 482, 3057, 487, 3062, 490, 3065, 3066, 3068, 496, 3071, 501, 3077, 3081, 506, 3086, 510, 3088, 3091, 511, 3089, 517, 3100, 522, 3105, 3109, 532, 537, 538, 3114, 542, 547, 3123, 552, 3129, 557, 562, 3139, 566, 567, 572, 3148, 576, 3152, 581, 584, 586, 3166, 594, 596, 601, 3180, 606, 3182, 3181, 611, 3185, 3184, 616, 3188, 618, 622, 3193, 627, 3198, 633, 3203, 636, 638, 3208, 3214, 643, 648, 3222, 3223, 653, 658, 3229, 3228, 662, 3233, 668, 3236, 672, 677, 3246, 679, 3248, 681, 3251, 3256, 685, 3257, 689, 3262, 692, 694, 3266, 699, 704, 3276, 709, 711, 716, 3287, 3289, 3295, 3296, 728, 3299, 732, 3301, 3306, 3309, 742, 3312, 3318, 746, 3322, 756, 3327, 761, 3333, 764, 3338, 771, 3343, 774, 3351, 3353, 781, 3358, 783, 3360, 786, 3366, 3368, 794, 797, 799, 3379, 3383, 804, 3387, 3384, 808, 819, 824, 829, 834, 838, 844, 849, 851, 852, 857, 863, 866, 867, 877, 883, 886, 888, 891, 893, 896, 902, 907, 910, 911, 916, 921, 932, 938, 942, 948, 959, 963, 964, 969, 974, 978, 983, 989, 996, 1000, 1004, 1011, 1014, 1017, 1018, 1019, 1022, 1023, 1033, 1038, 1042, 1048, 1053, 1058, 1063, 1068, 1073, 1077, 1080, 1081, 1086, 1088, 1089, 1092, 1097, 1101, 1106, 1111, 1119, 1124, 1129, 1134, 1137, 1139, 1143, 1144, 1145, 1148, 1153, 1158, 1163, 1173, 1178, 1183, 1187, 1188, 1189, 1193, 1199, 1203, 1208, 1211, 1212, 1214, 1217, 1218, 1222, 1223, 1227, 1232, 1236, 1244, 1248, 1252, 1258, 1263, 1265, 1267, 1272, 1277, 1287, 1293, 1294, 1298, 1299, 1305, 1307, 1312, 1317, 1322, 1328, 1332, 1333, 1344, 1347, 1351, 1357, 1359, 1361, 1364, 1366, 1373, 1376, 1380, 1389, 1396, 1399, 1402, 1407, 1411, 1416, 1421, 1426, 1431, 1436, 1438, 1441, 1446, 1451, 1459, 1463, 1471, 1473, 1474, 1477, 1482, 1484, 1488, 1493, 1502, 1507, 1512, 1515, 1516, 1519, 1521, 1526, 1532, 1533, 1537, 1542, 1547, 1552, 1553, 1556, 1561, 1562, 1569, 1571, 1576, 1580, 1584, 1586, 1587, 1591, 1599, 1604, 1608, 1609, 1610, 1613, 1618, 1622, 1627, 1629, 1633, 1639, 1642, 1649, 1654, 1659, 1664, 1669, 1672, 1674, 1676, 1679, 1682, 1683, 1684, 1687, 1692, 1716, 1721, 1734, 1739, 1744, 1745, 1749, 1751, 1759, 1764, 1768, 1773, 1775, 1777, 1782, 1783, 1792, 1801, 1805, 1811, 1821, 1822, 1827, 1829, 1836, 1844, 1846, 1847, 1851, 1860, 1866, 1868, 1874, 1876, 1884, 1886, 1891, 1894, 1896, 1900, 1904, 1908, 1910, 1918, 1919, 1924, 1932, 1942, 1948, 1953, 1958, 1959, 1963, 1966, 1969, 1970, 1974, 1978, 1983, 1987, 1988, 1989, 1990, 1992, 1997, 1998, 2003, 2009, 2011, 2014, 2016, 2027, 2029, 2049, 2056, 2066, 2071, 2075, 2076, 2077, 2079, 2083, 2089, 2090, 2098, 2103, 2104, 2105, 2106, 2107, 2112, 2117, 2118, 2122, 2123, 2128, 2133, 2137, 2138, 2143, 2148, 2152, 2156, 2161, 2166, 2172, 2176, 2178, 2182, 2185, 2187, 2192, 2198, 2201, 2204, 2208, 2209, 2213, 2214, 2219, 2222, 2226, 2227, 2234, 2246, 2254, 2258, 2263, 2265, 2267, 2273, 2282, 2283, 2284, 2287, 2291, 2299, 2302, 2309, 2316, 2319, 2321, 2326, 2329, 2331, 2335, 2344, 2351, 2379, 2381, 2383, 2385, 2387, 2393, 2399, 2408, 2413, 2414, 2417, 2419, 2421, 2426, 2427, 2430, 2433, 2452, 2456, 2461, 2463, 2471, 2476, 2479, 2485, 2486, 2487, 2491, 2500, 2504, 2507, 2509, 2511, 2513, 2528, 2536, 2538, 2546, 2552, 2558, 2567, 2, 7, 12, 17, 22, 27, 32, 37, 42, 47, 52, 58, 63, 68, 73, 78, 83, 88, 93, 98, 103, 108, 113, 118, 123, 128, 133, 138, 143, 148, 153, 158, 164, 169, 171, 173, 174, 179, 182, 183, 188, 193, 198, 203, 208, 212, 217, 223, 231, 236, 237, 239, 244, 251, 256, 264, 271, 281, 286, 289, 294, 299, 306, 311, 316, 319, 324, 331, 336, 341, 344, 348, 352, 357, 363, 364, 371, 376, 382, 386, 393, 396, 401, 407, 414, 419, 423, 429, 434, 441, 451, 456, 457, 461, 466, 476, 478, 481, 486, 491, 492, 494, 499, 504, 512, 518, 523, 531, 536, 543, 548, 551, 553, 558, 563, 569, 574, 578, 589, 593, 599, 604, 609, 614, 623, 634, 641, 646, 651, 656, 666, 674, 684, 688, 696, 701, 706, 713, 721, 724, 729, 730, 733, 737, 741, 744, 745, 749, 753, 758, 763, 768, 772, 773, 776, 778, 784, 789, 792, 796, 803, 805, 807, 812, 817, 822, 827, 832, 836, 842, 847, 858, 864, 872, 873, 876, 878, 882, 887, 894, 901, 906, 912, 917, 923, 927, 931, 941, 946, 952, 957, 962, 968, 973, 977, 982, 986, 988, 994, 999, 1003, 1009, 1021, 1032, 1037, 1040, 1041, 1047, 1052, 1061, 1062, 1067, 1072, 1075, 1076, 1082, 1091, 1095, 1104, 1108, 1109, 1117, 1123, 1128, 1133, 1141, 1147, 1152, 1157, 1162, 1167, 1171, 1181, 1186, 1194, 1204, 1209, 1216, 1224, 1229, 1234, 1238, 1240, 1241, 1242, 1245, 1246, 1249, 1250, 1254, 1264, 1274, 1279, 1283, 1289, 1296, 1301, 1302, 1311, 1316, 1321, 1327, 1334, 1342, 1346, 1356, 1367, 1368, 1372, 1377, 1378, 1382, 1383, 1386, 1393, 1397, 1404, 1406, 1413, 1418, 1422, 1428, 1444, 1450, 1454, 1457, 1462, 1467, 1469, 1486, 1491, 1498, 1503, 1508, 1513, 1518, 1523, 1524, 1531, 1538, 1543, 1548, 1554, 1555, 1558, 1564, 1574, 1579, 1588, 1589, 1593, 1595, 1596, 1598, 1600, 1606, 1612, 1617, 1621, 1634, 1645, 1646, 1651, 1653, 1656, 1661, 1665, 1671, 1678, 1680, 1694, 1697, 1698, 1702, 1705, 1708, 1712, 1717, 1719, 1722, 1729, 1737, 1754, 1758, 1767, 1772, 1778, 1786, 1787, 1789, 1790, 1793, 1804, 1809, 1812, 1814, 1819, 1834, 1841, 1852, 1858, 1861, 1864, 1878, 1882, 1885, 1887, 1898, 1906, 1909, 1913, 1917, 1927, 1928, 1931, 1937, 1943, 1949, 1964, 1965, 1968, 1979, 1982, 1984, 1993, 1999, 2006, 2008, 2013, 2031, 2034, 2039, 2041, 2043, 2046, 2047, 2054, 2057, 2058, 2061, 2064, 2069, 2074, 2080, 2087, 2091, 2092, 2096, 2100, 2101, 2109, 2114, 2124, 2129, 2134, 2135, 2141, 2146, 2154, 2163, 2169, 2174, 2179, 2189, 2193, 2194, 2202, 2211, 2216, 2221, 2229, 2231, 2233, 2241, 2242, 2247, 2253, 2255, 2257, 2262, 2268, 2270, 2275, 2279, 2289, 2293, 2297, 2298, 2307, 2313, 2318, 2323, 2332, 2336, 2339, 2343, 2349, 2356, 2357, 2361, 2364, 2366, 2371, 2374, 2377, 2382, 2388, 2401, 2407, 2429, 2436, 2443, 2462, 2464, 2465, 2466, 2472, 2473, 2475, 2496, 2502, 2516, 2517, 2532, 2533, 2544, 2556, 2563, 2570, 2572, 2577, 2587, 2591, 2597, 2604, 2605, 2606, 2610, 2619, 2623, 2627, 2637, 2643, 2651, 2659, 2666, 2669, 2673, 2677, 2682, 2687, 2696, 2704, 2708, 2713, 2717, 2718, 2720, 2722, 2728, 2733, 2746, 2749, 2752, 2753, 2756, 2766, 2771, 2780, 2781, 2784, 2788, 2789, 2793, 2806, 2810, 2818, 2825, 2827, 2831, 2836, 2838, 2841, 2849, 2850, 2861, 2867, 2869, 2870, 2871, 2877, 2883, 2898, 2908, 2916, 2919, 2926, 2928, 2936, 2938, 2941, 2945, 2947, 2956, 2957, 2959, 2981, 2982, 2985, 2991, 2996, 3001, 3007, 3010, 3017, 3018, 3024, 3032, 3036, 3038, 3041, 3053, 3058, 3059, 3063, 3069, 3076, 3087, 3093, 3098, 3103, 3107, 3112, 3115, 3116, 3124, 3130, 3134, 3137, 3141, 3143, 3146, 3150, 3156, 3159, 3161, 3163, 3164, 3165, 3177, 3189, 3194, 3204, 3209, 3216, 3219, 3224, 3234, 3242, 3247, 3252, 3254, 3259, 3270, 3273, 3279, 3292, 3293, 3314, 3316, 3319, 3323, 3326, 3328, 3334, 3339, 3341, 3346, 3356, 3357, 3364, 3372, 3374, 3377, 3378, 3382, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 56, 61, 66, 71, 76, 81, 86, 91, 96, 101, 106, 111, 116, 121, 126, 131, 136, 141, 146, 151, 156, 163, 168, 178, 184, 189, 194, 199, 204, 209, 210, 213, 218, 226, 233, 238, 243, 248, 254, 262, 269, 278, 283, 287, 291, 296, 302, 307, 312, 317, 321, 325, 329, 334, 339, 342, 343, 347, 351, 353, 354, 366, 372, 377, 387, 394, 395, 403, 408, 416, 420, 426, 427, 431, 436, 442, 446, 448, 453, 462, 467, 469, 472, 483, 488, 493, 498, 503, 508, 516, 521, 526, 527, 528, 533, 539, 544, 549, 556, 561, 568, 573, 575, 577, 582, 587, 591, 592, 597, 602, 607, 612, 617, 621, 624, 628, 632, 639, 644, 649, 652, 654, 659, 661, 667, 670, 673, 678, 682, 686, 691, 697, 702, 703, 714, 722, 726, 731, 736, 739, 747, 754, 759, 779, 787, 793, 795, 801, 806, 811, 814, 816, 821, 826, 831, 841, 846, 854, 861, 865, 868, 879, 884, 885, 889, 898, 903, 908, 913, 914, 918, 924, 928, 933, 944, 951, 953, 954, 961, 967, 972, 976, 981, 991, 997, 1001, 1007, 1024, 1027, 1028, 1031, 1036, 1043, 1045, 1049, 1054, 1070, 1074, 1084, 1094, 1099, 1102, 1114, 1116, 1121, 1132, 1138, 1146, 1151, 1156, 1161, 1169, 1174, 1176, 1177, 1182, 1191, 1196, 1197, 1198, 1202, 1207, 1210, 1228, 1233, 1237, 1251, 1257, 1259, 1261, 1266, 1269, 1273, 1278, 1286, 1292, 1303, 1309, 1314, 1319, 1326, 1331, 1337, 1339, 1343, 1348, 1352, 1354, 1360, 1363, 1379, 1384, 1388, 1395, 1403, 1410, 1412, 1417, 1423, 1427, 1432, 1439, 1443, 1448, 1453, 1461, 1464, 1466, 1478, 1481, 1489, 1494, 1495, 1496, 1497, 1499, 1504, 1509, 1514, 1522, 1536, 1541, 1546, 1549, 1551, 1559, 1566, 1568, 1578, 1583, 1594, 1601, 1607, 1614, 1619, 1623, 1626, 1631, 1637, 1643, 1648, 1652, 1657, 1658, 1662, 1666, 1668, 1673, 1681, 1686, 1699, 1703, 1709, 1713, 1718, 1724, 1727, 1728, 1732, 1733, 1736, 1741, 1746, 1747, 1752, 1755, 1757, 1762, 1765, 1769, 1774, 1781, 1788, 1794, 1795, 1796, 1798, 1802, 1806, 1808, 1813, 1816, 1818, 1824, 1826, 1828, 1833, 1839, 1842, 1843, 1848, 1849, 1850, 1853, 1857, 1867, 1871, 1881, 1889, 1895, 1897, 1901, 1903, 1905, 1907, 1914, 1916, 1922, 1925, 1926, 1929, 1933, 1934, 1939, 1941, 1944, 1951, 1957, 1971, 1977, 1986, 1994, 2001, 2004, 2012, 2021, 2022, 2036, 2037, 2045, 2051, 2059, 2063, 2067, 2068, 2086, 2093, 2099, 2108, 2113, 2121, 2126, 2127, 2132, 2147, 2164, 2171, 2175, 2183, 2184, 2191, 2199, 2212, 2218, 2228, 2239, 2243, 2248, 2249, 2252, 2259, 2264, 2269, 2272, 2286, 2292, 2294, 2296, 2304, 2311, 2317, 2324, 2328, 2334, 2338, 2348, 2354, 2362, 2369, 2373, 2375, 2376, 2386, 2392, 2394, 2396, 2402, 2403, 2404, 2405, 2406, 2411, 2412, 2418, 2420, 2422, 2423, 2424, 2431, 2438, 2442, 2448, 2450, 2454, 2458, 2467, 2474, 2478, 2482, 2489, 2493, 2494, 2498, 2503, 2510, 2518, 2521, 2526, 2529, 2537, 2539, 2542, 2548, 2553, 2554, 2561, 2564, 2566, 2569, 2571, 2573, 2579, 2593, 2599, 2611, 2613, 2617, 2629, 2634, 2639, 2646, 2652, 2656, 2657, 2664, 2678, 2681, 2683, 2688, 2699, 2701, 2710, 2714, 2723, 2729, 2734, 2740, 2743, 2754, 2757, 2759, 2762, 2763, 2765, 2767, 2772, 2776, 2779, 2786, 2787, 2791, 2799, 2801, 2802, 2807, 2811, 2812, 2826, 2832, 2839, 2845, 2847, 2856, 2866, 2876, 2882, 2887, 2891, 2894, 2899, 2907, 2923, 2927, 2929, 2934, 2942, 2943, 2946, 2949, 2962, 2966, 2969, 2972, 2976, 2977, 2986, 2988, 3000, 3004, 3021, 3023, 3033, 3042, 3049, 3064, 3072, 3078, 3094, 3099, 3104, 3113, 3119, 3121, 3126, 3131, 3135, 3142, 3144, 3145, 3149, 3153, 3158, 3162, 3168, 3169, 3171, 3176, 3179, 3183, 3186, 3191, 3196, 3201, 3202, 3207, 3211, 3212, 3218, 3221, 3227, 3231, 3241, 3244, 3258, 3261, 3263, 3264, 3268, 3277, 3281, 3284, 3286, 3294, 3298, 3303, 3307, 3317, 3320, 3321, 3329, 3336, 3342, 3361, 3362, 3363, 3371, 3381]\n","2744 647\n","<class 'list'> <class 'list'>\n","[0, 2050, 2053, 5, 2055, 10, 2060, 15, 2065, 20, 2070, 25, 30, 35, 2084, 2085, 40, 45, 2095, 2097, 50, 53, 55, 60, 2110, 65, 2115, 2116, 70, 2120, 75, 2125, 80, 2130, 85, 2136, 90, 2140, 95, 2145, 100, 105, 110, 2160, 115, 2165, 2168, 120, 2170, 125, 130, 2180, 2181, 135, 2186, 140, 2188, 2190, 145, 2195, 2196, 150, 2200, 155, 2205, 160, 2210, 165, 2215, 170, 2220, 175, 2224, 2225, 180, 2230, 185, 2235, 190, 2240, 195, 2245, 200, 205, 2260, 215, 220, 222, 225, 230, 2278, 2280, 2281, 235, 2285, 240, 245, 2295, 250, 2300, 2301, 2303, 255, 2305, 2306, 259, 260, 2310, 2312, 265, 2315, 270, 2320, 2325, 280, 2330, 285, 290, 2340, 295, 2345, 2346, 300, 301, 2350, 2352, 305, 2355, 310, 2358, 2360, 315, 2363, 2365, 320, 2368, 2370, 330, 335, 340, 2389, 2390, 2395, 350, 2398, 2400, 355, 358, 360, 2410, 365, 369, 370, 375, 2425, 380, 2432, 385, 2435, 388, 2437, 390, 392, 2445, 400, 402, 2453, 405, 2455, 410, 412, 413, 2460, 415, 2468, 2470, 424, 425, 430, 2480, 435, 2484, 438, 440, 2488, 2490, 445, 450, 2499, 455, 2505, 460, 2508, 465, 2514, 2515, 470, 2520, 473, 474, 2523, 475, 2522, 2525, 480, 2530, 2531, 485, 2534, 2535, 2540, 495, 2545, 2547, 500, 2550, 505, 2555, 2557, 509, 2560, 513, 515, 520, 525, 2575, 2578, 530, 535, 2586, 540, 545, 2594, 2595, 550, 2601, 555, 2607, 560, 565, 2615, 570, 2618, 2620, 2624, 580, 2630, 585, 2635, 590, 2640, 595, 2644, 2645, 600, 2649, 2650, 2653, 605, 610, 2660, 615, 2665, 619, 620, 2670, 2671, 625, 2675, 629, 631, 2680, 635, 2685, 640, 2690, 2692, 645, 2694, 2695, 650, 2700, 655, 2711, 664, 665, 2716, 675, 2725, 2727, 2730, 2735, 2737, 690, 2738, 695, 2744, 2745, 700, 2750, 705, 2755, 710, 712, 2761, 2760, 715, 718, 719, 720, 2770, 725, 2775, 735, 2785, 740, 2790, 2795, 750, 2798, 2800, 755, 2805, 760, 2809, 765, 2814, 2815, 770, 2820, 775, 780, 2829, 2830, 785, 2835, 2840, 798, 800, 2854, 2858, 810, 2860, 813, 815, 2865, 820, 2872, 825, 2875, 830, 2880, 2881, 2885, 839, 840, 2890, 845, 2893, 2895, 850, 2900, 853, 855, 856, 2903, 2905, 860, 862, 2910, 2915, 2917, 870, 2918, 2920, 875, 2925, 880, 2930, 2935, 890, 2940, 895, 897, 900, 2950, 905, 2955, 2960, 915, 2965, 920, 2970, 922, 925, 2975, 930, 2980, 934, 935, 2989, 2990, 2993, 945, 947, 2995, 950, 955, 956, 3005, 3006, 960, 3009, 965, 3013, 3015, 970, 3020, 3025, 980, 3030, 984, 985, 990, 992, 3040, 995, 3045, 3047, 3050, 3051, 1005, 1006, 3055, 1010, 3060, 1015, 1016, 1020, 3070, 1025, 3074, 3075, 1029, 1030, 3080, 3082, 1035, 3084, 3085, 3090, 1044, 1046, 3095, 3096, 1050, 3101, 1055, 1060, 3110, 1065, 1069, 3120, 3125, 3128, 3132, 1085, 1087, 3138, 1090, 3140, 1096, 1105, 3154, 3155, 1110, 1112, 1113, 1115, 1120, 1122, 1125, 3173, 3175, 1130, 1135, 1140, 3190, 3195, 1150, 3200, 1155, 3205, 1160, 3210, 3213, 1165, 3215, 1170, 1175, 3226, 1180, 1185, 1190, 3240, 1195, 3245, 1200, 3250, 3253, 1205, 3255, 3260, 1215, 1220, 3269, 1225, 3275, 3278, 1230, 3280, 3285, 3290, 3300, 1253, 1255, 3305, 3308, 1260, 3310, 3313, 3315, 1270, 1275, 3325, 1280, 3330, 3332, 1285, 3335, 1288, 1290, 3340, 1295, 3344, 3345, 1300, 3348, 3350, 3352, 3354, 3355, 1310, 1315, 3365, 3367, 1320, 3370, 1323, 1325, 3373, 3375, 1330, 3380, 1335, 3385, 3386, 1340, 3390, 1350, 1355, 1365, 1370, 1375, 1385, 1390, 1391, 1392, 1400, 1401, 1405, 1415, 1419, 1425, 1430, 1435, 1440, 1445, 1449, 1460, 1465, 1468, 1470, 1475, 1476, 1480, 1483, 1485, 1490, 1500, 1505, 1510, 1520, 1525, 1529, 1530, 1535, 1540, 1545, 1550, 1560, 1565, 1570, 1572, 1582, 1585, 1590, 1602, 1605, 1615, 1625, 1630, 1632, 1635, 1638, 1640, 1644, 1650, 1655, 1660, 1667, 1670, 1675, 1690, 1695, 1700, 1710, 1715, 1720, 1725, 1735, 1740, 1742, 1743, 1750, 1760, 1770, 1779, 1780, 1785, 1800, 1807, 1810, 1815, 1817, 1820, 1825, 1835, 1838, 1840, 1845, 1859, 1862, 1865, 1870, 1875, 1877, 1880, 1890, 1893, 1911, 1912, 1915, 1920, 1935, 1940, 1945, 1946, 1950, 1955, 1960, 1961, 1962, 1975, 1981, 1985, 1995, 2000, 2002, 2005, 2007, 2010, 2015, 2018, 2019, 2020, 2023, 2025, 2028, 2030, 2035, 2038, 2040, 1, 6, 2576, 11, 2581, 2584, 16, 21, 2590, 26, 2596, 31, 2602, 2603, 36, 41, 46, 2614, 51, 2621, 2625, 57, 2626, 2628, 62, 2632, 2633, 2631, 67, 2636, 2638, 72, 2641, 77, 2647, 82, 87, 2658, 2661, 92, 2663, 97, 102, 2676, 2674, 107, 2679, 112, 2684, 117, 2689, 2693, 122, 2698, 2697, 127, 2703, 2702, 132, 2706, 2709, 137, 142, 2715, 2719, 147, 152, 2726, 2731, 157, 161, 162, 2736, 167, 2742, 2747, 172, 2748, 177, 2758, 187, 2764, 192, 2768, 197, 2773, 2778, 202, 207, 2782, 211, 216, 2792, 2794, 221, 2797, 224, 227, 2803, 229, 232, 2808, 2813, 2816, 242, 2819, 2823, 247, 2824, 249, 252, 2833, 257, 2834, 261, 2837, 263, 266, 268, 2844, 273, 274, 2848, 277, 2852, 282, 2859, 2857, 2863, 2864, 292, 2868, 297, 2873, 303, 2878, 308, 2884, 313, 2889, 2892, 2896, 2897, 323, 2901, 2909, 332, 2912, 337, 2924, 346, 2931, 2932, 356, 2939, 361, 362, 2944, 367, 2951, 2952, 373, 2953, 378, 379, 2963, 2968, 389, 2973, 2974, 399, 2978, 406, 411, 2992, 418, 2994, 2998, 422, 2999, 428, 3003, 433, 3011, 439, 444, 449, 3027, 3026, 3029, 454, 3034, 459, 464, 3039, 3044, 477, 3052, 482, 3057, 487, 3062, 490, 3065, 3066, 3068, 496, 3071, 501, 3077, 3081, 506, 3086, 510, 3088, 3091, 511, 3089, 517, 3100, 522, 3105, 3109, 532, 537, 538, 3114, 542, 547, 3123, 552, 3129, 557, 562, 3139, 566, 567, 572, 3148, 576, 3152, 581, 584, 586, 3166, 594, 596, 601, 3180, 606, 3182, 3181, 611, 3185, 3184, 616, 3188, 618, 622, 3193, 627, 3198, 633, 3203, 636, 638, 3208, 3214, 643, 648, 3222, 3223, 653, 658, 3229, 3228, 662, 3233, 668, 3236, 672, 677, 3246, 679, 3248, 681, 3251, 3256, 685, 3257, 689, 3262, 692, 694, 3266, 699, 704, 3276, 709, 711, 716, 3287, 3289, 3295, 3296, 728, 3299, 732, 3301, 3306, 3309, 742, 3312, 3318, 746, 3322, 756, 3327, 761, 3333, 764, 3338, 771, 3343, 774, 3351, 3353, 781, 3358, 783, 3360, 786, 3366, 3368, 794, 797, 799, 3379, 3383, 804, 3387, 3384, 808, 819, 824, 829, 834, 838, 844, 849, 851, 852, 857, 863, 866, 867, 877, 883, 886, 888, 891, 893, 896, 902, 907, 910, 911, 916, 921, 932, 938, 942, 948, 959, 963, 964, 969, 974, 978, 983, 989, 996, 1000, 1004, 1011, 1014, 1017, 1018, 1019, 1022, 1023, 1033, 1038, 1042, 1048, 1053, 1058, 1063, 1068, 1073, 1077, 1080, 1081, 1086, 1088, 1089, 1092, 1097, 1101, 1106, 1111, 1119, 1124, 1129, 1134, 1137, 1139, 1143, 1144, 1145, 1148, 1153, 1158, 1163, 1173, 1178, 1183, 1187, 1188, 1189, 1193, 1199, 1203, 1208, 1211, 1212, 1214, 1217, 1218, 1222, 1223, 1227, 1232, 1236, 1244, 1248, 1252, 1258, 1263, 1265, 1267, 1272, 1277, 1287, 1293, 1294, 1298, 1299, 1305, 1307, 1312, 1317, 1322, 1328, 1332, 1333, 1344, 1347, 1351, 1357, 1359, 1361, 1364, 1366, 1373, 1376, 1380, 1389, 1396, 1399, 1402, 1407, 1411, 1416, 1421, 1426, 1431, 1436, 1438, 1441, 1446, 1451, 1459, 1463, 1471, 1473, 1474, 1477, 1482, 1484, 1488, 1493, 1502, 1507, 1512, 1515, 1516, 1519, 1521, 1526, 1532, 1533, 1537, 1542, 1547, 1552, 1553, 1556, 1561, 1562, 1569, 1571, 1576, 1580, 1584, 1586, 1587, 1591, 1599, 1604, 1608, 1609, 1610, 1613, 1618, 1622, 1627, 1629, 1633, 1639, 1642, 1649, 1654, 1659, 1664, 1669, 1672, 1674, 1676, 1679, 1682, 1683, 1684, 1687, 1692, 1716, 1721, 1734, 1739, 1744, 1745, 1749, 1751, 1759, 1764, 1768, 1773, 1775, 1777, 1782, 1783, 1792, 1801, 1805, 1811, 1821, 1822, 1827, 1829, 1836, 1844, 1846, 1847, 1851, 1860, 1866, 1868, 1874, 1876, 1884, 1886, 1891, 1894, 1896, 1900, 1904, 1908, 1910, 1918, 1919, 1924, 1932, 1942, 1948, 1953, 1958, 1959, 1963, 1966, 1969, 1970, 1974, 1978, 1983, 1987, 1988, 1989, 1990, 1992, 1997, 1998, 2003, 2009, 2011, 2014, 2016, 2027, 2029, 2049, 2056, 2066, 2071, 2075, 2076, 2077, 2079, 2083, 2089, 2090, 2098, 2103, 2104, 2105, 2106, 2107, 2112, 2117, 2118, 2122, 2123, 2128, 2133, 2137, 2138, 2143, 2148, 2152, 2156, 2161, 2166, 2172, 2176, 2178, 2182, 2185, 2187, 2192, 2198, 2201, 2204, 2208, 2209, 2213, 2214, 2219, 2222, 2226, 2227, 2234, 2246, 2254, 2258, 2263, 2265, 2267, 2273, 2282, 2283, 2284, 2287, 2291, 2299, 2302, 2309, 2316, 2319, 2321, 2326, 2329, 2331, 2335, 2344, 2351, 2379, 2381, 2383, 2385, 2387, 2393, 2399, 2408, 2413, 2414, 2417, 2419, 2421, 2426, 2427, 2430, 2433, 2452, 2456, 2461, 2463, 2471, 2476, 2479, 2485, 2486, 2487, 2491, 2500, 2504, 2507, 2509, 2511, 2513, 2528, 2536, 2538, 2546, 2552, 2558, 2567, 2, 7, 12, 17, 22, 27, 32, 37, 42, 47, 52, 58, 63, 68, 73, 78, 83, 88, 93, 98, 103, 108, 113, 118, 123, 128, 133, 138, 143, 148, 153, 158, 164, 169, 171, 173, 174, 179, 182, 183, 188, 193, 198, 203, 208, 212, 217, 223, 231, 236, 237, 239, 244, 251, 256, 264, 271, 281, 286, 289, 294, 299, 306, 311, 316, 319, 324, 331, 336, 341, 344, 348, 352, 357, 363, 364, 371, 376, 382, 386, 393, 396, 401, 407, 414, 419, 423, 429, 434, 441, 451, 456, 457, 461, 466, 476, 478, 481, 486, 491, 492, 494, 499, 504, 512, 518, 523, 531, 536, 543, 548, 551, 553, 558, 563, 569, 574, 578, 589, 593, 599, 604, 609, 614, 623, 634, 641, 646, 651, 656, 666, 674, 684, 688, 696, 701, 706, 713, 721, 724, 729, 730, 733, 737, 741, 744, 745, 749, 753, 758, 763, 768, 772, 773, 776, 778, 784, 789, 792, 796, 803, 805, 807, 812, 817, 822, 827, 832, 836, 842, 847, 858, 864, 872, 873, 876, 878, 882, 887, 894, 901, 906, 912, 917, 923, 927, 931, 941, 946, 952, 957, 962, 968, 973, 977, 982, 986, 988, 994, 999, 1003, 1009, 1021, 1032, 1037, 1040, 1041, 1047, 1052, 1061, 1062, 1067, 1072, 1075, 1076, 1082, 1091, 1095, 1104, 1108, 1109, 1117, 1123, 1128, 1133, 1141, 1147, 1152, 1157, 1162, 1167, 1171, 1181, 1186, 1194, 1204, 1209, 1216, 1224, 1229, 1234, 1238, 1240, 1241, 1242, 1245, 1246, 1249, 1250, 1254, 1264, 1274, 1279, 1283, 1289, 1296, 1301, 1302, 1311, 1316, 1321, 1327, 1334, 1342, 1346, 1356, 1367, 1368, 1372, 1377, 1378, 1382, 1383, 1386, 1393, 1397, 1404, 1406, 1413, 1418, 1422, 1428, 1444, 1450, 1454, 1457, 1462, 1467, 1469, 1486, 1491, 1498, 1503, 1508, 1513, 1518, 1523, 1524, 1531, 1538, 1543, 1548, 1554, 1555, 1558, 1564, 1574, 1579, 1588, 1589, 1593, 1595, 1596, 1598, 1600, 1606, 1612, 1617, 1621, 1634, 1645, 1646, 1651, 1653, 1656, 1661, 1665, 1671, 1678, 1680, 1694, 1697, 1698, 1702, 1705, 1708, 1712, 1717, 1719, 1722, 1729, 1737, 1754, 1758, 1767, 1772, 1778, 1786, 1787, 1789, 1790, 1793, 1804, 1809, 1812, 1814, 1819, 1834, 1841, 1852, 1858, 1861, 1864, 1878, 1882, 1885, 1887, 1898, 1906, 1909, 1913, 1917, 1927, 1928, 1931, 1937, 1943, 1949, 1964, 1965, 1968, 1979, 1982, 1984, 1993, 1999, 2006, 2008, 2013, 2031, 2034, 2039, 2041, 2043, 2046, 2047, 2054, 2057, 2058, 2061, 2064, 2069, 2074, 2080, 2087, 2091, 2092, 2096, 2100, 2101, 2109, 2114, 2124, 2129, 2134, 2135, 2141, 2146, 2154, 2163, 2169, 2174, 2179, 2189, 2193, 2194, 2202, 2211, 2216, 2221, 2229, 2231, 2233, 2241, 2242, 2247, 2253, 2255, 2257, 2262, 2268, 2270, 2275, 2279, 2289, 2293, 2297, 2298, 2307, 2313, 2318, 2323, 2332, 2336, 2339, 2343, 2349, 2356, 2357, 2361, 2364, 2366, 2371, 2374, 2377, 2382, 2388, 2401, 2407, 2429, 2436, 2443, 2462, 2464, 2465, 2466, 2472, 2473, 2475, 2496, 2502, 2516, 2517, 2532, 2533, 2544, 2556, 2563, 2570, 2572, 2577, 2587, 2591, 2597, 2604, 2605, 2606, 2610, 2619, 2623, 2627, 2637, 2643, 2651, 2659, 2666, 2669, 2673, 2677, 2682, 2687, 2696, 2704, 2708, 2713, 2717, 2718, 2720, 2722, 2728, 2733, 2746, 2749, 2752, 2753, 2756, 2766, 2771, 2780, 2781, 2784, 2788, 2789, 2793, 2806, 2810, 2818, 2825, 2827, 2831, 2836, 2838, 2841, 2849, 2850, 2861, 2867, 2869, 2870, 2871, 2877, 2883, 2898, 2908, 2916, 2919, 2926, 2928, 2936, 2938, 2941, 2945, 2947, 2956, 2957, 2959, 2981, 2982, 2985, 2991, 2996, 3001, 3007, 3010, 3017, 3018, 3024, 3032, 3036, 3038, 3041, 3053, 3058, 3059, 3063, 3069, 3076, 3087, 3093, 3098, 3103, 3107, 3112, 3115, 3116, 3124, 3130, 3134, 3137, 3141, 3143, 3146, 3150, 3156, 3159, 3161, 3163, 3164, 3165, 3177, 3189, 3194, 3204, 3209, 3216, 3219, 3224, 3234, 3242, 3247, 3252, 3254, 3259, 3270, 3273, 3279, 3292, 3293, 3314, 3316, 3319, 3323, 3326, 3328, 3334, 3339, 3341, 3346, 3356, 3357, 3364, 3372, 3374, 3377, 3378, 3382, 3, 8, 13, 18, 23, 28, 33, 38, 43, 48, 54, 59, 64, 69, 74, 79, 84, 89, 94, 99, 104, 109, 114, 119, 124, 129, 134, 139, 144, 149, 154, 159, 166, 176, 181, 186, 191, 196, 201, 206, 214, 219, 228, 234, 241, 246, 253, 258, 267, 272, 275, 276, 279, 284, 288, 293, 298, 304, 309, 314, 318, 322, 326, 327, 328, 333, 338, 345, 349, 359, 368, 374, 381, 383, 384, 391, 397, 398, 404, 409, 417, 421, 432, 437, 443, 447, 452, 458, 463, 468, 471, 479, 484, 489, 497, 502, 507, 514, 519, 524, 529, 534, 541, 546, 554, 559, 564, 571, 579, 583, 588, 598, 603, 608, 613, 626, 630, 637, 642, 647, 657, 660, 663, 669, 671, 676, 680, 683, 687, 693, 698, 707, 708, 717, 723, 727, 734, 738, 743, 748, 751, 752, 757, 762, 766, 767, 769, 777, 782, 788, 790, 791, 802, 809, 818, 823, 828, 833, 835, 837, 843, 848, 859, 869, 871, 874, 881, 892, 899, 904, 909, 919, 926, 929, 936, 937, 939, 940, 943, 949, 958, 966, 971, 975, 979, 987, 993, 998, 1002, 1008, 1012, 1013, 1026, 1034, 1039, 1051, 1056, 1057, 1059, 1064, 1066, 1071, 1078, 1079, 1083, 1093, 1098, 1100, 1103, 1107, 1118, 1126, 1127, 1131, 1136, 1142, 1149, 1154, 1159, 1164, 1166, 1168, 1172, 1179, 1184, 1192, 1201, 1206, 1213, 1219, 1221, 1226, 1231, 1235, 1239, 1243, 1247, 1256, 1262, 1268, 1271, 1276, 1281, 1282, 1284, 1291, 1297, 1304, 1306, 1308, 1313, 1318, 1324, 1329, 1336, 1338, 1341, 1345, 1349, 1353, 1358, 1362, 1369, 1371, 1374, 1381, 1387, 1394, 1398, 1408, 1409, 1414, 1420, 1424, 1429, 1433, 1434, 1437, 1442, 1447, 1452, 1455, 1456, 1458, 1472, 1479, 1487, 1492, 1501, 1506, 1511, 1517, 1527, 1528, 1534, 1539, 1544, 1557, 1563, 1567, 1573, 1575, 1577, 1581, 1592, 1597, 1603, 1611, 1616, 1620, 1624, 1628, 1636, 1641, 1647, 1663, 1677, 1685, 1688, 1689, 1691, 1693, 1696, 1701, 1704, 1706, 1707, 1711, 1714, 1723, 1726, 1730, 1731, 1738, 1748, 1753, 1756, 1761, 1763, 1766, 1771, 1776, 1784, 1791, 1797, 1799, 1803, 1823, 1830, 1831, 1832, 1837, 1854, 1855, 1856, 1863, 1869, 1872, 1873, 1879, 1883, 1888, 1892, 1899, 1902, 1921, 1923, 1930, 1936, 1938, 1947, 1952, 1954, 1956, 1967, 1972, 1973, 1976, 1980, 1991, 1996, 2017, 2024, 2026, 2032, 2033, 2042, 2044, 2048, 2052, 2062, 2072, 2073, 2078, 2081, 2082, 2088, 2094, 2102, 2111, 2119, 2131, 2139, 2142, 2144, 2149, 2150, 2151, 2153, 2155, 2157, 2158, 2159, 2162, 2167, 2173, 2177, 2197, 2203, 2206, 2207, 2217, 2223, 2232, 2236, 2237, 2238, 2244, 2250, 2251, 2256, 2261, 2266, 2271, 2274, 2276, 2277, 2288, 2290, 2308, 2314, 2322, 2327, 2333, 2337, 2341, 2342, 2347, 2353, 2359, 2367, 2372, 2378, 2380, 2384, 2391, 2397, 2409, 2415, 2416, 2428, 2434, 2439, 2440, 2441, 2444, 2446, 2447, 2449, 2451, 2457, 2459, 2469, 2477, 2481, 2483, 2492, 2495, 2497, 2501, 2506, 2512, 2519, 2524, 2527, 2541, 2543, 2549, 2551, 2559, 2562, 2565, 2568, 2574, 2580, 2582, 2583, 2585, 2588, 2589, 2592, 2598, 2600, 2608, 2609, 2612, 2616, 2622, 2642, 2648, 2654, 2655, 2662, 2667, 2668, 2672, 2686, 2691, 2705, 2707, 2712, 2721, 2724, 2732, 2739, 2741, 2751, 2769, 2774, 2777, 2783, 2796, 2804, 2817, 2821, 2822, 2828, 2842, 2843, 2846, 2851, 2853, 2855, 2862, 2874, 2879, 2886, 2888, 2902, 2904, 2906, 2911, 2913, 2914, 2921, 2922, 2933, 2937, 2948, 2954, 2958, 2961, 2964, 2967, 2971, 2979, 2983, 2984, 2987, 2997, 3002, 3008, 3012, 3014, 3016, 3019, 3022, 3028, 3031, 3035, 3037, 3043, 3046, 3048, 3054, 3056, 3061, 3067, 3073, 3079, 3083, 3092, 3097, 3102, 3106, 3108, 3111, 3117, 3118, 3122, 3127, 3133, 3136, 3147, 3151, 3157, 3160, 3167, 3170, 3172, 3174, 3178, 3187, 3192, 3197, 3199, 3206, 3217, 3220, 3225, 3230, 3232, 3235, 3237, 3238, 3239, 3243, 3249, 3265, 3267, 3271, 3272, 3274, 3282, 3283, 3288, 3291, 3297, 3302, 3304, 3311, 3324, 3331, 3337, 3347, 3349, 3359, 3369, 3376, 3388, 3389]\n"]}]},{"cell_type":"markdown","metadata":{"id":"yM72AO4uXhf8"},"source":["#### (c) Training and validating model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkZ6W9MUXhf9"},"outputs":[],"source":["param = {'learning_rate': 0.051447544749765035,\n","         'max_delta_step': 2.956459783615207,\n","         'max_depth': 5.034202474908222,\n","         'min_child_weight': 7.457989829577018,\n","         'num_rounds': 400.50601395689256,\n","         'reg_alpha': 1.0858835704466614,\n","         'reg_lambda': 1.1385559144302175}\n","\n","\n","\n","\n","\n","num_round = param[\"num_rounds\"]\n","param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","\n","del param[\"num_rounds\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"mGyIGsWlXhf9","executionInfo":{"status":"ok","timestamp":1713077209826,"user_tz":-300,"elapsed":14,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"5c31b5ca-b1c8-42b4-c68e-414d543e2306"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nR2 = []\\nMSE = []\\nPearson = []\\n\\nfor i in range(5):\\n    train_index, test_index  = train_indices[i], test_indices[i]\\n    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\\n    dvalid = xgb.DMatrix(train_X[test_index])\\n\\n    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\\n\\n    y_valid_pred = bst.predict(dvalid)\\n    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\\n    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\\n    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\\n\\nprint(Pearson)\\nprint(MSE)\\nprint(R2)\\n\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b.npy\"), np.array(Pearson))\\nnp.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_ESM1b.npy\"), np.array(MSE))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b.npy\"), np.array(R2))'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["'''\n","R2 = []\n","MSE = []\n","Pearson = []\n","\n","for i in range(5):\n","    train_index, test_index  = train_indices[i], test_indices[i]\n","    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","    dvalid = xgb.DMatrix(train_X[test_index])\n","\n","    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","    y_valid_pred = bst.predict(dvalid)\n","    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n","    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n","\n","print(Pearson)\n","print(MSE)\n","print(R2)\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b.npy\"), np.array(Pearson))\n","np.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_ESM1b.npy\"), np.array(MSE))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b.npy\"), np.array(R2))'''\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"dRn6zSctXhf-","executionInfo":{"status":"ok","timestamp":1713077310625,"user_tz":-300,"elapsed":100811,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"71c42767-eb11-413f-9706-7df48b118981"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.621 0.823 0.384\n"]}],"source":["dtrain = xgb.DMatrix(train_X, label = train_Y)\n","dtest = xgb.DMatrix(test_X)\n","\n","bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","y_test_pred = bst.predict(dtest)\n","\n","# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n","MSE_dif_fp_test = mean_squared_error(np.reshape(test_Y, (-1)), y_test_pred)\n","R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n","Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n","\n","print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b.npy\"), bst.predict(dtest))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b.npy\"), test_Y)"]},{"cell_type":"markdown","source":["### Pearson 0.621 MSE 0.823 R2 0.384\n"],"metadata":{"id":"kov1dY-uFMr0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hL3UKy7BXhf-"},"outputs":[],"source":["save_pickel_model(bst, \"xgboost_esm1b.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbSt0tnDXhf_"},"outputs":[],"source":["y_test_pred_esm1b = y_test_pred"]},{"cell_type":"markdown","metadata":{"id":"YA_OJh6EXhf_"},"source":["## 2. Training a model with only sequence information (ESM-1b_ts):"]},{"cell_type":"markdown","metadata":{"id":"gGVodXoHXhf_"},"source":["#### (a) Creating input matrices:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPoxQXf2Xhf_"},"outputs":[],"source":["train_ESM1b = np.array(list(data_train[\"ESM1b_ts\"]))\n","train_X = train_ESM1b\n","train_Y = np.array(list(data_train[\"log10_kcat\"]))\n","\n","test_ESM1b = np.array(list(data_test[\"ESM1b_ts\"]))\n","test_X = test_ESM1b\n","test_Y = np.array(list(data_test[\"log10_kcat\"]))"]},{"cell_type":"markdown","metadata":{"id":"oc_YW3AbXhf_"},"source":["#### (b) Hyperparameter optimization:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpDJcM33Xhf_"},"outputs":[],"source":["\n","'''def cross_validation_mse_gradient_boosting(param):\n","    num_round = param[\"num_rounds\"]\n","    del param[\"num_rounds\"]\n","    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","    param[\"tree_method\"] = \"gpu_hist\"\n","    param[\"sampling_method\"] = \"gradient_based\"\n","\n","    MSE = []\n","    R2 = []\n","    for i in range(5):\n","        train_index, test_index  = train_indices[i], test_indices[i]\n","        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","        dvalid = xgb.DMatrix(train_X[test_index])\n","        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","        y_valid_pred = bst.predict(dvalid)\n","        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n","    return(-np.mean(R2))\n","\n","\n","from hyperopt import fmin, tpe, rand, hp, Trials\n","\n","space_gradient_boosting = {\n","    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n","    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n","    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n","    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n","    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n","    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n","    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n","    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n","\n","\n","trials = Trials()\n","best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n","            algo=rand.suggest, max_evals = 200, trials=trials)''';"]},{"cell_type":"markdown","metadata":{"id":"roJpbL9hXhgA"},"source":["#### (c) Training and validating model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYohvHhlXhgA"},"outputs":[],"source":["param = {'learning_rate': 0.2831145406836757,\n","         'max_delta_step': 0.07686715986169101,\n","         'max_depth': 4.96836783761305,\n","          'min_child_weight': 6.905400087083855,\n","           'num_rounds': 313.1498988074061,\n","            'reg_alpha': 1.717314107718892,\n","             'reg_lambda': 2.470354543039016}\n","\n","num_round = param[\"num_rounds\"]\n","param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","\n","del param[\"num_rounds\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E98TGq7FXhgA","outputId":"219349ca-4f10-4ec4-b307-5f198ccfd13b","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1713077333132,"user_tz":-300,"elapsed":5,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nR2 = []\\nMSE = []\\nPearson = []\\ny_valid_pred_esm1b_ts = []\\n\\nfor i in range(5):\\n    train_index, test_index  = train_indices[i], test_indices[i]\\n    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\\n    dvalid = xgb.DMatrix(train_X[test_index])\\n\\n    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\\n\\n    y_valid_pred = bst.predict(dvalid)\\n    y_valid_pred_esm1b_ts.append(y_valid_pred)\\n    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\\n    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\\n    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\\n\\nprint(Pearson)\\nprint(MSE)\\nprint(R2)\\n\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts.npy\"), np.array(Pearson))\\nnp.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_ESM1b_ts.npy\"), np.array(MSE))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts.npy\"), np.array(R2))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}],"source":["'''\n","R2 = []\n","MSE = []\n","Pearson = []\n","y_valid_pred_esm1b_ts = []\n","\n","for i in range(5):\n","    train_index, test_index  = train_indices[i], test_indices[i]\n","    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","    dvalid = xgb.DMatrix(train_X[test_index])\n","\n","    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","    y_valid_pred = bst.predict(dvalid)\n","    y_valid_pred_esm1b_ts.append(y_valid_pred)\n","    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n","    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n","\n","print(Pearson)\n","print(MSE)\n","print(R2)\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts.npy\"), np.array(Pearson))\n","np.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_ESM1b_ts.npy\"), np.array(MSE))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts.npy\"), np.array(R2))\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-93SzHbWXhgA","executionInfo":{"status":"ok","timestamp":1713077400312,"user_tz":-300,"elapsed":66830,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"9ce207fa-9176-4bb3-a51b-6dd626b8e353"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.629 0.808 0.396\n"]}],"source":["dtrain = xgb.DMatrix(train_X, label = train_Y)\n","dtest = xgb.DMatrix(test_X)\n","\n","bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","y_test_pred = bst.predict(dtest)\n","MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n","R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n","Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n","\n","print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts.npy\"), bst.predict(dtest))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts.npy\"), test_Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6q6p6ULXhgB"},"outputs":[],"source":["save_pickel_model(bst, \"xgboost_esm1b_ts.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unQCCgN0XhgB"},"outputs":[],"source":["y_test_pred_esm1b_ts = y_test_pred"]},{"cell_type":"markdown","metadata":{"id":"OhpLe6pBXhgB"},"source":["## 3. Training a model with only reaction information (DRFP):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CW-ey5I3XhgB"},"outputs":[],"source":["train_X = np.array(list(data_train[\"DRFP\"]))\n","train_Y = np.array(list(data_train[\"log10_kcat\"]))\n","\n","test_X = np.array(list(data_test[\"DRFP\"]))\n","test_Y = np.array(list(data_test[\"log10_kcat\"]))"]},{"cell_type":"markdown","metadata":{"id":"GVHFm5KXXhgC"},"source":["#### (b) Hyperparameter optimization:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeUi-qtJXhgC"},"outputs":[],"source":["'''def cross_validation_mse_gradient_boosting(param):\n","    num_round = param[\"num_rounds\"]\n","    del param[\"num_rounds\"]\n","    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","    param[\"tree_method\"] = \"gpu_hist\"\n","    param[\"sampling_method\"] = \"gradient_based\"\n","\n","    MSE = []\n","    R2 = []\n","    for i in range(5):\n","        train_index, test_index  = train_indices[i], test_indices[i]\n","        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","        dvalid = xgb.DMatrix(train_X[test_index])\n","        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","        y_valid_pred = bst.predict(dvalid)\n","        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n","    return(-np.mean(R2))\n","\n","\n","from hyperopt import fmin, tpe, rand, hp, Trials\n","\n","space_gradient_boosting = {\n","    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n","    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n","    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n","    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n","    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n","    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n","    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n","    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n","\n","\n","trials = Trials()\n","best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n","            algo=rand.suggest, max_evals = 200, trials=trials)''';"]},{"cell_type":"markdown","metadata":{"id":"wLdikOD9XhgC"},"source":["#### (c) Training and validating model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cu5uridFXhgC"},"outputs":[],"source":["param = {'learning_rate': 0.08987247189322463,\n","         'max_delta_step': 1.1939737318908727,\n","         'max_depth': 11.268531225242574,\n","         'min_child_weight': 2.8172720953826302,\n","         'num_rounds': 400.03643430746544,\n","         'reg_alpha': 1.9412226989868904,\n","         'reg_lambda': 4.950543905603358}\n","\n","\n","num_round = param[\"num_rounds\"]\n","param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","\n","del param[\"num_rounds\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"tzmU-TbGXhgC","executionInfo":{"status":"ok","timestamp":1713077400314,"user_tz":-300,"elapsed":10,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"ace91797-f1f6-4aa3-ccf6-9bba358b0264"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nR2 = []\\nMSE = []\\nPearson = []\\ny_valid_pred_DRFP = []\\n\\nfor i in range(5):\\n    train_index, test_index  = train_indices[i], test_indices[i]\\n    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\\n    dvalid = xgb.DMatrix(train_X[test_index])\\n\\n    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\\n\\n    y_valid_pred = bst.predict(dvalid)\\n    y_valid_pred_DRFP.append(y_valid_pred)\\n    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\\n    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\\n    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\\n\\nprint(Pearson)\\nprint(MSE)\\nprint(R2)\\n\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_DRFP.npy\"), np.array(Pearson))\\nnp.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_DRFP.npy\"), np.array(MSE))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_DRFP.npy\"), np.array(R2))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}],"source":["'''\n","R2 = []\n","MSE = []\n","Pearson = []\n","y_valid_pred_DRFP = []\n","\n","for i in range(5):\n","    train_index, test_index  = train_indices[i], test_indices[i]\n","    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","    dvalid = xgb.DMatrix(train_X[test_index])\n","\n","    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","    y_valid_pred = bst.predict(dvalid)\n","    y_valid_pred_DRFP.append(y_valid_pred)\n","    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n","    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n","\n","print(Pearson)\n","print(MSE)\n","print(R2)\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_DRFP.npy\"), np.array(Pearson))\n","np.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_DRFP.npy\"), np.array(MSE))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_DRFP.npy\"), np.array(R2))\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aXS7esAgXhgF","executionInfo":{"status":"ok","timestamp":1713077407257,"user_tz":-300,"elapsed":6949,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"30d8f78f-3ca9-4d5e-ac0e-c48df9193b73"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.606 0.867 0.352\n"]}],"source":["dtrain = xgb.DMatrix(train_X, label = train_Y)\n","dtest = xgb.DMatrix(test_X)\n","\n","bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","y_test_pred = bst.predict(dtest)\n","MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n","R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n","Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n","\n","print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_DRFP.npy\"), bst.predict(dtest))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_DRFP.npy\"), test_Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcUhchFaXhgM"},"outputs":[],"source":["save_pickel_model(bst, \"xgboost_drfp.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Nh5nemeXhgM"},"outputs":[],"source":["y_test_pred_drfp = y_test_pred"]},{"cell_type":"markdown","metadata":{"id":"fmLhN6qoXhgM"},"source":["## 4. Training a model with only reaction information (difference fingerprint):"]},{"cell_type":"markdown","metadata":{"id":"RIaSOV9GXhgM"},"source":["#### (a) Creating input matrices:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ds_6GavXhgM"},"outputs":[],"source":["train_X = np.array(list(data_train[\"difference_fp\"]))\n","train_Y = np.array(list(data_train[\"log10_kcat\"]))\n","\n","test_X = np.array(list(data_test[\"difference_fp\"]))\n","test_Y = np.array(list(data_test[\"log10_kcat\"]))"]},{"cell_type":"markdown","metadata":{"id":"M2o0ZV7lXhgN"},"source":["#### (b) Hyperparameter optimization:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bsbhhnU5XhgN"},"outputs":[],"source":["'''def cross_validation_mse_gradient_boosting(param):\n","    num_round = param[\"num_rounds\"]\n","    del param[\"num_rounds\"]\n","    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","    param[\"tree_method\"] = \"gpu_hist\"\n","    param[\"sampling_method\"] = \"gradient_based\"\n","\n","    MSE = []\n","    R2 = []\n","    for i in range(5):\n","        train_index, test_index  = train_indices[i], test_indices[i]\n","        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","        dvalid = xgb.DMatrix(train_X[test_index])\n","        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","        y_valid_pred = bst.predict(dvalid)\n","        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n","    return(-np.mean(R2))\n","\n","\n","from hyperopt import fmin, tpe, rand, hp, Trials\n","\n","space_gradient_boosting = {\n","    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n","    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n","    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n","    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n","    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n","    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n","    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n","    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n","\n","\n","trials = Trials()\n","best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n","            algo=rand.suggest, max_evals = 200, trials=trials)''';"]},{"cell_type":"markdown","metadata":{"id":"aBEYlxjkXhgN"},"source":["#### (c) Training and validating model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-mi2LOmXhgN"},"outputs":[],"source":["param = {'learning_rate': 0.14154883958006167,\n","         'max_delta_step': 0.02234358170535966,\n","         'max_depth': 10.869653004093198,\n","         'min_child_weight': 1.7936882442746056,\n","         'num_rounds': 361.6168542774665,\n","         'reg_alpha': 4.825525325323308,\n","         'reg_lambda': 2.74944090578774}\n","\n","\n","num_round = param[\"num_rounds\"]\n","param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","\n","del param[\"num_rounds\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"pBaGb0PUXhgO","executionInfo":{"status":"ok","timestamp":1713077407259,"user_tz":-300,"elapsed":21,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"cda4ac68-9c85-4e21-cd3e-856cc17b65ba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nR2 = []\\nMSE = []\\nPearson = []\\n\\nfor i in range(5):\\n    train_index, test_index  = train_indices[i], test_indices[i]\\n    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\\n    dvalid = xgb.DMatrix(train_X[test_index])\\n\\n    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\\n\\n    y_valid_pred = bst.predict(dvalid)\\n    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\\n    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\\n    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\\n\\nprint(Pearson)\\nprint(MSE)\\nprint(R2)\\n\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_diff_fp.npy\"), np.array(Pearson))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_diff_fp.npy\"), np.array(MSE))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_diff_fp.npy\"), np.array(R2))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}],"source":["'''\n","R2 = []\n","MSE = []\n","Pearson = []\n","\n","for i in range(5):\n","    train_index, test_index  = train_indices[i], test_indices[i]\n","    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","    dvalid = xgb.DMatrix(train_X[test_index])\n","\n","    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","    y_valid_pred = bst.predict(dvalid)\n","    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n","    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n","\n","print(Pearson)\n","print(MSE)\n","print(R2)\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_diff_fp.npy\"), np.array(Pearson))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_diff_fp.npy\"), np.array(MSE))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_diff_fp.npy\"), np.array(R2))\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esnkn93CXhgO","executionInfo":{"status":"ok","timestamp":1713077431271,"user_tz":-300,"elapsed":24032,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"2f784284-0189-4673-dbf0-3dfa249cefc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.624 0.849 0.365\n"]}],"source":["dtrain = xgb.DMatrix(train_X, label = train_Y)\n","dtest = xgb.DMatrix(test_X)\n","\n","bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","y_test_pred = bst.predict(dtest)\n","MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n","R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n","Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n","\n","print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n","\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_diff_fp.npy\"), bst.predict(dtest))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_diff_fp.npy\"), test_Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OE5C8UE1XhgP"},"outputs":[],"source":["save_pickel_model(bst, \"xgboost_diff.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Po9VBo-SXhgP"},"outputs":[],"source":["y_test_pred_diff_fp = y_test_pred"]},{"cell_type":"markdown","metadata":{"id":"N63El6EgXhgP"},"source":["## 5. Training a model with only reaction information (structural fingerprint):"]},{"cell_type":"markdown","metadata":{"id":"PsZQlBPNXhgP"},"source":["#### (a) Creating input matrices:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8auKfm_FXhgQ"},"outputs":[],"source":["train_X = np.array(data_train[\"structural_fp\"].tolist())\n","train_Y = np.array(list(data_train[\"log10_kcat\"]))\n","\n","test_X = np.array(list(data_test[\"structural_fp\"]))\n","test_Y = np.array(list(data_test[\"log10_kcat\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8UXwxruCXhgQ","executionInfo":{"status":"ok","timestamp":1713077432479,"user_tz":-300,"elapsed":11,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"4cb20148-72a6-4d5b-c9dd-7f08ab6e0a96"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((3391, 4096), (874, 4096))"]},"metadata":{},"execution_count":39}],"source":["train_X.shape, test_X.shape"]},{"cell_type":"markdown","metadata":{"id":"IDH5yoHrXhgQ"},"source":["#### (b) Hyperparameter optimization:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDRwRs41XhgQ"},"outputs":[],"source":["'''def cross_validation_mse_gradient_boosting(param):\n","    num_round = param[\"num_rounds\"]\n","    del param[\"num_rounds\"]\n","    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","    param[\"tree_method\"] = \"gpu_hist\"\n","    param[\"sampling_method\"] = \"gradient_based\"\n","\n","    MSE = []\n","    R2 = []\n","    for i in range(5):\n","        train_index, test_index  = train_indices[i], test_indices[i]\n","        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","        dvalid = xgb.DMatrix(train_X[test_index])\n","        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","        y_valid_pred = bst.predict(dvalid)\n","        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n","    return(-np.mean(R2))\n","\n","\n","from hyperopt import fmin, tpe, rand, hp, Trials\n","\n","space_gradient_boosting = {\n","    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n","    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n","    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n","    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n","    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n","    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n","    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n","    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n","\n","\n","trials = Trials()\n","best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n","            algo=rand.suggest, max_evals = 200, trials=trials)''';"]},{"cell_type":"markdown","metadata":{"id":"aNogD0rXXhgR"},"source":["#### (c) Training and validating model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_N_mMf3XXhgR"},"outputs":[],"source":["param = {'learning_rate': 0.01126910440903659,\n","         'max_delta_step': 0.5777120839605732,\n","         'max_depth': 5.486901609313889,\n","         'min_child_weight': 6.14467742389769,\n","         'num_rounds': 488.943459090126,\n","         'reg_alpha': 4.629840853377147,\n","         'reg_lambda': 2.1047561335691745}\n","\n","num_round = param[\"num_rounds\"]\n","param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","\n","del param[\"num_rounds\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"FP33BEzcXhgR","executionInfo":{"status":"ok","timestamp":1713077432480,"user_tz":-300,"elapsed":10,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"7575b182-7d20-43fa-eaeb-654034c0a9b9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nR2 = []\\nMSE = []\\nPearson = []\\n\\nfor i in range(5):\\n    train_index, test_index  = train_indices[i], test_indices[i]\\n    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\\n    dvalid = xgb.DMatrix(train_X[test_index])\\n\\n    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\\n\\n    y_valid_pred = bst.predict(dvalid)\\n    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\\n    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\\n    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\\n\\nprint(Pearson)\\nprint(MSE)\\nprint(R2)\\n\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_str_fp.npy\"), np.array(Pearson))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_str_fp.npy\"), np.array(MSE))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_str_fp.npy\"), np.array(R2))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}],"source":["'''\n","R2 = []\n","MSE = []\n","Pearson = []\n","\n","for i in range(5):\n","    train_index, test_index  = train_indices[i], test_indices[i]\n","    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","    dvalid = xgb.DMatrix(train_X[test_index])\n","\n","    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","    y_valid_pred = bst.predict(dvalid)\n","    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n","    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n","\n","print(Pearson)\n","print(MSE)\n","print(R2)\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_str_fp.npy\"), np.array(Pearson))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_str_fp.npy\"), np.array(MSE))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_str_fp.npy\"), np.array(R2))\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_vDYukqXhgS","executionInfo":{"status":"ok","timestamp":1713077446527,"user_tz":-300,"elapsed":14055,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"8d102f27-81c3-43fc-962d-fd9a986103ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.567 0.918 0.313\n"]}],"source":["dtrain = xgb.DMatrix(train_X, label = train_Y)\n","dtest = xgb.DMatrix(test_X)\n","\n","bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","y_test_pred = bst.predict(dtest)\n","MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n","R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n","Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n","\n","print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n","\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_str_fp.npy\"), bst.predict(dtest))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_str_fp.npy\"), test_Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z44-nHwTXhgS"},"outputs":[],"source":["save_pickel_model(bst, \"xgboost_structFp.h5\")"]},{"cell_type":"markdown","metadata":{"id":"PGxSQXwVXhgS"},"source":["## 6. Training a model with enzyme and reaction information (ESM1b_ts/DRFP):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZJtORevXhgS"},"outputs":[],"source":["train_X = np.array(list(data_train[\"DRFP\"]))\n","train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"]))], axis = 1)\n","train_Y = np.array(list(data_train[\"log10_kcat\"]))\n","\n","test_X = np.array(list(data_test[\"DRFP\"]))\n","test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"]))], axis = 1)\n","test_Y = np.array(list(data_test[\"log10_kcat\"]))"]},{"cell_type":"markdown","metadata":{"id":"Ioj6xySBXhgT"},"source":["#### (b) Hyperparameter optimization:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1feN1C7BXhgT"},"outputs":[],"source":["'''def cross_validation_mse_gradient_boosting(param):\n","    num_round = param[\"num_rounds\"]\n","    del param[\"num_rounds\"]\n","    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","    param[\"tree_method\"] = \"gpu_hist\"\n","    param[\"sampling_method\"] = \"gradient_based\"\n","\n","    MSE = []\n","    R2 = []\n","    for i in range(5):\n","        train_index, test_index  = train_indices[i], test_indices[i]\n","        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","        dvalid = xgb.DMatrix(train_X[test_index])\n","        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","        y_valid_pred = bst.predict(dvalid)\n","        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n","    return(-np.mean(R2))\n","\n","\n","from hyperopt import fmin, tpe, rand, hp, Trials\n","\n","space_gradient_boosting = {\n","    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n","    \"max_depth\": hp.uniform(\"max_depth\", 6,14),\n","    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n","    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n","    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n","    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n","    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n","    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n","\n","\n","trials = Trials()\n","best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n","            algo=rand.suggest, max_evals = 200, trials=trials)''';"]},{"cell_type":"markdown","metadata":{"id":"XjjO9e88XhgT"},"source":["#### (c) Training and validating model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUo_95QKXhgT"},"outputs":[],"source":["param =  {\n","    'learning_rate': 0.0685249638804391,\n","    'max_depth': 9,\n","    'reg_lambda': 3.6231955480293934,\n","    'reg_alpha': 3.8923304126950957,\n","    'max_delta_step': 1.0371427260254895,\n","    'min_child_weight': 14.209819958496073,\n","    # 'sampling_method': 'gradient_based',\n","    'num_rounds': 400.9598325756988,\n","    }\n","\n","\n","num_round = param[\"num_rounds\"]\n","param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","\n","del param[\"num_rounds\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"qNqEhDxpXhgU","executionInfo":{"status":"ok","timestamp":1713077446528,"user_tz":-300,"elapsed":9,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"0720d2e1-a548-4d9c-f640-7227bf0702fd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nR2 = []\\nMSE = []\\nPearson = []\\n\\nfor i in range(5):\\n    train_index, test_index  = train_indices[i], test_indices[i]\\n    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\\n    dvalid = xgb.DMatrix(train_X[test_index])\\n\\n    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\\n\\n    y_valid_pred = bst.predict(dvalid)\\n    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\\n    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\\n    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\\n\\nprint(Pearson)\\nprint(MSE)\\nprint(R2)\\n\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(Pearson))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(MSE))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(R2))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":48}],"source":["'''\n","R2 = []\n","MSE = []\n","Pearson = []\n","\n","for i in range(5):\n","    train_index, test_index  = train_indices[i], test_indices[i]\n","    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","    dvalid = xgb.DMatrix(train_X[test_index])\n","\n","    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","    y_valid_pred = bst.predict(dvalid)\n","    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n","    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n","\n","print(Pearson)\n","print(MSE)\n","print(R2)\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(Pearson))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(MSE))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(R2))\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqM3wmQyXhgU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713077644154,"user_tz":-300,"elapsed":197634,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"4ee48b32-9eaa-46ca-a386-3da406f00603"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.656 0.766 0.427\n"]}],"source":["dtrain = xgb.DMatrix(train_X, label = train_Y)\n","dtest = xgb.DMatrix(test_X, label = test_Y)\n","\n","\n","bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","y_test_pred = bst.predict(dtest)\n","# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n","MSE_dif_fp_test = mean_squared_error(np.reshape(test_Y, (-1)), y_test_pred)\n","R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n","Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n","\n","print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n","\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts_DRFP.npy\"), bst.predict(dtest))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts_DRFP.npy\"), test_Y)\n","save_pickel_model(bst, \"xgboost_esm1bts_drfp.h5\")"]},{"cell_type":"markdown","source":["### Pearson = 0.656 MSE = 0.766 R2 = 0.427"],"metadata":{"id":"HWpt2XskTs7F"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"scx3BJ6TXhgU"},"outputs":[],"source":["y_test_pred_esm1b_ts_drfp = y_test_pred"]},{"cell_type":"markdown","metadata":{"id":"eUW7KwFKXhgU"},"source":["## 7. Training a model with enzyme and reaction information (ESM1b_ts/diff_fp):"]},{"cell_type":"markdown","metadata":{"id":"ETGmL0OtXhgU"},"source":["#### (a) Creating input matrices:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdK1tUT7XhgV"},"outputs":[],"source":["train_X = np.array(list(data_train[\"difference_fp\"]))\n","train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"]))], axis = 1)\n","train_Y = np.array(list(data_train[\"log10_kcat\"]))\n","\n","test_X = np.array(list(data_test[\"difference_fp\"]))\n","test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"]))], axis = 1)\n","test_Y = np.array(list(data_test[\"log10_kcat\"]))"]},{"cell_type":"markdown","metadata":{"id":"eyU5iL-eXhgV"},"source":["#### (b) Hyperparameter optimization:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMmKpz3KXhgV"},"outputs":[],"source":["'''def cross_validation_mse_gradient_boosting(param):\n","    num_round = param[\"num_rounds\"]\n","    del param[\"num_rounds\"]\n","    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","    param[\"tree_method\"] = \"gpu_hist\"\n","    param[\"sampling_method\"] = \"gradient_based\"\n","\n","    MSE = []\n","    R2 = []\n","    for i in range(5):\n","        train_index, test_index  = train_indices[i], test_indices[i]\n","        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","        dvalid = xgb.DMatrix(train_X[test_index])\n","        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","        y_valid_pred = bst.predict(dvalid)\n","        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n","    return(-np.mean(R2))\n","\n","\n","from hyperopt import fmin, tpe, rand, hp, Trials\n","\n","space_gradient_boosting = {\n","    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n","    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n","    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n","    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n","    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n","    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n","    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n","    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n","\n","\n","trials = Trials()\n","best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n","            algo=rand.suggest, max_evals = 200, trials=trials)''';"]},{"cell_type":"markdown","metadata":{"id":"n4N9EF_-XhgV"},"source":["#### (c) Training and validating model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D8WU9CcTXhgV"},"outputs":[],"source":["param = {\n","    'learning_rate': 0.15638876027725748,\n","    'max_depth': 8,\n","    'reg_lambda': 2.344616112944763,\n","    'reg_alpha': 4.262441473730964,\n","    'max_delta_step': 0.9349225200130806,\n","    'min_child_weight': 8.26447488212503,\n","    'num_rounds': 400.69265795096726,\n","    # 'sampling_method': 'gradient_based'\n","    }\n","\n","num_round = param[\"num_rounds\"]\n","param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","\n","del param[\"num_rounds\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYB-hp9EXhgW","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1713077644155,"user_tz":-300,"elapsed":8,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"875f9cc9-4e6f-413b-9f7a-ad6bc5cb67bc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nR2 = []\\nMSE = []\\nPearson = []\\n\\nfor i in range(5):\\n    train_index, test_index  = train_indices[i], test_indices[i]\\n    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\\n    dvalid = xgb.DMatrix(train_X[test_index])\\n\\n    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\\n\\n    y_valid_pred = bst.predict(dvalid)\\n    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\\n    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\\n    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\\n\\nprint(Pearson)\\nprint(MSE)\\nprint(R2)\\n\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(Pearson))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(MSE))\\nnp.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(R2))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54}],"source":["'''\n","R2 = []\n","MSE = []\n","Pearson = []\n","\n","for i in range(5):\n","    train_index, test_index  = train_indices[i], test_indices[i]\n","    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","    dvalid = xgb.DMatrix(train_X[test_index])\n","\n","    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","    y_valid_pred = bst.predict(dvalid)\n","    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n","    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n","\n","print(Pearson)\n","print(MSE)\n","print(R2)\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(Pearson))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(MSE))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(R2))\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"BGGLNO3vXhgW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713077748726,"user_tz":-300,"elapsed":104578,"user":{"displayName":"M Faizan","userId":"00467975065851664124"}},"outputId":"1ad4de8d-29b7-44a3-fd09-10c5b858f389"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.642 0.787 0.412\n"]}],"source":["dtrain = xgb.DMatrix(train_X, label = train_Y)\n","dtest = xgb.DMatrix(test_X, label = test_Y)\n","\n","\n","bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","y_test_pred = bst.predict(dtest)\n","MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n","R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n","Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n","\n","print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n","\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts_diff_fp.npy\"), bst.predict(dtest))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts_diff_fp.npy\"), test_Y)\n","\n","save_pickel_model(bst, \"xgboost_esm1b_ts_diff.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQaRZfrIXhgW"},"outputs":[],"source":["y_test_pred_esm1b_ts_drfp = y_test_pred"]},{"cell_type":"markdown","source":["## Training model using all features"],"metadata":{"id":"h9Ak5nzGafQB"}},{"cell_type":"code","source":["train_indices = list(np.load(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"CV_train_indices.npy\"), allow_pickle = True))\n","test_indices = list(np.load(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"CV_test_indices.npy\"), allow_pickle = True))"],"metadata":{"id":"Ej7VK5qYajcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"train_df_kcat_ours.pkl\"))\n","data_test = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"test_df_kcat_ours.pkl\"))\n","\n","# print((train_indices))\n","# print((test_indices))\n","\n","data_train.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n","data_test.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n","len(data_train), len(data_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iNQPpqdIamRr","executionInfo":{"status":"ok","timestamp":1705600187816,"user_tz":-120,"elapsed":6155,"user":{"displayName":"Sahar Alaa","userId":"13446251185895554495"}},"outputId":"6198c0b8-0671-40f4-ee0c-85134edcb524"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3391, 874)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["data_train.columns[:-7]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l4Ft2iF0AgA2","executionInfo":{"status":"ok","timestamp":1705600187816,"user_tz":-120,"elapsed":19,"user":{"displayName":"Sahar Alaa","userId":"13446251185895554495"}},"outputId":"0c5f3f7e-1547-41d0-97af-531396f4cb5d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Reaction ID', 'Sequence ID', 'kcat_values', 'Uniprot IDs',\n","       'from_BRENDA', 'from_Sabio', 'from_Uniprot', 'checked', 'Sequence',\n","       'substrates',\n","       ...\n","       '765', '766', '767', '771', '772', '773', '774', '775', '776', '777'],\n","      dtype='object', length=2270)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["np.array(data_train['structural_fp'].tolist())[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oigolRgi6PC","executionInfo":{"status":"ok","timestamp":1705600188901,"user_tz":-120,"elapsed":1101,"user":{"displayName":"Sahar Alaa","userId":"13446251185895554495"}},"outputId":"56483fe0-484b-43b4-99a4-3f213af9550a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, ..., 0, 0, 0])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"CRD3rwDGavee"},"source":["#### (a) Creating input matrices:"]},{"cell_type":"code","source":["train_X = np.array(list(data_train[\"ESM1b\"]))\n","print(train_X.shape)\n","test_X = np.array(list(data_test[\"ESM1b\"]))\n","for col in data_train.columns[17:21]:\n","  print(col)\n","  if col == 'structural_fp':\n","    train_X = np.concatenate([train_X, np.array(data_train[col].tolist(), dtype='float64')], axis = 1)\n","    test_X = np.concatenate([test_X, np.array(data_test[col].tolist(), dtype='float64')], axis = 1)\n","  else:\n","    train_X = np.concatenate([train_X, np.array(list(data_train[col]), dtype='float64')], axis = 1)\n","    test_X = np.concatenate([test_X, np.array(list(data_test[col]), dtype='float64')], axis = 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxWsFEzxazZw","executionInfo":{"status":"ok","timestamp":1705600190147,"user_tz":-120,"elapsed":1250,"user":{"displayName":"Sahar Alaa","userId":"13446251185895554495"}},"outputId":"4c785c15-fb7b-4f4d-9475-0fea38f47b12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(3391, 1280)\n","ESM1b_ts\n","structural_fp\n","difference_fp\n","DRFP\n"]}]},{"cell_type":"code","source":["train_X = np.concatenate([train_X, np.array(list(data_train['ESM1b_norm']), dtype='float64')], axis = 1)\n","test_X = np.concatenate([test_X, np.array(list(data_test['ESM1b_norm']), dtype='float64')], axis = 1)\n","train_X = np.concatenate([train_X, np.array(list(data_train['ESM1b_ts_norm']), dtype='float64')], axis = 1)\n","test_X = np.concatenate([test_X, np.array(list(data_test['ESM1b_ts_norm']), dtype='float64')], axis = 1)"],"metadata":{"id":"0RIJq9C2awPS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_X = np.concatenate([train_X, np.array(data_train.iloc[:,22:-7], dtype='float64')], axis = 1)\n","test_X = np.concatenate([test_X, np.array(data_test.iloc[:,22:-7], dtype='float64')], axis = 1)"],"metadata":{"id":"iOmagTvzGIx7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_X = train_X.astype('float64')\n","test_X = test_X.astype('float64')"],"metadata":{"id":"ZiHckc77XLrR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_Y = np.array(list(data_train[\"log10_kcat\"]))\n","test_Y = np.array(list(data_test[\"log10_kcat\"]))"],"metadata":{"id":"iLaYOGxqJFr4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_X.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAc3hyH5GgTa","executionInfo":{"status":"ok","timestamp":1705600191545,"user_tz":-120,"elapsed":11,"user":{"displayName":"Sahar Alaa","userId":"13446251185895554495"}},"outputId":"d48fadc4-e0ca-439f-b5ba-43642baedbb0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3391, 15560)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["type(train_X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76JlB5njLkNe","executionInfo":{"status":"ok","timestamp":1705600191545,"user_tz":-120,"elapsed":9,"user":{"displayName":"Sahar Alaa","userId":"13446251185895554495"}},"outputId":"05536318-175b-4c54-c3f5-95dd1d715a6e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"2XuKhPNTaveg"},"source":["#### (b) Hyperparameter optimization:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t913VsmFaveh","outputId":"881ff494-642c-474d-ba4d-de6d5f5c78f0"},"outputs":[{"output_type":"stream","name":"stdout","text":[" 87%|████████▋ | 174/200 [5:08:10<40:19, 93.06s/trial, best loss: -0.3735840090139679]"]}],"source":["def cross_validation_mse_gradient_boosting(param):\n","    num_round = param[\"num_rounds\"]\n","    del param[\"num_rounds\"]\n","    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","    param[\"tree_method\"] = \"gpu_hist\"\n","    param[\"sampling_method\"] = \"gradient_based\"\n","\n","    MSE = []\n","    R2 = []\n","    for i in range(5):\n","        train_index, test_index  = train_indices[i], test_indices[i]\n","        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","        dvalid = xgb.DMatrix(train_X[test_index])\n","        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","        y_valid_pred = bst.predict(dvalid)\n","        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n","    return(-np.mean(R2))\n","\n","\n","from hyperopt import fmin, tpe, rand, hp, Trials\n","\n","space_gradient_boosting = {\n","    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n","    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n","    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n","    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n","    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n","    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n","    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n","    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n","\n","\n","trials = Trials()\n","best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n","            algo=rand.suggest, max_evals = 200, trials=trials)"]},{"cell_type":"code","source":["best"],"metadata":{"id":"48MhRzmNgbaL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExIXj3__avei"},"source":["#### (c) Training and validating model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUfA7Cj6avej"},"outputs":[],"source":["param = {\n","    'learning_rate': 0.15638876027725748,\n","    'max_depth': 8,\n","    'reg_lambda': 2.344616112944763,\n","    'reg_alpha': 4.262441473730964,\n","    'max_delta_step': 0.9349225200130806,\n","    'min_child_weight': 8.26447488212503,\n","    'num_rounds': 400.69265795096726,\n","    # 'sampling_method': 'gradient_based'\n","    }\n","\n","num_round = param[\"num_rounds\"]\n","param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n","\n","del param[\"num_rounds\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705574591461,"user_tz":-120,"elapsed":2646870,"user":{"displayName":"Sahar Alaa","userId":"13446251185895554495"}},"outputId":"3bab1798-9df8-45b0-f3f5-cd854cc44ffa","id":"YGPWyR3_avek"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.6133773796060704, 0.5927733649751115, 0.5696579476183089, 0.5664901288879961, 0.6059675908688378]\n","[0.8995350132420895, 0.8907148311609805, 0.9570658132750683, 1.0011614220188685, 0.9055139379678919]\n","[0.37289620835138915, 0.3481887641919241, 0.3220370010024399, 0.31963431757814464, 0.3656110621769998]\n"]}],"source":["\n","R2 = []\n","MSE = []\n","Pearson = []\n","\n","for i in range(5):\n","    train_index, test_index  = train_indices[i], test_indices[i]\n","    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n","    dvalid = xgb.DMatrix(train_X[test_index])\n","\n","    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","    y_valid_pred = bst.predict(dvalid)\n","    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n","    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n","    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n","\n","print(Pearson)\n","print(MSE)\n","print(R2)\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_all.npy\"), np.array(Pearson))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_all.npy\"), np.array(MSE))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_all.npy\"), np.array(R2))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705575230176,"user_tz":-120,"elapsed":638729,"user":{"displayName":"Sahar Alaa","userId":"13446251185895554495"}},"outputId":"5649dd90-a7a7-4a3b-e5e6-4a8afb90a5b9","id":"_GxZL1IHavel"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.613 0.835 0.375\n"]}],"source":["dtrain = xgb.DMatrix(train_X, label = train_Y)\n","dtest = xgb.DMatrix(test_X, label = test_Y)\n","\n","\n","bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n","\n","y_test_pred = bst.predict(dtest)\n","MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n","R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n","Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n","\n","print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n","\n","\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_all.npy\"), bst.predict(dtest))\n","np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_all.npy\"), test_Y)\n","\n","save_pickel_model(bst, \"xgboost_all.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3PhopXTavem"},"outputs":[],"source":["y_test_pred_esm1b_ts_drfp = y_test_pred"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}